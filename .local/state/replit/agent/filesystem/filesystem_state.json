{"file_contents":{"backend/main.py":{"content":"from fastapi import FastAPI, Depends, HTTPException, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom sqlalchemy.orm import Session\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom contextlib import asynccontextmanager\nimport logging\nimport threading\nimport os\n\nfrom models import get_db, init_db\nfrom models.content import ContentQueue, ApprovalLog\nfrom services.claude_service import claude_service\nfrom services.image_generator import image_generator\nfrom services.social_poster import social_poster\nfrom services.notification_service import notification_service\nfrom services.news_scraper import news_scraper\nfrom services.translation_service import translation_service\nfrom services.facebook_poster import facebook_poster\nfrom services.scheduler import content_scheduler\nfrom services.telegram_webhook import telegram_webhook_handler\nfrom services.api_token_monitor import api_token_monitor\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Application lifespan - start/stop scheduler\"\"\"\n    logger.info(\"üöÄ Starting Gradus Media AI Agent...\")\n    init_db()\n    content_scheduler.start()\n    logger.info(\"‚úÖ Scheduler started - automation enabled!\")\n    \n    # Check for missed scraping in background thread (non-blocking)\n    def check_missed_scraping():\n        try:\n            content_scheduler.check_and_run_missed_scraping()\n        except Exception as e:\n            logger.error(f\"Error in missed scraping check: {e}\")\n    \n    check_thread = threading.Thread(target=check_missed_scraping, daemon=True)\n    check_thread.start()\n    logger.info(\"üîç Checking for missed scraping tasks in background...\")\n    \n    yield\n    \n    logger.info(\"Shutting down scheduler...\")\n    content_scheduler.stop()\n\napp = FastAPI(\n    title=\"Gradus Media AI Agent\",\n    description=\"Automated content creation and distribution\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass ChatRequest(BaseModel):\n    message: str\n    system_prompt: Optional[str] = None\n\nclass TranslateRequest(BaseModel):\n    text: str\n\nclass ApproveRequest(BaseModel):\n    moderator: str\n    scheduled_time: Optional[datetime] = None\n    platforms: Optional[List[str]] = None  # None = keep original platform from scraping\n\nclass RejectRequest(BaseModel):\n    moderator: str\n    reason: str\n\nclass EditRequest(BaseModel):\n    translated_text: Optional[str] = None\n    image_prompt: Optional[str] = None\n    platforms: Optional[List[str]] = None\n\nclass CreateContentRequest(BaseModel):\n    title: str\n    content: str\n    source: str = \"Manual\"\n    source_url: Optional[str] = None\n    language: str = \"uk\"\n    needs_translation: bool = False\n    platforms: List[str] = [\"facebook\", \"linkedin\"]\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"message\": \"Gradus Media AI Agent API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": {\n            \"health\": \"/health\",\n            \"chat\": \"/chat\",\n            \"translate\": \"/translate\",\n            \"content\": \"/api/content/*\"\n        }\n    }\n\n@app.get(\"/health\")\nasync def health_check():\n    fb_status = \"configured\" if (facebook_poster.page_access_token and facebook_poster.page_id) else \"awaiting credentials\"\n    return {\n        \"status\": \"healthy\",\n        \"services\": {\n            \"claude\": \"configured\",\n            \"database\": \"connected\",\n            \"image_generator\": \"ready\",\n            \"social_poster\": fb_status\n        }\n    }\n\n@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    try:\n        response = await claude_service.chat(request.message, request.system_prompt)\n        return {\"response\": response}\n    except Exception as e:\n        logger.error(f\"Chat error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/translate\")\nasync def translate(request: TranslateRequest):\n    try:\n        translation = await claude_service.translate_to_ukrainian(request.text)\n        return {\"translation\": translation}\n    except Exception as e:\n        logger.error(f\"Translation error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/test/telegram\")\nasync def test_telegram():\n    \"\"\"Test Telegram notification\"\"\"\n    result = notification_service.send_test_notification()\n    return result\n\n@app.get(\"/api/content\")\nasync def get_content(\n    status: Optional[str] = None,\n    platform: Optional[str] = None,\n    limit: int = 10,\n    db: Session = Depends(get_db)\n):\n    \"\"\"Get content with optional filters for status and platform\"\"\"\n    try:\n        query = db.query(ContentQueue)\n        \n        if status:\n            query = query.filter(ContentQueue.status == status)\n        \n        if platform:\n            from sqlalchemy import cast, String\n            query = query.filter(cast(ContentQueue.platforms, String).like(f'%{platform}%'))\n        \n        articles = query.order_by(ContentQueue.created_at.desc()).limit(limit).all()\n        \n        return [\n            {\n                \"id\": article.id,\n                \"title\": article.extra_metadata.get(\"title\") if article.extra_metadata else None,\n                \"translated_title\": article.translated_title,\n                \"status\": article.status,\n                \"platforms\": article.platforms,\n                \"language\": article.language,\n                \"needs_translation\": article.needs_translation,\n                \"created_at\": article.created_at.isoformat() if article.created_at else None,\n                \"source\": article.source,\n                \"image_url\": article.image_url,\n                \"local_image_path\": article.local_image_path,\n            }\n            for article in articles\n        ]\n    except Exception as e:\n        logger.error(f\"Error fetching content: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/content/pending\")\nasync def get_pending_content(db: Session = Depends(get_db)):\n    try:\n        content = db.query(ContentQueue).filter(\n            ContentQueue.status == \"pending_approval\"\n        ).order_by(ContentQueue.created_at.desc()).all()\n        return content\n    except Exception as e:\n        logger.error(f\"Error fetching pending content: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/content/history\")\nasync def get_content_history(\n    limit: int = 50,\n    status: Optional[str] = None,\n    db: Session = Depends(get_db)\n):\n    try:\n        query = db.query(ContentQueue)\n        \n        if status:\n            query = query.filter(ContentQueue.status == status)\n        \n        content = query.order_by(ContentQueue.created_at.desc()).limit(limit).all()\n        return content\n    except Exception as e:\n        logger.error(f\"Error fetching content history: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/content/{content_id}/approve\")\nasync def approve_content(\n    content_id: int,\n    request: ApproveRequest,\n    db: Session = Depends(get_db)\n):\n    try:\n        content = db.query(ContentQueue).filter(ContentQueue.id == content_id).first()\n        \n        if not content:\n            raise HTTPException(status_code=404, detail=\"Content not found\")\n        \n        if content.status != \"pending_approval\":\n            raise HTTPException(status_code=400, detail=\"Content is not pending approval\")\n        \n        content.status = \"approved\"\n        content.reviewed_at = datetime.utcnow()\n        content.reviewed_by = request.moderator\n        content.scheduled_post_time = request.scheduled_time or datetime.utcnow()\n        # Only update platforms if explicitly provided, otherwise keep original\n        if request.platforms is not None:\n            content.platforms = request.platforms\n        # Ensure platforms is set (fallback if somehow missing)\n        if not content.platforms:\n            content.platforms = ['facebook']  # Safe default\n        \n        log_entry = ApprovalLog(\n            content_id=content_id,\n            action=\"approved\",\n            moderator=request.moderator,\n            details={\"platforms\": content.platforms, \"scheduled_time\": str(request.scheduled_time)}\n        )\n        \n        db.add(log_entry)\n        db.commit()\n        db.refresh(content)\n        \n        notification_service.notify_content_approved({\n            'id': content_id,\n            'title': content.translated_title or (content.extra_metadata.get('title', '') if content.extra_metadata else ''),\n            'scheduled_time': str(request.scheduled_time) if request.scheduled_time else '–í—ñ–¥—Ä–∞–∑—É'\n        })\n        \n        logger.info(f\"Content {content_id} approved by {request.moderator}\")\n        \n        fb_result = None\n        if \"facebook\" in [p.lower() for p in request.platforms]:\n            post_data = {\n                'translated_title': content.translated_title or (content.extra_metadata.get('title', '') if content.extra_metadata else ''),\n                'translated_content': content.translated_text or '',\n                'url': content.source_url or '',\n                'source': content.source or 'The Spirits Business',\n                'author': (content.extra_metadata.get('author', '') if content.extra_metadata else ''),\n                'image_url': content.image_url,\n                'local_image_path': content.local_image_path\n            }\n            \n            fb_result = facebook_poster.post_with_image(post_data)\n            \n            if fb_result:\n                content.status = \"posted\"\n                \n                if not content.extra_metadata:\n                    content.extra_metadata = {}\n                content.extra_metadata['fb_post_id'] = fb_result['post_id']\n                content.extra_metadata['fb_post_url'] = fb_result['post_url']\n                \n                db.commit()\n                \n                notification_service.notify_content_posted({\n                    'id': content_id,\n                    'title': post_data['translated_title'],\n                    'platforms': ['Facebook'],\n                    'fb_post_url': fb_result['post_url'],\n                    'posted_at': fb_result['posted_at']\n                })\n                \n                logger.info(f\"Content {content_id} posted to Facebook: {fb_result['post_url']}\")\n        \n        if fb_result:\n            return {\n                \"status\": \"success\",\n                \"message\": \"Content approved and posted to Facebook\",\n                \"content\": content,\n                \"fb_post_url\": fb_result['post_url']\n            }\n        else:\n            return {\"message\": \"Content approved successfully\", \"content\": content}\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Error approving content: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/content/{content_id}/reject\")\nasync def reject_content(\n    content_id: int,\n    request: RejectRequest,\n    db: Session = Depends(get_db)\n):\n    try:\n        content = db.query(ContentQueue).filter(ContentQueue.id == content_id).first()\n        \n        if not content:\n            raise HTTPException(status_code=404, detail=\"Content not found\")\n        \n        content.status = \"rejected\"\n        content.reviewed_at = datetime.utcnow()\n        content.reviewed_by = request.moderator\n        content.rejection_reason = request.reason\n        \n        log_entry = ApprovalLog(\n            content_id=content_id,\n            action=\"rejected\",\n            moderator=request.moderator,\n            details={\"reason\": request.reason}\n        )\n        \n        db.add(log_entry)\n        db.commit()\n        db.refresh(content)\n        \n        logger.info(f\"Content {content_id} rejected by {request.moderator}\")\n        \n        return {\"message\": \"Content rejected successfully\", \"content\": content}\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Error rejecting content: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.put(\"/api/content/{content_id}/edit\")\nasync def edit_content(\n    content_id: int,\n    request: EditRequest,\n    db: Session = Depends(get_db)\n):\n    try:\n        content = db.query(ContentQueue).filter(ContentQueue.id == content_id).first()\n        \n        if not content:\n            raise HTTPException(status_code=404, detail=\"Content not found\")\n        \n        changes = {}\n        \n        if request.translated_text:\n            old_translation = content.translated_text\n            content.translated_text = request.translated_text\n            changes[\"translated_text\"] = {\"old\": old_translation, \"new\": request.translated_text}\n        \n        if request.image_prompt:\n            new_image_url = await image_generator.generate_image(request.image_prompt)\n            old_image = content.image_url\n            content.image_url = new_image_url\n            content.image_prompt = request.image_prompt\n            changes[\"image\"] = {\"old\": old_image, \"new\": new_image_url}\n        \n        if request.platforms:\n            old_platforms = content.platforms\n            content.platforms = request.platforms\n            changes[\"platforms\"] = {\"old\": old_platforms, \"new\": request.platforms}\n        \n        if content.edit_history:\n            content.edit_history.append({\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"changes\": changes\n            })\n        else:\n            content.edit_history = [{\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"changes\": changes\n            }]\n        \n        log_entry = ApprovalLog(\n            content_id=content_id,\n            action=\"edited\",\n            moderator=\"system\",\n            details=changes\n        )\n        \n        db.add(log_entry)\n        db.commit()\n        db.refresh(content)\n        \n        logger.info(f\"Content {content_id} edited\")\n        \n        return {\"message\": \"Content updated successfully\", \"content\": content}\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Error editing content: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/content/create\")\nasync def create_content(\n    request: CreateContentRequest,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Create Ukrainian content manually (bypasses translation)\n    Useful for adding Ukrainian news sources that don't need translation\n    \"\"\"\n    try:\n        new_content = ContentQueue(\n            status='draft',\n            source=request.source,\n            source_url=request.source_url,\n            original_text=request.content,\n            language=request.language,\n            needs_translation=request.needs_translation,\n            platforms=request.platforms,\n            extra_metadata={\n                'title': request.title,\n                'created_via': 'api',\n                'created_at': datetime.utcnow().isoformat()\n            }\n        )\n        \n        db.add(new_content)\n        db.commit()\n        db.refresh(new_content)\n        \n        logger.info(f\"Ukrainian content created: ID {new_content.id}, language={request.language}, needs_translation={request.needs_translation}\")\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Ukrainian content created successfully\",\n            \"content_id\": new_content.id,\n            \"language\": request.language,\n            \"needs_translation\": request.needs_translation,\n            \"next_steps\": \"Content will be processed for images and sent for approval\"\n        }\n        \n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Error creating Ukrainian content: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/content/stats\")\nasync def get_stats(db: Session = Depends(get_db)):\n    try:\n        pending = db.query(ContentQueue).filter(ContentQueue.status == \"pending_approval\").count()\n        approved = db.query(ContentQueue).filter(ContentQueue.status == \"approved\").count()\n        posted = db.query(ContentQueue).filter(ContentQueue.status == \"posted\").count()\n        rejected = db.query(ContentQueue).filter(ContentQueue.status == \"rejected\").count()\n        \n        return {\n            \"pending\": pending,\n            \"approved\": approved,\n            \"posted\": posted,\n            \"rejected\": rejected\n        }\n    except Exception as e:\n        logger.error(f\"Error fetching stats: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/scraper/test\")\nasync def test_scraper():\n    \"\"\"Test the news scraper with 1 article\"\"\"\n    try:\n        logger.info(\"Testing news scraper with 1 article...\")\n        articles = news_scraper.scrape_spirits_business(limit=1)\n        \n        return {\n            \"success\": True,\n            \"message\": f\"Successfully scraped {len(articles)} article(s)\",\n            \"articles\": articles\n        }\n    except Exception as e:\n        logger.error(f\"Scraper test error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/scraper/run\")\nasync def run_scraper(limit: int = 5, db: Session = Depends(get_db)):\n    \"\"\"\n    Manually trigger the news scraper.\n    Scrapes articles and adds them to the content queue for approval.\n    \"\"\"\n    try:\n        logger.info(f\"Running news scraper (limit: {limit})...\")\n        \n        articles = news_scraper.scrape_spirits_business(limit=limit)\n        \n        created_count = 0\n        for article in articles:\n            try:\n                existing = db.query(ContentQueue).filter(\n                    ContentQueue.source_url == article['url']\n                ).first()\n                \n                if existing:\n                    logger.info(f\"Article already exists: {article['title'][:50]}...\")\n                    continue\n                \n                content_entry = ContentQueue(\n                    status=\"draft\",\n                    source=\"The Spirits Business\",\n                    source_url=article['url'],\n                    original_text=article.get('content', ''),\n                    translated_text=None,\n                    image_url=article.get('image_url'),\n                    platforms=[\"linkedin\"],  # The Spirits Business is for LinkedIn\n                    extra_metadata={\n                        \"title\": article.get('title'),\n                        \"published_date\": article.get('published_date'),\n                        \"author\": article.get('author'),\n                        \"scraped_at\": article.get('scraped_at')\n                    }\n                )\n                \n                db.add(content_entry)\n                created_count += 1\n                \n            except Exception as e:\n                logger.error(f\"Error saving article: {str(e)}\")\n                continue\n        \n        db.commit()\n        \n        logger.info(f\"Scraper completed: {len(articles)} scraped, {created_count} new articles added\")\n        \n        return {\n            \"success\": True,\n            \"message\": f\"Scraper completed successfully\",\n            \"scraped\": len(articles),\n            \"new_articles\": created_count,\n            \"articles\": articles\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Scraper error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/translate/article/{article_id}\")\nasync def translate_article_endpoint(article_id: int, db: Session = Depends(get_db)):\n    \"\"\"\n    Translate a single article by ID and send Telegram notification\n    Updates the article's translated_text and status\n    \"\"\"\n    try:\n        article = db.query(ContentQueue).filter(ContentQueue.id == article_id).first()\n        \n        if not article:\n            raise HTTPException(status_code=404, detail=\"Article not found\")\n        \n        original_text = article.original_text or ''\n        \n        article_data = {\n            'id': article_id,\n            'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n            'content': original_text,\n            'summary': original_text[:1000] if original_text else ''\n        }\n        \n        logger.info(f\"Translating article {article_id}: {article_data['title'][:50]}...\")\n        \n        translation, notification_sent = translation_service.translate_article_with_notification(\n            article_data,\n            article_id,\n            article.image_url\n        )\n        \n        if translation and translation.get('title') and translation.get('content'):\n            article.translated_title = translation['title']\n            article.translated_text = translation['content']\n            article.status = 'pending_approval'\n            db.commit()\n            \n            logger.info(f\"Article {article_id} translated successfully\")\n            \n            return {\n                \"status\": \"success\",\n                \"article_id\": article_id,\n                \"notification_sent\": notification_sent,\n                \"translated_title\": translation['title'],\n                \"translated_content_length\": len(translation['content']),\n                \"preview\": translation['content'][:200] + \"...\"\n            }\n        else:\n            raise HTTPException(status_code=500, detail=\"Translation failed\")\n            \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Translation error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/translate/pending\")\nasync def translate_pending_articles(limit: int = 10, db: Session = Depends(get_db)):\n    \"\"\"\n    Translate all articles with status='draft' (notifications sent later with images)\n    \"\"\"\n    try:\n        draft_articles = db.query(ContentQueue).filter(\n            ContentQueue.status == 'draft',\n            ContentQueue.translated_text == None\n        ).limit(limit).all()\n        \n        if not draft_articles:\n            return {\n                \"status\": \"success\",\n                \"message\": \"No articles to translate\",\n                \"translated_count\": 0,\n                \"total_draft\": 0\n            }\n        \n        logger.info(f\"Translating {len(draft_articles)} draft articles...\")\n        \n        translated_count = 0\n        \n        for article in draft_articles:\n            original_text = article.original_text or ''\n            \n            article_data = {\n                'id': article.id,\n                'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n                'content': original_text,\n                'summary': original_text[:1000] if original_text else ''\n            }\n            \n            try:\n                translation = await claude_service.translate_to_ukrainian(original_text)\n                \n                if translation:\n                    article.translated_title = article_data['title']\n                    article.translated_text = translation\n                    article.status = 'pending_approval'\n                    translated_count += 1\n                    logger.info(f\"Translated article {article.id}: {article_data['title'][:50]}...\")\n            except Exception as e:\n                logger.error(f\"Failed to translate article {article.id}: {e}\")\n                continue\n        \n        db.commit()\n        \n        logger.info(f\"Translation completed: {translated_count}/{len(draft_articles)} articles translated\")\n        \n        return {\n            \"status\": \"success\",\n            \"translated_count\": translated_count,\n            \"total_draft\": len(draft_articles),\n            \"message\": f\"Translated {translated_count} articles. Run image generation next to send notifications.\"\n        }\n        \n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Batch translation error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/images/generate/{article_id}\")\nasync def generate_image_for_article(article_id: int, db: Session = Depends(get_db)):\n    \"\"\"\n    Generate image for a specific article\n    Updates article with image_url and image_prompt\n    \"\"\"\n    try:\n        article = db.query(ContentQueue).filter(ContentQueue.id == article_id).first()\n        \n        if not article:\n            raise HTTPException(status_code=404, detail=\"Article not found\")\n        \n        article_data = {\n            'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n            'content': article.original_text or article.translated_text or ''\n        }\n        \n        logger.info(f\"Generating image for article {article_id}: {article_data['title'][:50]}...\")\n        \n        result = image_generator.generate_article_image(article_data)\n        \n        if result.get('image_url'):\n            article.image_url = result['image_url']\n            article.image_prompt = result['prompt']\n            article.local_image_path = result.get('local_path', '')\n            db.commit()\n            \n            logger.info(f\"Image generated successfully for article {article_id}\")\n            \n            return {\n                \"status\": \"success\",\n                \"article_id\": article_id,\n                \"image_url\": result['image_url'],\n                \"prompt\": result['prompt']\n            }\n        else:\n            raise HTTPException(status_code=500, detail=\"Image generation failed\")\n            \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Error generating image: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/images/regenerate/{article_id}\")\nasync def regenerate_image(\n    article_id: int,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Regenerate image for article (generates new prompt and new image)\n    \"\"\"\n    try:\n        article = db.query(ContentQueue).filter(ContentQueue.id == article_id).first()\n        \n        if not article:\n            raise HTTPException(status_code=404, detail=\"Article not found\")\n        \n        article_data = {\n            'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n            'content': article.original_text or article.translated_text or ''\n        }\n        \n        logger.info(f\"Regenerating image for article {article_id}\")\n        \n        result = image_generator.generate_article_image(article_data)\n        \n        if result.get('image_url'):\n            article.image_url = result['image_url']\n            article.image_prompt = result['prompt']\n            article.local_image_path = result.get('local_path', '')\n            db.commit()\n            \n            logger.info(f\"Image regenerated for article {article_id}\")\n            \n            return {\n                \"status\": \"success\",\n                \"article_id\": article_id,\n                \"image_url\": result['image_url'],\n                \"prompt\": result['prompt']\n            }\n        else:\n            raise HTTPException(status_code=500, detail=\"Image regeneration failed\")\n            \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Error regenerating image: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/images/generate-pending\")\nasync def generate_images_for_pending(limit: int = 10, db: Session = Depends(get_db)):\n    \"\"\"\n    Generate images AND send Telegram notifications with image previews\n    \"\"\"\n    try:\n        articles_without_images = db.query(ContentQueue).filter(\n            ContentQueue.status == 'pending_approval',\n            ContentQueue.image_url == None\n        ).limit(limit).all()\n        \n        if not articles_without_images:\n            return {\n                \"status\": \"success\",\n                \"message\": \"No articles need images\",\n                \"generated_count\": 0,\n                \"total_without_images\": 0,\n                \"notifications_sent\": 0\n            }\n        \n        logger.info(f\"Generating images for {len(articles_without_images)} articles...\")\n        \n        generated_count = 0\n        notifications_sent = 0\n        \n        for article in articles_without_images:\n            article_data = {\n                'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n                'content': article.original_text or article.translated_text or ''\n            }\n            \n            try:\n                result = image_generator.generate_article_image(article_data)\n                \n                if result.get('image_url'):\n                    article.image_url = result['image_url']\n                    article.image_prompt = result['prompt']\n                    article.local_image_path = result.get('local_path', '')\n                    generated_count += 1\n                    \n                    logger.info(f\"Generated image for article {article.id}\")\n                    \n                    notification_data = {\n                        'id': article.id,\n                        'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n                        'translated_text': article.translated_text or article.original_text or '',\n                        'image_url': article.image_url,\n                        'source': article.source or 'The Spirits Business',\n                        'created_at': article.created_at.strftime('%Y-%m-%d %H:%M') if article.created_at else ''\n                    }\n                    \n                    try:\n                        notification_service.send_approval_notification(notification_data)\n                        notifications_sent += 1\n                        logger.info(f\"‚úÖ Notification with image sent for article {article.id}\")\n                    except Exception as notif_error:\n                        logger.error(f\"Failed to send notification for article {article.id}: {notif_error}\")\n                        \n            except Exception as e:\n                logger.error(f\"Failed to generate image for article {article.id}: {e}\")\n                continue\n        \n        db.commit()\n        \n        logger.info(f\"Image generation completed: {generated_count}/{len(articles_without_images)}, notifications sent: {notifications_sent}\")\n        \n        return {\n            \"status\": \"success\",\n            \"generated_count\": generated_count,\n            \"total_without_images\": len(articles_without_images),\n            \"notifications_sent\": notifications_sent,\n            \"message\": f\"Generated {generated_count} images and sent {notifications_sent} Telegram notifications\"\n        }\n        \n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Batch image generation error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/images/migrate-local-storage\")\nasync def migrate_images_to_local_storage(db: Session = Depends(get_db)):\n    \"\"\"\n    Retroactively download existing DALL-E images and save them locally.\n    This fixes articles that have image_url but no local_image_path.\n    \"\"\"\n    try:\n        articles_need_migration = db.query(ContentQueue).filter(\n            ContentQueue.image_url != None,\n            ContentQueue.local_image_path == None\n        ).all()\n        \n        if not articles_need_migration:\n            return {\n                \"status\": \"success\",\n                \"message\": \"No articles need migration\",\n                \"migrated_count\": 0\n            }\n        \n        logger.info(f\"Migrating {len(articles_need_migration)} images to local storage...\")\n        \n        migrated_count = 0\n        failed_count = 0\n        \n        for article in articles_need_migration:\n            try:\n                logger.info(f\"Downloading image for article {article.id}...\")\n                local_path = image_generator.download_and_save_image(article.image_url)\n                \n                if local_path:\n                    article.local_image_path = local_path\n                    migrated_count += 1\n                    logger.info(f\"‚úÖ Article {article.id}: Saved to {local_path}\")\n                else:\n                    failed_count += 1\n                    logger.warning(f\"‚ö†Ô∏è Article {article.id}: Download failed (URL may be expired)\")\n                    \n            except Exception as e:\n                failed_count += 1\n                logger.error(f\"‚ùå Article {article.id}: Migration error: {e}\")\n                continue\n        \n        db.commit()\n        \n        logger.info(f\"Migration completed: {migrated_count} succeeded, {failed_count} failed\")\n        \n        return {\n            \"status\": \"success\",\n            \"total_articles\": len(articles_need_migration),\n            \"migrated_count\": migrated_count,\n            \"failed_count\": failed_count,\n            \"message\": f\"Migrated {migrated_count}/{len(articles_need_migration)} images to local storage\"\n        }\n        \n    except Exception as e:\n        db.rollback()\n        logger.error(f\"Migration error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/facebook/test\")\nasync def test_facebook_connection():\n    \"\"\"Test Facebook API connection\"\"\"\n    try:\n        if facebook_poster.verify_token():\n            return {\n                \"status\": \"success\",\n                \"message\": \"Facebook token is valid\",\n                \"page_id\": facebook_poster.page_id\n            }\n        else:\n            raise HTTPException(status_code=500, detail=\"Facebook token invalid\")\n    except Exception as e:\n        logger.error(f\"Facebook test error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/facebook/post-test\")\nasync def test_facebook_post():\n    \"\"\"Post a test message to Facebook\"\"\"\n    try:\n        test_data = {\n            'translated_title': 'üß™ –¢–µ—Å—Ç–æ–≤–∏–π –ø–æ—Å—Ç –≤—ñ–¥ Gradus AI',\n            'translated_content': '–¶–µ —Ç–µ—Å—Ç–æ–≤–∏–π –ø–æ—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –∑ Facebook. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –ø–æ—Å—Ç–∏–Ω–≥—É –ø—Ä–∞—Ü—é—î –∫–æ—Ä–µ–∫—Ç–Ω–æ!',\n            'url': 'https://www.thespiritsbusiness.com/',\n            'source': 'Gradus AI Bot',\n            'author': 'Test System',\n            'image_url': None\n        }\n        \n        result = facebook_poster.post_with_image(test_data)\n        \n        if result:\n            return {\n                \"status\": \"success\",\n                \"message\": \"Test post created!\",\n                \"post_url\": result['post_url'],\n                \"post_id\": result['post_id']\n            }\n        else:\n            raise HTTPException(status_code=500, detail=\"Failed to create test post\")\n    except Exception as e:\n        logger.error(f\"Facebook post test error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/facebook/token-status\")\nasync def check_facebook_token():\n    \"\"\"Check Facebook token expiration status\"\"\"\n    from services.facebook_token_manager import facebook_token_manager\n    \n    status = facebook_token_manager.check_token_expiration()\n    \n    return status\n\n@app.get(\"/api/monitor/all\")\nasync def monitor_all_api_services():\n    \"\"\"\n    Comprehensive monitoring of all API services\n    Returns health status, quotas, and expiration info\n    \"\"\"\n    try:\n        results = api_token_monitor.check_all_services()\n        return results\n    except Exception as e:\n        logger.error(f\"API monitoring error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/monitor/anthropic\")\nasync def monitor_anthropic():\n    \"\"\"Check Claude/Anthropic API status\"\"\"\n    try:\n        result = api_token_monitor.check_anthropic_api()\n        return result\n    except Exception as e:\n        logger.error(f\"Anthropic monitoring error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/monitor/openai\")\nasync def monitor_openai():\n    \"\"\"Check OpenAI/DALL-E API status\"\"\"\n    try:\n        result = api_token_monitor.check_openai_api()\n        return result\n    except Exception as e:\n        logger.error(f\"OpenAI monitoring error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/monitor/test-alerts\")\nasync def test_monitoring_alerts():\n    \"\"\"\n    Manually trigger API monitoring check and send alerts if needed\n    Useful for testing notification system\n    \"\"\"\n    try:\n        results = api_token_monitor.check_all_services()\n        \n        if not results.get('warnings') and not results.get('errors'):\n            api_token_monitor.send_success_notification(results)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Monitoring check completed\",\n            \"results\": results\n        }\n    except Exception as e:\n        logger.error(f\"Monitoring test error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/facebook/token-alert\")\nasync def send_token_alert():\n    \"\"\"Manually trigger token expiration alert\"\"\"\n    from services.facebook_token_manager import facebook_token_manager\n    \n    status = facebook_token_manager.check_token_expiration()\n    \n    if status.get('days_remaining'):\n        facebook_token_manager.send_expiration_alert(status['days_remaining'])\n        return {\"status\": \"success\", \"message\": \"Alert sent\", \"days_remaining\": status['days_remaining']}\n    \n    return {\"status\": \"info\", \"message\": \"Token never expires or invalid\", \"token_status\": status}\n\n@app.get(\"/api/analytics/post/{content_id}\")\nasync def get_post_analytics(content_id: int, db: Session = Depends(get_db)):\n    \"\"\"Get analytics for specific post by content ID\"\"\"\n    from services.analytics_tracker import analytics_tracker\n    \n    article = db.query(ContentQueue).filter(ContentQueue.id == content_id).first()\n    \n    if not article or not article.extra_metadata or 'fb_post_id' not in article.extra_metadata:\n        raise HTTPException(status_code=404, detail=\"Post not found or not posted to Facebook\")\n    \n    post_id = article.extra_metadata['fb_post_id']\n    metrics = analytics_tracker.get_post_insights(post_id)\n    \n    if 'error' not in metrics:\n        if not article.extra_metadata:\n            article.extra_metadata = {}\n        article.extra_metadata['analytics'] = metrics\n        db.commit()\n    \n    return {\n        \"content_id\": content_id,\n        \"title\": article.translated_title or 'Untitled',\n        \"post_url\": article.extra_metadata.get('fb_post_url', ''),\n        \"metrics\": metrics\n    }\n\n@app.get(\"/api/analytics/recent\")\nasync def get_recent_posts_analytics(limit: int = 10):\n    \"\"\"Get performance metrics for recent posts\"\"\"\n    from services.analytics_tracker import analytics_tracker\n    \n    results = analytics_tracker.get_recent_posts_performance(limit=limit)\n    \n    return {\n        \"posts\": results,\n        \"count\": len(results)\n    }\n\n@app.get(\"/api/analytics/insights\")\nasync def get_posting_insights(days: int = 30):\n    \"\"\"Get best posting times and engagement insights\"\"\"\n    from services.analytics_tracker import analytics_tracker\n    \n    insights = analytics_tracker.get_best_posting_times(days=days)\n    \n    return insights\n\n@app.get(\"/api/scheduler/status\")\nasync def get_scheduler_status():\n    \"\"\"Get scheduler status and upcoming jobs\"\"\"\n    jobs = content_scheduler.get_jobs()\n    \n    return {\n        \"status\": \"running\",\n        \"scheduler_active\": content_scheduler.scheduler.running,\n        \"jobs\": jobs,\n        \"total_jobs\": len(jobs)\n    }\n\n@app.post(\"/api/telegram/webhook\")\nasync def telegram_webhook(request: Request, db: Session = Depends(get_db)):\n    \"\"\"\n    Telegram webhook endpoint to handle inline button callbacks\n    \n    Receives updates from Telegram Bot API when users click inline buttons\n    \n    Security: Set TELEGRAM_WEBHOOK_SECRET environment variable and configure webhook:\n    https://api.telegram.org/bot<TOKEN>/setWebhook?url=<URL>&secret_token=<SECRET>\n    \"\"\"\n    try:\n        telegram_secret = os.getenv('TELEGRAM_WEBHOOK_SECRET')\n        if telegram_secret:\n            provided_secret = request.headers.get('X-Telegram-Bot-Api-Secret-Token')\n            if provided_secret != telegram_secret:\n                logger.warning(\"Telegram webhook: Invalid or missing secret token\")\n                raise HTTPException(status_code=403, detail=\"Forbidden\")\n        \n        payload = await request.json()\n        \n        if 'callback_query' in payload:\n            result = telegram_webhook_handler.handle_callback_query(\n                payload['callback_query'],\n                db\n            )\n            return result\n        \n        return {\"status\": \"ok\", \"message\": \"No callback_query found\"}\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Telegram webhook error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/telegram/set-webhook\")\nasync def set_telegram_webhook():\n    \"\"\"\n    Set Telegram webhook URL\n    Call this once to configure the webhook\n    \n    Optional: Set TELEGRAM_WEBHOOK_SECRET in Secrets for webhook authentication\n    \"\"\"\n    import requests\n    \n    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n    \n    if not bot_token:\n        raise HTTPException(status_code=500, detail=\"TELEGRAM_BOT_TOKEN not configured\")\n    \n    replit_domains = os.getenv('REPLIT_DOMAINS')\n    if not replit_domains:\n        replit_domains = os.getenv('REPLIT_DEV_DOMAIN')\n    \n    if not replit_domains:\n        raise HTTPException(\n            status_code=500, \n            detail=\"Could not determine app URL. Make sure app is running on Replit.\"\n        )\n    \n    app_url = f\"https://{replit_domains.split(',')[0]}\"\n    webhook_url = f\"{app_url}/api/telegram/webhook\"\n    \n    telegram_secret = os.getenv('TELEGRAM_WEBHOOK_SECRET')\n    \n    url = f\"https://api.telegram.org/bot{bot_token}/setWebhook\"\n    payload = {\"url\": webhook_url}\n    \n    if telegram_secret:\n        payload[\"secret_token\"] = telegram_secret\n    \n    try:\n        response = requests.post(url, json=payload, timeout=10)\n        result = response.json()\n        \n        if result.get('ok'):\n            return {\n                \"status\": \"success\",\n                \"message\": \"Webhook set successfully\",\n                \"webhook_url\": webhook_url,\n                \"secured\": bool(telegram_secret),\n                \"telegram_response\": result\n            }\n        else:\n            return {\n                \"status\": \"error\",\n                \"message\": result.get('description', 'Unknown error'),\n                \"details\": result\n            }\n    except Exception as e:\n        logger.error(f\"Failed to set webhook: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/telegram/webhook-info\")\nasync def get_webhook_info():\n    \"\"\"Get current webhook information from Telegram\"\"\"\n    import requests\n    \n    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n    \n    if not bot_token:\n        raise HTTPException(status_code=500, detail=\"TELEGRAM_BOT_TOKEN not configured\")\n    \n    url = f\"https://api.telegram.org/bot{bot_token}/getWebhookInfo\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        result = response.json()\n        \n        if result.get('ok'):\n            webhook_info = result.get('result', {})\n            return {\n                \"status\": \"success\",\n                \"webhook_info\": webhook_info,\n                \"is_configured\": bool(webhook_info.get('url')),\n                \"pending_updates\": webhook_info.get('pending_update_count', 0)\n            }\n        else:\n            raise HTTPException(\n                status_code=500, \n                detail=result.get('description', 'Failed to get webhook info')\n            )\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to get webhook info: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","path":null,"size_bytes":44664,"size_tokens":null},"backend/services/facebook_poster.py":{"content":"import os\nimport requests\nfrom typing import Dict, Optional\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass FacebookPoster:\n    def __init__(self):\n        self.page_access_token = os.getenv('FACEBOOK_PAGE_ACCESS_TOKEN')\n        self.page_id = os.getenv('FACEBOOK_PAGE_ID')\n        self.graph_api_version = 'v18.0'\n        self.base_url = f'https://graph.facebook.com/{self.graph_api_version}'\n    \n    def format_post_text(self, article_data: Dict) -> str:\n        \"\"\"\n        Format post text for Facebook with FULL Ukrainian content\n        \n        Args:\n            article_data: Dict with title, content, source, author\n            \n        Returns:\n            Formatted post text\n        \"\"\"\n        title = article_data.get('translated_title', '')\n        content = article_data.get('translated_content', '')\n        source = article_data.get('source', 'The Spirits Business')\n        author = article_data.get('author', '')\n        \n        post_text = f\"\"\"üì∞ {title}\n\n{content}\n\nüì∞ {source}\"\"\"\n        \n        if author:\n            post_text += f\"\\n‚úçÔ∏è {author}\"\n        \n        return post_text\n    \n    def post_with_image(self, article_data: Dict) -> Optional[Dict]:\n        \"\"\"\n        Post to Facebook Page with image\n        \n        Args:\n            article_data: Dict with all article info including image_url or local_image_path\n            \n        Returns:\n            Dict with post_id and post_url, or None if failed\n        \"\"\"\n        if not self.page_access_token or not self.page_id:\n            logger.error(\"Facebook credentials not configured\")\n            return None\n        \n        message = self.format_post_text(article_data)\n        local_image_path = article_data.get('local_image_path')\n        image_url = article_data.get('image_url')\n        \n        # Prefer local image file over URL (URL may be expired)\n        # Try both absolute and relative paths to handle different path formats\n        found_path = None\n        if local_image_path:\n            # Try absolute path first\n            if os.path.exists(local_image_path):\n                found_path = local_image_path\n                logger.info(f\"‚úÖ Using local image (absolute): {local_image_path}\")\n            # Try relative to current working directory\n            elif os.path.exists(os.path.join(os.getcwd(), local_image_path)):\n                found_path = os.path.join(os.getcwd(), local_image_path)\n                logger.info(f\"‚úÖ Using local image (relative): {found_path}\")\n            else:\n                logger.warning(f\"‚ö†Ô∏è  Local image path not found: {local_image_path}\")\n        \n        if found_path:\n            return self._post_with_local_image(message, found_path)\n        elif image_url:\n            logger.warning(\"‚ö†Ô∏è  No local image, trying URL (may expire)\")\n            return self._post_with_image_url(message, image_url)\n        else:\n            logger.warning(\"No image provided, posting text only\")\n            return self.post_text_only(message)\n    \n    def _post_with_local_image(self, message: str, local_image_path: str) -> Optional[Dict]:\n        \"\"\"Post to Facebook using local image file (prevents expiration issues)\"\"\"\n        url = f\"{self.base_url}/{self.page_id}/photos\"\n        \n        try:\n            with open(local_image_path, 'rb') as image_file:\n                files = {'source': image_file}\n                data = {\n                    'message': message,\n                    'access_token': self.page_access_token\n                }\n                \n                logger.info(f\"Posting to Facebook with local image file...\")\n                response = requests.post(url, files=files, data=data, timeout=30)\n                result = response.json()\n                \n                return self._parse_facebook_response(result)\n                \n        except Exception as e:\n            logger.error(f\"Failed to post with local image: {e}\")\n            logger.exception(\"Full traceback:\")\n            return None\n    \n    def _post_with_image_url(self, message: str, image_url: str) -> Optional[Dict]:\n        \"\"\"Post to Facebook using image URL (may fail if URL expired)\"\"\"\n        url = f\"{self.base_url}/{self.page_id}/photos\"\n        \n        payload = {\n            'message': message,\n            'url': image_url,\n            'access_token': self.page_access_token\n        }\n        \n        try:\n            logger.info(f\"Posting to Facebook with image URL: {image_url[:100]}...\")\n            response = requests.post(url, data=payload, timeout=30)\n            result = response.json()\n            \n            return self._parse_facebook_response(result)\n                \n        except Exception as e:\n            logger.error(f\"Failed to post with image URL: {e}\")\n            logger.exception(\"Full traceback:\")\n            return None\n    \n    def _parse_facebook_response(self, result: Dict) -> Optional[Dict]:\n        \"\"\"Parse Facebook API response and return post data\"\"\"\n        logger.info(f\"Facebook API response: {result}\")\n        \n        if 'id' in result:\n            post_id = result['id']\n            \n            try:\n                if '_' in post_id:\n                    post_number = post_id.split('_')[1]\n                else:\n                    post_number = post_id\n                \n                post_url = f\"https://www.facebook.com/{self.page_id}/posts/{post_number}\"\n            except Exception as parse_error:\n                logger.error(f\"Error parsing post_id '{post_id}': {parse_error}\")\n                post_url = f\"https://www.facebook.com/{self.page_id}\"\n            \n            logger.info(f\"Posted to Facebook successfully: {post_id}\")\n            \n            return {\n                'post_id': post_id,\n                'post_url': post_url,\n                'posted_at': datetime.now().isoformat()\n            }\n        else:\n            logger.error(f\"Facebook API error: {result}\")\n            \n            if 'error' in result:\n                error = result['error']\n                logger.error(f\"Facebook error code {error.get('code')}: {error.get('message')}\")\n                \n                if error.get('code') == 190:\n                    logger.error(\"Facebook access token is invalid or expired!\")\n                elif error.get('code') == 324:\n                    logger.error(\"Image file missing or invalid - URL may have expired!\")\n            \n            return None\n    \n    def post_text_only(self, message: str) -> Optional[Dict]:\n        \"\"\"\n        Post text-only to Facebook Page (fallback if no image)\n        \"\"\"\n        url = f\"{self.base_url}/{self.page_id}/feed\"\n        \n        payload = {\n            'message': message,\n            'access_token': self.page_access_token\n        }\n        \n        try:\n            logger.info(f\"Posting text-only to Facebook...\")\n            response = requests.post(url, data=payload, timeout=30)\n            result = response.json()\n            \n            logger.info(f\"Facebook API response: {result}\")\n            \n            if 'id' in result:\n                post_id = result['id']\n                \n                try:\n                    if '_' in post_id:\n                        post_number = post_id.split('_')[1]\n                    else:\n                        post_number = post_id\n                    \n                    post_url = f\"https://www.facebook.com/{self.page_id}/posts/{post_number}\"\n                except Exception as parse_error:\n                    logger.error(f\"Error parsing post_id '{post_id}': {parse_error}\")\n                    post_url = f\"https://www.facebook.com/{self.page_id}\"\n                \n                logger.info(f\"Posted text to Facebook successfully: {post_id}\")\n                \n                return {\n                    'post_id': post_id,\n                    'post_url': post_url,\n                    'posted_at': datetime.now().isoformat()\n                }\n            else:\n                logger.error(f\"Facebook API error: {result}\")\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Failed to post to Facebook: {e}\")\n            logger.exception(\"Full traceback:\")\n            return None\n    \n    def verify_token(self) -> bool:\n        \"\"\"\n        Verify that the access token is valid\n        \"\"\"\n        if not self.page_access_token:\n            return False\n        \n        url = f\"{self.base_url}/me\"\n        params = {'access_token': self.page_access_token}\n        \n        try:\n            response = requests.get(url, params=params, timeout=10)\n            result = response.json()\n            \n            if 'id' in result:\n                logger.info(f\"Facebook token valid for page: {result.get('name')}\")\n                return True\n            else:\n                logger.error(f\"Invalid token: {result}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Token verification failed: {e}\")\n            return False\n\nfacebook_poster = FacebookPoster()\n","path":null,"size_bytes":9089,"size_tokens":null},"frontend/vite.config.js":{"content":"import { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\nimport path from 'path'\n\nexport default defineConfig({\n  plugins: [react()],\n  server: {\n    host: '0.0.0.0',\n    port: 5000,\n    allowedHosts: true,\n    strictPort: true,\n    proxy: {\n      '/api': {\n        target: 'http://localhost:8000',\n        changeOrigin: true\n      }\n    }\n  },\n  resolve: {\n    alias: {\n      '@': path.resolve(__dirname, './src'),\n    },\n  },\n})\n","path":null,"size_bytes":451,"size_tokens":null},"main.py":{"content":"def main():\n    print(\"Hello from repl-nix-workspace!\")\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":96,"size_tokens":null},"backend/migrations/add_language_support.py":{"content":"\"\"\"\nAdd language support to ContentQueue\nRun once to update existing database\n\"\"\"\n\nimport os\nimport sys\n\n# Add parent directory to path to import models\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom sqlalchemy import text\nimport models\n\ndef migrate():\n    \"\"\"Add language and needs_translation columns to content_queue table\"\"\"\n    \n    # Initialize database connection\n    models.init_db()\n    db = models.SessionLocal()\n    \n    try:\n        print(\"üîÑ Starting migration: Add language support...\")\n        \n        # Add language column\n        print(\"  Adding 'language' column...\")\n        db.execute(text(\"\"\"\n            ALTER TABLE content_queue \n            ADD COLUMN IF NOT EXISTS language VARCHAR(10) DEFAULT 'en'\n        \"\"\"))\n        \n        # Add needs_translation column\n        print(\"  Adding 'needs_translation' column...\")\n        db.execute(text(\"\"\"\n            ALTER TABLE content_queue \n            ADD COLUMN IF NOT EXISTS needs_translation BOOLEAN DEFAULT TRUE\n        \"\"\"))\n        \n        # Add posted_at column if it doesn't exist\n        print(\"  Adding 'posted_at' column...\")\n        db.execute(text(\"\"\"\n            ALTER TABLE content_queue \n            ADD COLUMN IF NOT EXISTS posted_at TIMESTAMP\n        \"\"\"))\n        \n        db.commit()\n        print(\"‚úÖ Migration completed successfully!\")\n        print(\"   - Added 'language' column (default: 'en')\")\n        print(\"   - Added 'needs_translation' column (default: TRUE)\")\n        print(\"   - Added 'posted_at' column\")\n        \n    except Exception as e:\n        print(f\"‚ùå Migration failed: {e}\")\n        db.rollback()\n        raise\n    finally:\n        db.close()\n\nif __name__ == \"__main__\":\n    migrate()\n","path":null,"size_bytes":1741,"size_tokens":null},"backend/services/claude_service.py":{"content":"import os\nfrom anthropic import Anthropic\nfrom typing import Optional\n\nclass ClaudeService:\n    def __init__(self):\n        self.client = None\n        self.model = \"claude-sonnet-4-5\"\n        self.api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n    \n    def _ensure_client(self):\n        \"\"\"Lazy initialization of Claude client.\"\"\"\n        if self.client is None:\n            if not self.api_key:\n                raise ValueError(\"ANTHROPIC_API_KEY environment variable is not set. Please configure it to use Claude AI features.\")\n            self.client = Anthropic(api_key=self.api_key)\n    \n    async def chat(self, message: str, system_prompt: Optional[str] = None) -> str:\n        \"\"\"\n        Send a chat message to Claude and get a response.\n        \"\"\"\n        self._ensure_client()\n        try:\n            response = self.client.messages.create(\n                model=self.model,\n                max_tokens=4096,\n                system=system_prompt if system_prompt else \"You are a helpful AI assistant.\",\n                messages=[{\"role\": \"user\", \"content\": message}]\n            )\n            \n            text_block = next((block for block in response.content if hasattr(block, 'text')), None)\n            if text_block and hasattr(text_block, 'text'):\n                return text_block.text\n            return \"\"\n        except Exception as e:\n            raise Exception(f\"Claude API error: {str(e)}\")\n    \n    async def translate_to_ukrainian(self, text: str) -> str:\n        \"\"\"\n        Translate English text to Ukrainian using Claude.\n        \"\"\"\n        system_prompt = \"\"\"You are a professional translator specializing in English to Ukrainian translation. \n        Your task is to translate the given text accurately while maintaining the original tone and meaning.\n        Focus on alcohol industry terminology and marketing language.\n        Return ONLY the translated text, nothing else.\"\"\"\n        \n        user_message = f\"Translate the following English text to Ukrainian:\\n\\n{text}\"\n        \n        try:\n            translation = await self.chat(user_message, system_prompt)\n            return translation.strip()\n        except Exception as e:\n            raise Exception(f\"Translation error: {str(e)}\")\n    \n    async def generate_image_prompt(self, article_text: str, title: str) -> str:\n        \"\"\"\n        Generate a DALL-E image prompt based on article content.\n        \"\"\"\n        system_prompt = \"\"\"You are an expert at creating detailed image generation prompts for DALL-E.\n        Create a visual, descriptive prompt for an image that would accompany an article about alcohol/spirits.\n        The prompt should be professional, eye-catching, and suitable for social media.\n        Return ONLY the prompt, nothing else.\"\"\"\n        \n        user_message = f\"Article title: {title}\\n\\nArticle excerpt: {article_text[:500]}\\n\\nCreate a DALL-E prompt:\"\n        \n        try:\n            prompt = await self.chat(user_message, system_prompt)\n            return prompt.strip()\n        except Exception as e:\n            raise Exception(f\"Image prompt generation error: {str(e)}\")\n\nclaude_service = ClaudeService()\n","path":null,"size_bytes":3140,"size_tokens":null},"frontend/src/lib/api.js":{"content":"import axios from 'axios'\n\nconst getApiUrl = () => {\n  if (typeof window === 'undefined') return '/api'\n  \n  return '/api'\n}\n\nexport const API_URL = getApiUrl()\n\nconst api = axios.create({\n  baseURL: API_URL,\n  headers: {\n    'Content-Type': 'application/json',\n  },\n})\n\nconsole.log('API URL configured (proxied):', API_URL)\n\nexport default api\n","path":null,"size_bytes":345,"size_tokens":null},"frontend/src/App.jsx":{"content":"import { BrowserRouter as Router, Routes, Route, Link } from 'react-router-dom'\nimport { Home, MessageSquare, FileText, Users, BarChart } from 'lucide-react'\nimport HomePage from './pages/HomePage'\nimport ChatPage from './pages/ChatPage'\nimport ContentApproval from './pages/ContentApproval'\n\nfunction App() {\n  return (\n    <Router>\n      <div className=\"min-h-screen bg-gray-50\">\n        <nav className=\"bg-purple-700 text-white shadow-lg\">\n          <div className=\"max-w-7xl mx-auto px-4\">\n            <div className=\"flex justify-between items-center h-16\">\n              <div className=\"flex items-center space-x-2\">\n                <BarChart className=\"h-8 w-8\" />\n                <h1 className=\"text-xl font-bold\">Gradus Media AI Agent</h1>\n              </div>\n              <div className=\"flex space-x-6\">\n                <Link to=\"/\" className=\"flex items-center space-x-1 hover:text-purple-200\">\n                  <Home size={20} />\n                  <span>Home</span>\n                </Link>\n                <Link to=\"/chat\" className=\"flex items-center space-x-1 hover:text-purple-200\">\n                  <MessageSquare size={20} />\n                  <span>Chat</span>\n                </Link>\n                <Link to=\"/content\" className=\"flex items-center space-x-1 hover:text-purple-200\">\n                  <FileText size={20} />\n                  <span>Content Approval</span>\n                </Link>\n              </div>\n            </div>\n          </div>\n        </nav>\n\n        <main className=\"max-w-7xl mx-auto px-4 py-8\">\n          <Routes>\n            <Route path=\"/\" element={<HomePage />} />\n            <Route path=\"/chat\" element={<ChatPage />} />\n            <Route path=\"/content\" element={<ContentApproval />} />\n          </Routes>\n        </main>\n      </div>\n    </Router>\n  )\n}\n\nexport default App\n","path":null,"size_bytes":1836,"size_tokens":null},"frontend/src/pages/ContentApproval.jsx":{"content":"import { useState, useEffect } from 'react'\nimport { CheckCircle, XCircle, Edit, Eye, Calendar } from 'lucide-react'\nimport api from '../lib/api'\n\nfunction ContentApproval() {\n  const [pendingContent, setPendingContent] = useState([])\n  const [stats, setStats] = useState({ pending: 0, approved: 0, posted: 0, rejected: 0 })\n  const [loading, setLoading] = useState(true)\n\n  useEffect(() => {\n    fetchData()\n  }, [])\n\n  const fetchData = async () => {\n    try {\n      const [contentRes, statsRes] = await Promise.all([\n        api.get('/content/pending'),\n        api.get('/content/stats')\n      ])\n      setPendingContent(contentRes.data)\n      setStats(statsRes.data)\n    } catch (error) {\n      console.error('Error fetching data:', error)\n    }\n    setLoading(false)\n  }\n\n  const handleApprove = async (contentId) => {\n    if (!window.confirm('‚úÖ Approve and post to Facebook?')) return\n    \n    try {\n      const response = await api.post(`/content/${contentId}/approve`, {\n        moderator: 'Admin',\n        platforms: ['facebook', 'linkedin']\n      })\n      \n      if (response.data.status === 'success' && response.data.fb_post_url) {\n        alert(`üéâ Posted to Facebook!\\n\\n${response.data.fb_post_url}`)\n      } else {\n        alert('‚úÖ Content approved successfully!')\n      }\n      \n      fetchData()\n    } catch (error) {\n      console.error('Error approving content:', error)\n      alert('‚ùå Failed to approve content')\n    }\n  }\n\n  const handleReject = async (contentId) => {\n    const reason = prompt('Reason for rejection:')\n    if (!reason) return\n\n    try {\n      await api.post(`/content/${contentId}/reject`, {\n        moderator: 'Admin',\n        reason: reason\n      })\n      fetchData()\n    } catch (error) {\n      console.error('Error rejecting content:', error)\n    }\n  }\n\n  const handleGenerateImage = async (contentId) => {\n    try {\n      console.log('Generating image for content', contentId)\n      await api.post(`/images/generate/${contentId}`)\n      alert('Image generated successfully!')\n      fetchData()\n    } catch (error) {\n      console.error('Error generating image:', error)\n      alert('Failed to generate image. Check console for details.')\n    }\n  }\n\n  const handleRegenerateImage = async (contentId) => {\n    try {\n      console.log('Regenerating image for content', contentId)\n      await api.post(`/images/regenerate/${contentId}`)\n      alert('Image regenerated successfully!')\n      fetchData()\n    } catch (error) {\n      console.error('Error regenerating image:', error)\n      alert('Failed to regenerate image. Check console for details.')\n    }\n  }\n\n  if (loading) {\n    return <div className=\"text-center py-8\">Loading...</div>\n  }\n\n  return (\n    <div>\n      <h1 className=\"text-3xl font-bold text-gray-900 mb-6\">Content Approval</h1>\n\n      <div className=\"grid grid-cols-1 md:grid-cols-4 gap-4 mb-8\">\n        <div className=\"bg-yellow-50 border border-yellow-200 rounded-lg p-4\">\n          <div className=\"text-sm text-yellow-700\">Pending</div>\n          <div className=\"text-2xl font-bold text-yellow-900\">{stats.pending}</div>\n        </div>\n        <div className=\"bg-green-50 border border-green-200 rounded-lg p-4\">\n          <div className=\"text-sm text-green-700\">Approved</div>\n          <div className=\"text-2xl font-bold text-green-900\">{stats.approved}</div>\n        </div>\n        <div className=\"bg-blue-50 border border-blue-200 rounded-lg p-4\">\n          <div className=\"text-sm text-blue-700\">Posted</div>\n          <div className=\"text-2xl font-bold text-blue-900\">{stats.posted}</div>\n        </div>\n        <div className=\"bg-red-50 border border-red-200 rounded-lg p-4\">\n          <div className=\"text-sm text-red-700\">Rejected</div>\n          <div className=\"text-2xl font-bold text-red-900\">{stats.rejected}</div>\n        </div>\n      </div>\n\n      {pendingContent.length === 0 ? (\n        <div className=\"bg-white rounded-lg shadow p-8 text-center\">\n          <p className=\"text-gray-500\">No content pending approval</p>\n          <p className=\"text-sm text-gray-400 mt-2\">New content from the news scraper will appear here</p>\n        </div>\n      ) : (\n        <div className=\"space-y-4\">\n          {pendingContent.map((content) => (\n            <div key={content.id} className=\"bg-white rounded-lg shadow p-6\">\n              <div className=\"flex justify-between items-start mb-4\">\n                <div>\n                  <h3 className=\"text-xl font-bold text-gray-900 mb-2\">\n                    {content.translated_title || content.extra_metadata?.title || 'Untitled'}\n                  </h3>\n                  <p className=\"text-sm text-gray-500\">Source: {content.source}</p>\n                </div>\n                <span className=\"px-3 py-1 bg-yellow-100 text-yellow-800 rounded-full text-sm\">\n                  Pending\n                </span>\n              </div>\n\n              <div className=\"space-y-4 mb-4\">\n                <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n                  <div>\n                    <p className=\"text-sm font-medium text-gray-700 mb-2\">Original Title (English):</p>\n                    <div className=\"p-3 bg-gray-50 rounded text-sm font-semibold\">\n                      {content.extra_metadata?.title || 'No title'}\n                    </div>\n                  </div>\n                  <div>\n                    <p className=\"text-sm font-medium text-gray-700 mb-2\">Translated Title (Ukrainian):</p>\n                    <div className=\"p-3 bg-blue-50 rounded text-sm font-semibold\">\n                      {content.translated_title || 'No translation'}\n                    </div>\n                  </div>\n                </div>\n                \n                <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n                  <div>\n                    <p className=\"text-sm font-medium text-gray-700 mb-2\">Original Content (English):</p>\n                    <div className=\"p-3 bg-gray-50 rounded text-sm max-h-64 overflow-y-auto\">\n                      {content.original_text || 'No content'}\n                    </div>\n                  </div>\n                  <div>\n                    <p className=\"text-sm font-medium text-gray-700 mb-2\">Translated Content (Ukrainian):</p>\n                    <div className=\"p-3 bg-blue-50 rounded text-sm max-h-64 overflow-y-auto\">\n                      {content.translated_text || 'No translation'}\n                    </div>\n                  </div>\n                </div>\n              </div>\n\n              {content.image_url ? (\n                <div className=\"mb-4\">\n                  <h4 className=\"font-semibold text-gray-700 mb-2\">Generated Image:</h4>\n                  <div className=\"bg-gray-50 p-4 rounded border border-gray-200\">\n                    <img \n                      src={content.image_url} \n                      alt=\"Generated\" \n                      className=\"w-full max-w-md rounded shadow-lg mb-2\"\n                    />\n                    {content.image_prompt && (\n                      <p className=\"text-xs text-gray-500 italic mb-2\">\n                        Prompt: {content.image_prompt}\n                      </p>\n                    )}\n                    <button\n                      onClick={() => handleRegenerateImage(content.id)}\n                      className=\"text-sm bg-blue-500 text-white px-3 py-1 rounded hover:bg-blue-600\"\n                    >\n                      üîÑ Regenerate Image\n                    </button>\n                  </div>\n                </div>\n              ) : (\n                <div className=\"mb-4\">\n                  <button\n                    onClick={() => handleGenerateImage(content.id)}\n                    className=\"bg-purple-500 text-white px-4 py-2 rounded hover:bg-purple-600\"\n                  >\n                    üé® Generate Image\n                  </button>\n                </div>\n              )}\n\n              <div className=\"flex space-x-2\">\n                <button\n                  onClick={() => handleApprove(content.id)}\n                  className=\"flex items-center space-x-1 px-4 py-2 bg-green-600 text-white rounded hover:bg-green-700\"\n                >\n                  <CheckCircle size={16} />\n                  <span>Approve</span>\n                </button>\n                <button\n                  onClick={() => handleReject(content.id)}\n                  className=\"flex items-center space-x-1 px-4 py-2 bg-red-600 text-white rounded hover:bg-red-700\"\n                >\n                  <XCircle size={16} />\n                  <span>Reject</span>\n                </button>\n              </div>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  )\n}\n\nexport default ContentApproval\n","path":null,"size_bytes":8712,"size_tokens":null},"backend/services/notification_service.py":{"content":"import os\nimport requests\nfrom typing import Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass NotificationService:\n    def __init__(self):\n        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n        self.chat_id = os.getenv('TELEGRAM_CHAT_ID')\n        self.base_url = f\"https://api.telegram.org/bot{self.bot_token}\"\n    \n    def send_approval_notification(self, content_data: Dict[str, Any]) -> bool:\n        \"\"\"\n        Send notification with photo and inline approval buttons\n        \n        Args:\n            content_data: Dict with content info including image_url and local_image_path\n        \"\"\"\n        if not self.bot_token or not self.chat_id:\n            logger.error(\"Telegram credentials not configured\")\n            return False\n        \n        content_id = content_data.get('id')\n        title = content_data.get('translated_title', content_data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'))\n        translated_text = content_data.get('translated_text', '')\n        image_url = content_data.get('image_url')\n        local_image_path = content_data.get('local_image_path')\n        source = content_data.get('source', 'The Spirits Business')\n        \n        preview_text = translated_text[:150] + \"...\" if len(translated_text) > 150 else translated_text\n        \n        caption = f\"\"\"üÜï <b>–ù–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏</b>\n\nüì∞ <b>{title}</b>\n\n{preview_text}\n\nüì∞ {source}\nüîó ID: {content_id}\n‚è∞ {content_data.get('created_at', '')}\"\"\"\n        \n        keyboard = {\n            \"inline_keyboard\": [\n                [\n                    {\"text\": \"‚úÖ Approve & Post\", \"callback_data\": f\"approve_{content_id}\"},\n                    {\"text\": \"‚ùå Reject\", \"callback_data\": f\"reject_{content_id}\"}\n                ]\n            ]\n        }\n        \n        import json\n        \n        try:\n            # PRIORITY 1: Use local image file (never expires)\n            if local_image_path and os.path.exists(local_image_path):\n                url = f\"{self.base_url}/sendPhoto\"\n                \n                with open(local_image_path, 'rb') as photo_file:\n                    files = {'photo': photo_file}\n                    data = {\n                        'chat_id': self.chat_id,\n                        'caption': caption,\n                        'parse_mode': 'HTML',\n                        'reply_markup': json.dumps(keyboard)\n                    }\n                    \n                    response = requests.post(url, files=files, data=data, timeout=30)\n                    result = response.json()\n                    \n                    if result.get('ok'):\n                        logger.info(f\"Approval notification sent with LOCAL image for content {content_id}\")\n                        return True\n                    else:\n                        logger.error(f\"Telegram API error (local image): {result}\")\n                        # Fall through to try URL\n            \n            # PRIORITY 2: Try image URL (may expire)\n            if image_url:\n                url = f\"{self.base_url}/sendPhoto\"\n                payload = {\n                    \"chat_id\": self.chat_id,\n                    \"photo\": image_url,\n                    \"caption\": caption,\n                    \"parse_mode\": \"HTML\",\n                    \"reply_markup\": keyboard\n                }\n                \n                response = requests.post(url, json=payload, timeout=15)\n                result = response.json()\n                \n                if result.get('ok'):\n                    logger.info(f\"Approval notification sent with URL for content {content_id}\")\n                    return True\n                else:\n                    logger.warning(f\"Telegram API error (URL - may be expired): {result}\")\n                    # Fall through to text-only\n            \n            # PRIORITY 3: Text only (no image)\n            url = f\"{self.base_url}/sendMessage\"\n            payload = {\n                \"chat_id\": self.chat_id,\n                \"text\": caption,\n                \"parse_mode\": \"HTML\",\n                \"reply_markup\": keyboard\n            }\n            \n            response = requests.post(url, json=payload, timeout=15)\n            result = response.json()\n            \n            if result.get('ok'):\n                logger.info(f\"Approval notification sent (text only) for content {content_id}\")\n                return True\n            else:\n                logger.error(f\"Telegram API error: {result}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Failed to send approval notification: {e}\")\n            return False\n    \n    def notify_content_approved(self, content_data: Dict[str, Any]) -> bool:\n        \"\"\"\n        Send notification when content is approved and ready to post\n        \n        Args:\n            content_data: Dict with approved content info\n            \n        Returns:\n            True if notification sent successfully\n        \"\"\"\n        title = content_data.get('translated_title', content_data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'))\n        \n        message = f\"\"\"\n‚úÖ <b>–ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ!</b>\n\nüì∞ <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫:</b> {title}\nüìÖ <b>–ó–∞–ø–ª–∞–Ω–æ–≤–∞–Ω–∏–π –ø–æ—Å—Ç–∏–Ω–≥:</b> {content_data.get('scheduled_time', '–í—ñ–¥—Ä–∞–∑—É')}\nüîó <b>ID:</b> {content_data.get('id')}\n\n–ö–æ–Ω—Ç–µ–Ω—Ç –±—É–¥–µ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ —Ä–æ–∑–∫–ª–∞–¥—É.\n        \"\"\"\n        \n        url = f\"{self.base_url}/sendMessage\"\n        payload = {\n            \"chat_id\": self.chat_id,\n            \"text\": message,\n            \"parse_mode\": \"HTML\"\n        }\n        \n        try:\n            response = requests.post(url, json=payload, timeout=10)\n            result = response.json()\n            \n            if result.get('ok'):\n                logger.info(f\"Approval notification sent for content {content_data.get('id')}\")\n                return True\n            else:\n                logger.error(f\"Telegram API error: {result}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Failed to send approval notification: {e}\")\n            return False\n    \n    def notify_content_posted(self, content_data: Dict[str, Any]) -> bool:\n        \"\"\"\n        Send notification when content is successfully posted to social media\n        \n        Args:\n            content_data: Dict with posted content info\n            \n        Returns:\n            True if notification sent successfully\n        \"\"\"\n        title = content_data.get('translated_title', content_data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'))\n        \n        message = f\"\"\"\nüéâ <b>–ö–æ–Ω—Ç–µ–Ω—Ç –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ!</b>\n\nüì∞ <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫:</b> {title}\nüì± <b>–ü–ª–∞—Ç—Ñ–æ—Ä–º–∏:</b> {', '.join(content_data.get('platforms', []))}\nüîó <b>Facebook:</b> {content_data.get('fb_post_url', 'N/A')}\n‚è∞ <b>–ß–∞—Å:</b> {content_data.get('posted_at', '')}\n\n–ö–æ–Ω—Ç–µ–Ω—Ç —É—Å–ø—ñ—à–Ω–æ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ –Ω–∞ —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö –º–µ—Ä–µ–∂–∞—Ö!\n        \"\"\"\n        \n        url = f\"{self.base_url}/sendMessage\"\n        payload = {\n            \"chat_id\": self.chat_id,\n            \"text\": message,\n            \"parse_mode\": \"HTML\"\n        }\n        \n        try:\n            response = requests.post(url, json=payload, timeout=10)\n            result = response.json()\n            \n            if result.get('ok'):\n                logger.info(f\"Posted notification sent for content {content_data.get('id')}\")\n                return True\n            else:\n                logger.error(f\"Telegram API error: {result}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Failed to send posted notification: {e}\")\n            return False\n    \n    def send_custom_notification(self, message: str) -> bool:\n        \"\"\"\n        Send custom notification message to Telegram\n        \n        Args:\n            message: Custom message text (supports HTML formatting)\n            \n        Returns:\n            True if notification sent successfully\n        \"\"\"\n        if not self.bot_token or not self.chat_id:\n            logger.error(\"Telegram credentials not configured\")\n            return False\n        \n        url = f\"{self.base_url}/sendMessage\"\n        payload = {\n            \"chat_id\": self.chat_id,\n            \"text\": message,\n            \"parse_mode\": \"HTML\",\n            \"disable_web_page_preview\": False\n        }\n        \n        try:\n            response = requests.post(url, json=payload, timeout=10)\n            result = response.json()\n            \n            if result.get('ok'):\n                logger.info(\"Custom notification sent successfully\")\n                return True\n            else:\n                logger.error(f\"Telegram API error: {result}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Failed to send custom notification: {e}\")\n            return False\n    \n    def send_test_notification(self) -> Dict[str, Any]:\n        \"\"\"Send test notification\"\"\"\n        url = f\"{self.base_url}/sendMessage\"\n        payload = {\n            \"chat_id\": self.chat_id,\n            \"text\": \"üß™ –¢–µ—Å—Ç–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤—ñ–¥ Gradus AI\\n\\n‚úÖ –°–µ—Ä–≤—ñ—Å –ø—Ä–∞—Ü—é—î –∫–æ—Ä–µ–∫—Ç–Ω–æ!\",\n            \"parse_mode\": \"HTML\"\n        }\n        \n        try:\n            response = requests.post(url, json=payload, timeout=10)\n            return response.json()\n        except Exception as e:\n            return {\"ok\": False, \"error\": str(e)}\n\nnotification_service = NotificationService()\n","path":null,"size_bytes":9667,"size_tokens":null},"backend/start.py":{"content":"#!/usr/bin/env python3\nimport uvicorn\nimport os\n\nif __name__ == \"__main__\":\n    port = int(os.getenv(\"PORT\", \"8000\"))\n    uvicorn.run(\n        \"main:app\",\n        host=\"0.0.0.0\",\n        port=port,\n        reload=True,\n        log_level=\"info\"\n    )\n","path":null,"size_bytes":250,"size_tokens":null},"backend/services/scrapers/base.py":{"content":"\"\"\"\nBase scraper interface for multi-source content ingestion\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport hashlib\n\n@dataclass\nclass ArticlePayload:\n    \"\"\"Standardized article data structure\"\"\"\n    source_name: str\n    language: str\n    needs_translation: bool\n    url: str\n    title: str\n    content: str\n    published_at: Optional[str] = None\n    author: Optional[str] = None\n    image_url: Optional[str] = None\n    tags: Optional[List[str]] = None\n    \n    def get_content_hash(self) -> str:\n        \"\"\"Generate hash for duplicate detection\"\"\"\n        slug = f\"{self.title}_{self.published_at or ''}\"\n        return hashlib.md5(slug.encode()).hexdigest()\n\nclass ScraperBase(ABC):\n    \"\"\"Base class for all news scrapers\"\"\"\n    \n    def __init__(self):\n        self.source_name = self.get_source_name()\n        self.language = self.get_language()\n        self.needs_translation = self.get_needs_translation()\n        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    \n    @abstractmethod\n    def get_source_name(self) -> str:\n        \"\"\"Return the human-readable source name\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_language(self) -> str:\n        \"\"\"Return source language code (en, uk, ru, etc.)\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_needs_translation(self) -> bool:\n        \"\"\"Return whether articles need translation\"\"\"\n        pass\n    \n    @abstractmethod\n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"\n        Scrape articles from the source\n        \n        Args:\n            limit: Maximum number of articles to scrape\n            \n        Returns:\n            List of ArticlePayload objects\n        \"\"\"\n        pass\n    \n    def is_enabled(self) -> bool:\n        \"\"\"Check if scraper is enabled (override for configuration)\"\"\"\n        return True\n    \n    def get_rate_limit_delay(self) -> float:\n        \"\"\"Return delay between requests in seconds\"\"\"\n        return 1.0\n","path":null,"size_bytes":2087,"size_tokens":null},"backend/services/scheduler.py":{"content":"from apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom datetime import datetime, timedelta\nimport logging\nfrom services.news_scraper import news_scraper\nfrom services.translation_service import translation_service\nfrom models.content import ContentQueue\n\nlogger = logging.getLogger(__name__)\n\nclass ContentScheduler:\n    def __init__(self):\n        # Use in-memory scheduler with misfire handling\n        # Startup check handles catch-up for missed jobs\n        self.scheduler = BackgroundScheduler(\n            job_defaults={\n                'coalesce': True,  # Combine multiple missed runs into one\n                'max_instances': 1,\n                'misfire_grace_time': 3600 * 2  # 2 hours grace period\n            }\n        )\n    \n    def _get_db_session(self):\n        \"\"\"\n        Get a properly initialized database session for scheduler tasks.\n        This ensures SessionLocal is initialized even in background threads.\n        \"\"\"\n        import models\n        \n        # Ensure database is initialized (handles background thread case)\n        if models.SessionLocal is None:\n            models.init_db()\n        \n        return models.SessionLocal()\n    \n    def scrape_linkedin_sources_task(self):\n        \"\"\"\n        Scrape LinkedIn sources: The Spirits Business, Drinks International\n        Runs: Mon/Wed/Fri at 1:00 AM\n        \n        Professional, business-oriented sources for B2B audience\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Scraping LinkedIn sources...\")\n        \n        try:\n            from services.scrapers.manager import scraper_manager\n            \n            # LinkedIn sources - professional content\n            linkedin_sources = ['spirits_business', 'drinks_international']\n            \n            db = self._get_db_session()\n            try:\n                # Get existing URLs for deduplication\n                existing_urls = set(\n                    url[0] for url in db.query(ContentQueue.source_url).filter(\n                        ContentQueue.source_url.isnot(None)\n                    ).all()\n                )\n                \n                existing_hashes = set()\n                for row in db.query(ContentQueue.extra_metadata).filter(\n                    ContentQueue.extra_metadata.isnot(None)\n                ).all():\n                    meta = row[0] if isinstance(row, tuple) else row\n                    if meta and isinstance(meta, dict) and meta.get('content_hash'):\n                        existing_hashes.add(meta.get('content_hash'))\n                \n                total_new = 0\n                \n                for source_name in linkedin_sources:\n                    try:\n                        articles = scraper_manager.scrape_source(source_name, limit=3)\n                        logger.info(f\"  üìä {source_name}: {len(articles)} articles scraped\")\n                        \n                        for article in articles:\n                            if scraper_manager.check_duplicate(article, existing_urls, existing_hashes):\n                                logger.info(f\"    ‚è≠Ô∏è  Duplicate skipped: {article.title[:50]}...\")\n                                continue\n                            \n                            content_hash = article.get_content_hash()\n                            new_article = ContentQueue(\n                                status='draft',\n                                source=article.source_name,\n                                source_url=article.url,\n                                original_text=article.content,\n                                language=article.language,\n                                needs_translation=article.needs_translation,\n                                platforms=['linkedin'],\n                                extra_metadata={\n                                    'title': article.title,\n                                    'published_date': article.published_at,\n                                    'author': article.author,\n                                    'content_hash': content_hash,\n                                    'scraped_at': datetime.utcnow().isoformat()\n                                }\n                            )\n                            db.add(new_article)\n                            total_new += 1\n                            \n                            existing_urls.add(article.url)\n                            existing_hashes.add(content_hash)\n                            \n                            lang_emoji = \"üá∫üá¶\" if article.language == 'uk' else \"üá¨üáß\"\n                            logger.info(f\"    ‚úÖ {lang_emoji} {article.title[:50]}...\")\n                    \n                    except Exception as e:\n                        logger.error(f\"  ‚ùå {source_name} failed: {e}\")\n                        continue\n                \n                db.commit()\n                logger.info(f\"‚úÖ [SCHEDULER] LinkedIn: Scraped {total_new} new articles\")\n                \n            finally:\n                db.close()\n        \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] LinkedIn scraping failed: {e}\")\n    \n    def scrape_facebook_sources_task(self):\n        \"\"\"\n        Scrape Facebook sources: Delo.ua, HoReCa-Ukraine, Just Drinks\n        Runs: Every day at 2:00 AM\n        \n        Mix of Ukrainian and English sources for general audience\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Scraping Facebook sources...\")\n        \n        try:\n            from services.scrapers.manager import scraper_manager\n            \n            # Facebook sources - 2 Ukrainian + 1 English\n            facebook_sources = ['delo_ua', 'restorator_ua', 'just_drinks']\n            \n            db = self._get_db_session()\n            try:\n                # Get existing URLs for deduplication\n                existing_urls = set(\n                    url[0] for url in db.query(ContentQueue.source_url).filter(\n                        ContentQueue.source_url.isnot(None)\n                    ).all()\n                )\n                \n                existing_hashes = set()\n                for row in db.query(ContentQueue.extra_metadata).filter(\n                    ContentQueue.extra_metadata.isnot(None)\n                ).all():\n                    meta = row[0] if isinstance(row, tuple) else row\n                    if meta and isinstance(meta, dict) and meta.get('content_hash'):\n                        existing_hashes.add(meta.get('content_hash'))\n                \n                total_new = 0\n                \n                for source_name in facebook_sources:\n                    try:\n                        articles = scraper_manager.scrape_source(source_name, limit=3)\n                        logger.info(f\"  üìä {source_name}: {len(articles)} articles scraped\")\n                        \n                        for article in articles:\n                            if scraper_manager.check_duplicate(article, existing_urls, existing_hashes):\n                                logger.info(f\"    ‚è≠Ô∏è  Duplicate skipped: {article.title[:50]}...\")\n                                continue\n                            \n                            content_hash = article.get_content_hash()\n                            new_article = ContentQueue(\n                                status='draft',\n                                source=article.source_name,\n                                source_url=article.url,\n                                original_text=article.content,\n                                language=article.language,\n                                needs_translation=article.needs_translation,\n                                platforms=['facebook'],\n                                extra_metadata={\n                                    'title': article.title,\n                                    'published_date': article.published_at,\n                                    'author': article.author,\n                                    'content_hash': content_hash,\n                                    'scraped_at': datetime.utcnow().isoformat()\n                                }\n                            )\n                            db.add(new_article)\n                            total_new += 1\n                            \n                            existing_urls.add(article.url)\n                            existing_hashes.add(content_hash)\n                            \n                            lang_emoji = \"üá∫üá¶\" if article.language == 'uk' else \"üá¨üáß\"\n                            logger.info(f\"    ‚úÖ {lang_emoji} {article.title[:50]}...\")\n                    \n                    except Exception as e:\n                        logger.error(f\"  ‚ùå {source_name} failed: {e}\")\n                        continue\n                \n                db.commit()\n                logger.info(f\"‚úÖ [SCHEDULER] Facebook: Scraped {total_new} new articles\")\n                \n            finally:\n                db.close()\n        \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] Facebook scraping failed: {e}\")\n    \n    def translate_pending_task(self):\n        \"\"\"\n        Task: Translate draft articles every hour (notifications sent later with images)\n        Runs at: Every hour at :15 (00:15, 01:15, etc.)\n        \n        ONLY translates articles that need translation (needs_translation=True)\n        Ukrainian sources (needs_translation=False) skip translation entirely\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Starting translation task...\")\n        \n        try:\n            db = self._get_db_session()\n            try:\n                # Get articles that NEED translation\n                draft_articles = db.query(ContentQueue).filter(\n                    ContentQueue.status == 'draft',\n                    ContentQueue.needs_translation == True,\n                    ContentQueue.translated_text == None\n                ).limit(10).all()\n                \n                if not draft_articles:\n                    logger.info(\"[SCHEDULER] No articles need translation\")\n                    return\n                \n                translated_count = 0\n                \n                for article in draft_articles:\n                    try:\n                        article_data = {\n                            'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n                            'content': article.original_text\n                        }\n                        \n                        translation = translation_service.translate_article(article_data)\n                        \n                        if translation and translation.get('title') and translation.get('content'):\n                            article.translated_title = translation['title']\n                            article.translated_text = translation['content']\n                            article.status = 'pending_approval'\n                            translated_count += 1\n                            logger.info(f\"[SCHEDULER] Translated article {article.id}: {article_data['title'][:50]}...\")\n                    except Exception as e:\n                        logger.error(f\"[SCHEDULER] Error translating article {article.id}: {e}\")\n                        db.rollback()\n                        continue\n                \n                db.commit()\n                logger.info(f\"‚úÖ [SCHEDULER] Translated {translated_count} articles (notifications will be sent with images)\")\n            finally:\n                db.close()\n            \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] Translation failed: {e}\")\n    \n    def generate_images_task(self):\n        \"\"\"\n        Task: Generate images AND send Telegram notifications with image previews\n        Runs at: Every hour at :30 (00:30, 01:30, etc.)\n        \n        Handles BOTH:\n        - Translated articles (status='pending_approval', has translated_text)\n        - Ukrainian articles (needs_translation=False, status='draft')\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Starting image generation task...\")\n        \n        try:\n            # Import services inside function to avoid circular imports\n            from services.image_generator import image_generator\n            from services.notification_service import notification_service\n            from sqlalchemy import or_\n            \n            db = self._get_db_session()\n            try:\n                # Get articles ready for images:\n                # 1. Already translated (status='pending_approval')\n                # 2. Ukrainian articles that don't need translation (needs_translation=False)\n                articles_without_images = db.query(ContentQueue).filter(\n                    or_(\n                        # Translated articles ready for images\n                        (ContentQueue.status == 'pending_approval'),\n                        # Ukrainian articles ready for images (skip translation)\n                        (ContentQueue.status == 'draft') & (ContentQueue.needs_translation == False)\n                    ),\n                    ContentQueue.image_url == None\n                ).limit(10).all()\n                \n                if not articles_without_images:\n                    logger.info(\"[SCHEDULER] No articles need images\")\n                    return\n                \n                generated_count = 0\n                notifications_sent = 0\n                \n                for article in articles_without_images:\n                    try:\n                        article_data = {\n                            'title': article.extra_metadata.get('title', '') if article.extra_metadata else '',\n                            'content': article.original_text or article.translated_text or ''\n                        }\n                        \n                        result = image_generator.generate_article_image(article_data)\n                        \n                        if result.get('image_url'):\n                            article.image_url = result['image_url']\n                            article.image_prompt = result['prompt']\n                            article.local_image_path = result.get('local_path', '')\n                            \n                            # Mark Ukrainian articles as pending_approval after image generation\n                            if not article.needs_translation and article.status == 'draft':\n                                article.status = 'pending_approval'\n                                # Use original text as \"translated\" text for Ukrainian sources\n                                if not article.translated_title:\n                                    article.translated_title = article.extra_metadata.get('title', '') if article.extra_metadata else ''\n                                if not article.translated_text:\n                                    article.translated_text = article.original_text\n                            \n                            generated_count += 1\n                            \n                            logger.info(f\"[SCHEDULER] Generated image for article {article.id}\")\n                            \n                            notification_data = {\n                                'id': article.id,\n                                'translated_title': article.translated_title,\n                                'translated_text': article.translated_text or article.original_text or '',\n                                'image_url': article.image_url,\n                                'local_image_path': article.local_image_path,\n                                'source': article.source or 'The Spirits Business',\n                                'created_at': article.created_at.strftime('%Y-%m-%d %H:%M') if article.created_at else ''\n                            }\n                            \n                            try:\n                                notification_service.send_approval_notification(notification_data)\n                                notifications_sent += 1\n                                logger.info(f\"‚úÖ [SCHEDULER] Notification with image sent for article {article.id}\")\n                            except Exception as notif_error:\n                                logger.error(f\"[SCHEDULER] Failed to send notification for article {article.id}: {notif_error}\")\n                            \n                    except Exception as e:\n                        logger.error(f\"[SCHEDULER] Error generating image for article {article.id}: {e}\")\n                        db.rollback()\n                        continue\n                \n                db.commit()\n                logger.info(f\"‚úÖ [SCHEDULER] Generated {generated_count} images, sent {notifications_sent} notifications\")\n            finally:\n                db.close()\n            \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] Image generation failed: {e}\")\n    \n    def cleanup_old_content_task(self):\n        \"\"\"\n        Task: Clean up old rejected content\n        Runs at: 03:00 AM daily\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Starting cleanup task...\")\n        \n        try:\n            db = self._get_db_session()\n            try:\n                cutoff_date = datetime.utcnow() - timedelta(days=30)\n                \n                deleted = db.query(ContentQueue).filter(\n                    ContentQueue.status == 'rejected',\n                    ContentQueue.created_at < cutoff_date\n                ).delete()\n                \n                db.commit()\n                logger.info(f\"‚úÖ [SCHEDULER] Cleaned up {deleted} old articles\")\n            finally:\n                db.close()\n            \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] Cleanup failed: {e}\")\n    \n    def check_api_services_task(self):\n        \"\"\"\n        Task: Comprehensive API monitoring - Check all services\n        Runs at: 09:00 daily\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Checking all API services...\")\n        \n        try:\n            from services.api_token_monitor import api_token_monitor\n            \n            results = api_token_monitor.check_all_services()\n            \n            services = results.get('services', {})\n            warnings = results.get('warnings', [])\n            errors = results.get('errors', [])\n            \n            healthy_count = sum(1 for s in services.values() if s.get('status') == 'healthy')\n            total_count = len(services)\n            \n            logger.info(f\"‚úÖ [SCHEDULER] API Monitor: {healthy_count}/{total_count} services healthy\")\n            \n            if errors:\n                logger.error(f\"‚ùå {len(errors)} service(s) with errors:\")\n                for error in errors:\n                    logger.error(f\"  ‚Ä¢ {error['service']}: {error['message']}\")\n            \n            if warnings:\n                logger.warning(f\"‚ö†Ô∏è {len(warnings)} service(s) with warnings:\")\n                for warning in warnings:\n                    logger.warning(f\"  ‚Ä¢ {warning['service']}: {warning['message']}\")\n            \n            if not warnings and not errors:\n                logger.info(\"‚úÖ All API services operational\")\n                \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] API monitoring failed: {e}\")\n    \n    def post_to_facebook_task(self):\n        \"\"\"\n        Post approved content to Facebook at scheduled time\n        Runs: Every day at 6:00 PM\n        Posts oldest approved content (FIFO queue)\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] Facebook scheduled posting...\")\n        \n        try:\n            from services.facebook_poster import facebook_poster\n            from services.notification_service import notification_service\n            \n            db = self._get_db_session()\n            try:\n                article = db.query(ContentQueue).filter(\n                    ContentQueue.status == 'approved'\n                ).order_by(ContentQueue.created_at.asc()).first()\n                \n                if not article:\n                    logger.info(\"[SCHEDULER] No approved content to post to Facebook\")\n                    return\n                \n                post_data = {\n                    'translated_title': article.translated_title or '',\n                    'translated_content': article.translated_text or '',\n                    'url': article.source_url or '',\n                    'source': article.source or 'The Spirits Business',\n                    'author': article.extra_metadata.get('author', '') if article.extra_metadata else '',\n                    'image_url': article.image_url,\n                    'local_image_path': article.local_image_path\n                }\n                \n                result = facebook_poster.post_with_image(post_data)\n                \n                if result:\n                    article.status = 'posted'\n                    article.posted_at = datetime.utcnow()\n                    \n                    if not article.extra_metadata:\n                        article.extra_metadata = {}\n                    article.extra_metadata['fb_post_id'] = result['post_id']\n                    article.extra_metadata['fb_post_url'] = result['post_url']\n                    article.extra_metadata['posted_platform'] = 'facebook'\n                    \n                    db.commit()\n                    \n                    logger.info(f\"‚úÖ [SCHEDULER] Posted to Facebook: Article {article.id}\")\n                    \n                    title = article.translated_title or 'No title'\n                    message = f\"\"\"üì¢ <b>–û–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ!</b>\n\nüì± <b>–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞:</b> Facebook\n\nüì∞ <b>{title}</b>\n\nüîó <a href=\"{result['post_url']}\">–ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ø–æ—Å—Ç</a>\n\n‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—è –∑–∞ —Ä–æ–∑–∫–ª–∞–¥–æ–º\nüÜî ID: {article.id}\nüïê {datetime.utcnow().strftime('%H:%M, %d %b %Y')}\"\"\"\n                    \n                    notification_service.send_custom_notification(message)\n                    \n                else:\n                    logger.error(f\"‚ùå [SCHEDULER] Facebook posting failed for article {article.id}\")\n                    \n            finally:\n                db.close()\n                \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] Facebook posting task failed: {e}\")\n            import traceback\n            logger.error(traceback.format_exc())\n    \n    def post_to_linkedin_task(self):\n        \"\"\"\n        Post approved content to LinkedIn at scheduled time\n        Runs: Mon/Wed/Fri at 9:00 AM\n        Posts oldest approved content (FIFO queue)\n        \"\"\"\n        logger.info(\"ü§ñ [SCHEDULER] LinkedIn scheduled posting...\")\n        \n        try:\n            from services.linkedin_poster import linkedin_poster\n            from services.notification_service import notification_service\n            \n            db = self._get_db_session()\n            try:\n                article = db.query(ContentQueue).filter(\n                    ContentQueue.status == 'approved'\n                ).order_by(ContentQueue.created_at.asc()).first()\n                \n                if not article:\n                    logger.info(\"[SCHEDULER] No approved content to post to LinkedIn\")\n                    return\n                \n                post_data = {\n                    'title': article.translated_title or '',\n                    'text': article.translated_text or '',\n                    'source': article.source or 'The Spirits Business',\n                    'source_url': article.source_url or '',\n                    'image_url': article.image_url,\n                    'local_image_path': article.local_image_path\n                }\n                \n                result = linkedin_poster.post_to_linkedin(post_data)\n                \n                if result.get('status') == 'success':\n                    article.status = 'posted'\n                    article.posted_at = datetime.utcnow()\n                    \n                    if not article.extra_metadata:\n                        article.extra_metadata = {}\n                    article.extra_metadata['linkedin_post_id'] = result.get('post_id', '')\n                    article.extra_metadata['linkedin_post_url'] = result.get('post_url', '')\n                    article.extra_metadata['posted_platform'] = 'linkedin'\n                    \n                    db.commit()\n                    \n                    logger.info(f\"‚úÖ [SCHEDULER] Posted to LinkedIn: Article {article.id}\")\n                    \n                    title = article.translated_title or 'No title'\n                    post_url = result.get('post_url', '')\n                    \n                    message = f\"\"\"üì¢ <b>–û–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ!</b>\n\nüíº <b>–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞:</b> LinkedIn\n\nüì∞ <b>{title}</b>\n\nüîó <a href=\"{post_url}\">–ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ø–æ—Å—Ç</a>\n\n‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—è –∑–∞ —Ä–æ–∑–∫–ª–∞–¥–æ–º\nüÜî ID: {article.id}\nüïê {datetime.utcnow().strftime('%H:%M, %d %b %Y')}\"\"\"\n                    \n                    notification_service.send_custom_notification(message)\n                    \n                else:\n                    logger.error(f\"‚ùå [SCHEDULER] LinkedIn posting failed: {result.get('message', 'Unknown error')}\")\n                    \n            finally:\n                db.close()\n                \n        except Exception as e:\n            logger.error(f\"‚ùå [SCHEDULER] LinkedIn posting task failed: {e}\")\n            import traceback\n            logger.error(traceback.format_exc())\n    \n    def check_and_run_missed_scraping(self):\n        \"\"\"\n        Check if scraping was missed and run immediately if needed.\n        Called on startup to catch up on missed jobs.\n        \n        Checks EACH platform independently to ensure both LinkedIn and Facebook\n        get caught up even if one was scraped recently.\n        \"\"\"\n        logger.info(\"üîç [SCHEDULER] Checking for missed scraping tasks...\")\n        \n        try:\n            db = self._get_db_session()\n            try:\n                from sqlalchemy import func, cast, String\n                \n                # Check Facebook sources (daily requirement)\n                facebook_sources = ['Delo.ua', 'HoReCa-–£–∫—Ä–∞—ó–Ω–∞', 'Just Drinks']\n                facebook_last = db.query(func.max(ContentQueue.created_at)).filter(\n                    ContentQueue.source.in_(facebook_sources)\n                ).scalar()\n                \n                # Check LinkedIn sources (Mon/Wed/Fri requirement)\n                linkedin_sources = ['The Spirits Business', 'Drinks International']\n                linkedin_last = db.query(func.max(ContentQueue.created_at)).filter(\n                    ContentQueue.source.in_(linkedin_sources)\n                ).scalar()\n                \n                today = datetime.utcnow().weekday()\n                is_linkedin_day = today in [0, 2, 4]  # Monday=0, Wednesday=2, Friday=4\n                \n                # Facebook catch-up: if more than 24 hours since last Facebook scrape\n                if facebook_last:\n                    fb_hours = (datetime.utcnow() - facebook_last).total_seconds() / 3600\n                    logger.info(f\"üìä Facebook sources: last scraped {fb_hours:.1f} hours ago\")\n                    \n                    if fb_hours > 24:\n                        logger.info(\"‚ö†Ô∏è Facebook sources overdue (>24h) - running catch-up...\")\n                        try:\n                            self.scrape_facebook_sources_task()\n                        except Exception as e:\n                            logger.error(f\"‚ùå Catch-up Facebook scraping failed: {e}\")\n                    else:\n                        logger.info(\"‚úÖ Facebook sources up-to-date\")\n                else:\n                    logger.info(\"üì≠ No Facebook articles in database, running initial scrape...\")\n                    try:\n                        self.scrape_facebook_sources_task()\n                    except Exception as e:\n                        logger.error(f\"‚ùå Initial Facebook scraping failed: {e}\")\n                \n                # LinkedIn catch-up: if more than 48 hours since last LinkedIn scrape\n                # (48h because LinkedIn only runs 3x/week on Mon/Wed/Fri)\n                # IMPORTANT: Catch-up runs ANY day if overdue, to recover from missed runs\n                if linkedin_last:\n                    li_hours = (datetime.utcnow() - linkedin_last).total_seconds() / 3600\n                    logger.info(f\"üìä LinkedIn sources: last scraped {li_hours:.1f} hours ago\")\n                    \n                    if li_hours > 48:  # More than 2 days = missed at least one scheduled run\n                        logger.info(\"‚ö†Ô∏è LinkedIn sources overdue (>48h) - running catch-up...\")\n                        try:\n                            self.scrape_linkedin_sources_task()\n                        except Exception as e:\n                            logger.error(f\"‚ùå Catch-up LinkedIn scraping failed: {e}\")\n                    else:\n                        logger.info(\"‚úÖ LinkedIn sources up-to-date\")\n                else:\n                    logger.info(\"üì≠ No LinkedIn articles in database, running initial scrape...\")\n                    try:\n                        self.scrape_linkedin_sources_task()\n                    except Exception as e:\n                        logger.error(f\"‚ùå Initial LinkedIn scraping failed: {e}\")\n                \n                logger.info(\"‚úÖ Catch-up check completed\")\n                    \n            finally:\n                db.close()\n                \n        except Exception as e:\n            logger.error(f\"‚ùå Error checking missed scraping: {e}\")\n    \n    def start(self):\n        \"\"\"Start scheduler with platform-specific scraping and posting\"\"\"\n        if self.scheduler.running:\n            logger.info(\"Scheduler already running, skipping start\")\n            return\n        \n        try:\n            # LinkedIn sources: Mon/Wed/Fri at 1:00 AM\n            self.scheduler.add_job(\n                self.scrape_linkedin_sources_task,\n                CronTrigger(day_of_week='mon,wed,fri', hour=1, minute=0),\n                id='scrape_linkedin',\n                name='Scrape LinkedIn sources (TSB, Drinks Int)',\n                replace_existing=True\n            )\n        except Exception as e:\n            logger.info(f\"Scheduler issue, recreating... ({e})\")\n            # Recreate scheduler with same config\n            self.scheduler = BackgroundScheduler(\n                job_defaults={\n                    'coalesce': True,\n                    'max_instances': 1,\n                    'misfire_grace_time': 3600 * 2\n                }\n            )\n            self.scheduler.add_job(\n                self.scrape_linkedin_sources_task,\n                CronTrigger(day_of_week='mon,wed,fri', hour=1, minute=0),\n                id='scrape_linkedin',\n                name='Scrape LinkedIn sources (TSB, Drinks Int)',\n                replace_existing=True\n            )\n        \n        # Facebook sources: Daily at 2:00 AM\n        self.scheduler.add_job(\n            self.scrape_facebook_sources_task,\n            CronTrigger(hour=2, minute=0),\n            id='scrape_facebook',\n            name='Scrape Facebook sources (Delo, HoReCa, Just Drinks)',\n            replace_existing=True\n        )\n        \n        # Translation: 3x per day (morning, afternoon, evening)\n        self.scheduler.add_job(\n            self.translate_pending_task,\n            CronTrigger(hour='6,14,20', minute=0),\n            id='translate_articles',\n            name='Translate pending articles (3x/day)',\n            replace_existing=True\n        )\n        \n        # Images: 3x per day (15 minutes after translation)\n        self.scheduler.add_job(\n            self.generate_images_task,\n            CronTrigger(hour='6,14,20', minute=15),\n            id='generate_images',\n            name='Generate images & send Telegram notifications (3x/day)',\n            replace_existing=True\n        )\n        \n        # LinkedIn posting: Mon/Wed/Fri at 9:00 AM\n        self.scheduler.add_job(\n            self.post_to_linkedin_task,\n            CronTrigger(day_of_week='mon,wed,fri', hour=9, minute=0),\n            id='post_linkedin',\n            name='Post to LinkedIn',\n            replace_existing=True\n        )\n        \n        # Facebook posting: Daily at 6:00 PM\n        self.scheduler.add_job(\n            self.post_to_facebook_task,\n            CronTrigger(hour=18, minute=0),\n            id='post_facebook',\n            name='Post to Facebook',\n            replace_existing=True\n        )\n        \n        # Cleanup: Daily at 3:00 AM\n        self.scheduler.add_job(\n            self.cleanup_old_content_task,\n            CronTrigger(hour=3, minute=0),\n            id='cleanup_old_content',\n            name='Cleanup old rejected content',\n            replace_existing=True\n        )\n        \n        # API monitoring: Daily at 8:00 AM\n        self.scheduler.add_job(\n            self.check_api_services_task,\n            CronTrigger(hour=8, minute=0),\n            id='check_api_services',\n            name='Check all API services',\n            replace_existing=True\n        )\n        \n        self.scheduler.start()\n        \n        logger.info(\"=\" * 60)\n        logger.info(\"‚úÖ GRADUS MEDIA AI AGENT - FULLY OPERATIONAL\")\n        logger.info(\"=\" * 60)\n        logger.info(\"\")\n        logger.info(\"üì∞ CONTENT SOURCES:\")\n        logger.info(\"   LinkedIn (Mon/Wed/Fri):\")\n        logger.info(\"      ‚Ä¢ The Spirits Business üá¨üáß\")\n        logger.info(\"      ‚Ä¢ Drinks International üá¨üáß\")\n        logger.info(\"\")\n        logger.info(\"   Facebook (Daily):\")\n        logger.info(\"      ‚Ä¢ Delo.ua üá∫üá¶\")\n        logger.info(\"      ‚Ä¢ HoReCa-–£–∫—Ä–∞—ó–Ω–∞ üá∫üá¶\")\n        logger.info(\"      ‚Ä¢ Just Drinks üá¨üáß\")\n        logger.info(\"\")\n        logger.info(\"üìÖ SCRAPING SCHEDULE:\")\n        logger.info(\"   ‚Ä¢ LinkedIn: Mon/Wed/Fri 1:00 AM\")\n        logger.info(\"   ‚Ä¢ Facebook: Daily 2:00 AM\")\n        logger.info(\"\")\n        logger.info(\"üîÑ PROCESSING:\")\n        logger.info(\"   ‚Ä¢ Translation: 3x/day at 6am, 2pm, 8pm\")\n        logger.info(\"   ‚Ä¢ Images: 3x/day at 6:15am, 2:15pm, 8:15pm\")\n        logger.info(\"\")\n        logger.info(\"üì¢ POSTING SCHEDULE:\")\n        logger.info(\"   ‚Ä¢ LinkedIn: Mon/Wed/Fri 9:00 AM\")\n        logger.info(\"   ‚Ä¢ Facebook: Daily 6:00 PM\")\n        logger.info(\"\")\n        logger.info(\"üîß MAINTENANCE:\")\n        logger.info(\"   ‚Ä¢ API monitoring: Daily 8:00 AM\")\n        logger.info(\"   ‚Ä¢ Cleanup: Daily 3:00 AM\")\n        logger.info(\"\")\n        logger.info(\"=\" * 60)\n        logger.info(\"üöÄ System ready! Waiting for next scheduled task...\")\n        logger.info(\"=\" * 60)\n    \n    def stop(self):\n        \"\"\"Stop the scheduler (idempotent)\"\"\"\n        if self.scheduler.running:\n            self.scheduler.shutdown()\n            logger.info(\"Scheduler stopped\")\n        else:\n            logger.info(\"Scheduler already stopped\")\n    \n    def get_jobs(self):\n        \"\"\"Get list of scheduled jobs\"\"\"\n        jobs = []\n        for job in self.scheduler.get_jobs():\n            jobs.append({\n                'id': job.id,\n                'name': job.name,\n                'next_run': job.next_run_time.isoformat() if job.next_run_time else None,\n                'trigger': str(job.trigger)\n            })\n        return jobs\n\ncontent_scheduler = ContentScheduler()\n","path":null,"size_bytes":35623,"size_tokens":null},"backend/services/social_poster.py":{"content":"import logging\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass SocialPoster:\n    \"\"\"\n    Service for posting content to social media platforms.\n    This is a stub - requires Facebook Graph API and LinkedIn API credentials.\n    \"\"\"\n    \n    def __init__(self):\n        self.facebook_enabled = False\n        self.linkedin_enabled = False\n        logger.info(\"SocialPoster initialized - awaiting API credentials\")\n    \n    async def post_to_facebook(self, text: str, image_url: str) -> dict:\n        \"\"\"\n        Post content to Facebook using Graph API.\n        Requires: FACEBOOK_PAGE_ACCESS_TOKEN, FACEBOOK_PAGE_ID\n        \"\"\"\n        if not self.facebook_enabled:\n            logger.warning(\"Facebook posting not enabled - credentials not configured\")\n            return {\n                \"success\": False,\n                \"platform\": \"facebook\",\n                \"message\": \"Facebook API credentials not configured\"\n            }\n        \n        return {\n            \"success\": True,\n            \"platform\": \"facebook\",\n            \"post_id\": \"placeholder_fb_post_id\"\n        }\n    \n    async def post_to_linkedin(self, text: str, image_url: str) -> dict:\n        \"\"\"\n        Post content to LinkedIn using LinkedIn API.\n        Requires: LINKEDIN_ACCESS_TOKEN, LINKEDIN_PERSON_URN\n        \"\"\"\n        if not self.linkedin_enabled:\n            logger.warning(\"LinkedIn posting not enabled - credentials not configured\")\n            return {\n                \"success\": False,\n                \"platform\": \"linkedin\",\n                \"message\": \"LinkedIn API credentials not configured\"\n            }\n        \n        return {\n            \"success\": True,\n            \"platform\": \"linkedin\",\n            \"post_id\": \"placeholder_li_post_id\"\n        }\n    \n    async def post_content(self, text: str, image_url: str, platforms: List[str]) -> List[dict]:\n        \"\"\"\n        Post content to specified platforms.\n        \"\"\"\n        results = []\n        \n        if \"facebook\" in platforms:\n            result = await self.post_to_facebook(text, image_url)\n            results.append(result)\n        \n        if \"linkedin\" in platforms:\n            result = await self.post_to_linkedin(text, image_url)\n            results.append(result)\n        \n        return results\n\nsocial_poster = SocialPoster()\n","path":null,"size_bytes":2317,"size_tokens":null},"frontend/src/index.css":{"content":"@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\nbody {\n  margin: 0;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',\n    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',\n    sans-serif;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\ncode {\n  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',\n    monospace;\n}\n","path":null,"size_bytes":426,"size_tokens":null},"backend/services/scrapers/just_drinks.py":{"content":"\"\"\"\nJust Drinks scraper - English source, NEEDS translation\nScrapes lighter, more accessible alcohol news from just-drinks.com\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Optional\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass JustDrinksScraper(ScraperBase):\n    \"\"\"Scraper for Just Drinks news (English source)\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"Just Drinks\"\n    \n    def get_language(self) -> str:\n        return \"en\"\n    \n    def get_needs_translation(self) -> bool:\n        return True\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape articles from Just Drinks news section\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name} (English)...\")\n            news_url = \"https://www.just-drinks.com/news/\"\n            \n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(news_url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article cards\n            article_elements = (\n                soup.select('article') or\n                soup.select('.article-card') or\n                soup.select('.news-item') or\n                soup.select('.post') or\n                soup.select('[class*=\"article\"]')\n            )\n            \n            logger.info(f\"  Found {len(article_elements)} potential articles\")\n            \n            for element in article_elements[:limit * 2]:\n                if len(articles) >= limit:\n                    break\n                \n                try:\n                    article_data = self._parse_article_card(element)\n                    \n                    if not article_data:\n                        continue\n                    \n                    # Fetch full article content (pass title for cleaning)\n                    content = self._fetch_article_content(article_data['url'], article_data['title'])\n                    \n                    if content and len(content) > 100:\n                        article = ArticlePayload(\n                            source_name=self.source_name,\n                            language=self.language,\n                            needs_translation=self.needs_translation,\n                            url=article_data['url'],\n                            title=article_data['title'],\n                            content=content,\n                            published_at=article_data.get('published_date'),\n                            author=article_data.get('author'),\n                            image_url=article_data.get('image_url')\n                        )\n                        articles.append(article)\n                        logger.info(f\"  ‚úÖ {article_data['title'][:50]}...\")\n                        \n                except Exception as e:\n                    logger.error(f\"  Error parsing article element: {e}\")\n                    continue\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} English articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _parse_article_card(self, element) -> Optional[dict]:\n        \"\"\"Parse article card from listing page\"\"\"\n        try:\n            # Find title\n            title_elem = (\n                element.select_one('h2') or\n                element.select_one('h3') or\n                element.select_one('.title') or\n                element.select_one('.article-title') or\n                element.select_one('.headline') or\n                element.select_one('[class*=\"title\"]')\n            )\n            \n            if not title_elem:\n                return None\n            \n            title = title_elem.get_text(strip=True)\n            \n            if len(title) < 10:\n                return None\n            \n            # Find the article link - need to be careful to get the actual article URL\n            # not the category link (/news/)\n            url = None\n            \n            # Strategy 1: Check if title element is inside a link\n            link_elem = title_elem.find_parent('a')\n            if link_elem:\n                href = link_elem.get('href', '')\n                # Make sure it's not just the category page\n                if href and '/news/' in href and href.rstrip('/') != 'https://www.just-drinks.com/news' and href != '/news/':\n                    url = href\n            \n            # Strategy 2: Check for link inside the title element\n            if not url:\n                link_inside = title_elem.select_one('a')\n                if link_inside:\n                    href = link_inside.get('href', '')\n                    if href and '/news/' in href and href.rstrip('/') != 'https://www.just-drinks.com/news' and href != '/news/':\n                        url = href\n            \n            # Strategy 3: Look for article links in the card element\n            if not url:\n                all_links = element.select('a[href]')\n                for link in all_links:\n                    href = link.get('href', '')\n                    # Skip category links, author links, etc.\n                    if not href:\n                        continue\n                    if href == '/news/' or href.rstrip('/') == 'https://www.just-drinks.com/news':\n                        continue\n                    if '/author/' in href:\n                        continue\n                    # This looks like an article link\n                    if '/news/' in href and len(href) > len('/news/') + 5:\n                        url = href\n                        break\n            \n            if not url:\n                return None\n            \n            # Make URL absolute\n            if not url.startswith('http'):\n                base_url = \"https://www.just-drinks.com\"\n                url = base_url + url if url.startswith('/') else f\"{base_url}/{url}\"\n            \n            # Find image (optional)\n            img_elem = element.select_one('img')\n            image_url = None\n            if img_elem:\n                image_url = img_elem.get('src') or img_elem.get('data-src')\n                if image_url and not image_url.startswith('http'):\n                    if image_url.startswith('//'):\n                        image_url = f\"https:{image_url}\"\n                    else:\n                        image_url = f\"https://www.just-drinks.com{image_url}\"\n            \n            # Find date (optional)\n            date_elem = element.select_one('.date') or element.select_one('time') or element.select_one('.published') or element.select_one('.post-date')\n            published_date = None\n            if date_elem:\n                published_date = date_elem.get_text(strip=True)\n                if not published_date and date_elem.has_attr('datetime'):\n                    published_date = date_elem['datetime']\n            \n            # Find author (optional)\n            author_elem = element.select_one('.author') or element.select_one('.byline')\n            author = None\n            if author_elem:\n                author = author_elem.get_text(strip=True)\n            \n            return {\n                'title': title,\n                'url': url,\n                'image_url': image_url,\n                'published_date': published_date,\n                'author': author\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article card: {e}\")\n            return None\n    \n    def _fetch_article_content(self, url: str, title: str = \"\") -> Optional[str]:\n        \"\"\"Fetch full article content from article page\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Extended list of content selectors - try in order of specificity\n            content_selectors = [\n                '[itemprop=\"articleBody\"]',    # Schema.org markup (most reliable)\n                '.article-body',                # Main article content\n                '.article__body',               # Alternative naming\n                '.story-body',                  # News story body\n                '.full-article',                # Full article container\n                '.article-content',             # Article content class\n                '.entry-content',               # WordPress style\n                '.post-content',                # Post body\n                '.content-body',                # Content body\n                'article .content',             # Article content wrapper\n                '[class*=\"article-body\"]',      # Partial match\n                '[class*=\"story-content\"]',     # Story content\n                'article',                      # Fallback to article tag\n            ]\n            \n            content_elem = None\n            for selector in content_selectors:\n                content_elem = soup.select_one(selector)\n                if content_elem:\n                    # Check if this has substantial content\n                    text_len = len(content_elem.get_text(strip=True))\n                    if text_len > 100:\n                        logger.debug(f\"  Using selector: {selector} ({text_len} chars)\")\n                        break\n                    else:\n                        content_elem = None  # Too short, try next selector\n            \n            if not content_elem:\n                # Fallback: try trafilatura for clean extraction\n                try:\n                    import trafilatura\n                    downloaded = trafilatura.fetch_url(url)\n                    if downloaded:\n                        content = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n                        if content and len(content) > 100:\n                            logger.info(f\"  Used trafilatura for content extraction\")\n                            return self._clean_content(content, title)\n                except Exception as e:\n                    logger.debug(f\"  Trafilatura fallback failed: {e}\")\n                \n                logger.warning(f\"  Could not find content container for: {url}\")\n                return None\n            \n            # Remove unwanted elements (ads, scripts, social, etc.)\n            for unwanted in content_elem.select('script, style, aside, .ads, .advertisement, .social-share, .related-articles, nav, footer, .comments, .share, .newsletter, .subscription, .promo'):\n                unwanted.decompose()\n            \n            # Remove metadata elements BEFORE extracting text\n            metadata_selectors = [\n                '.author', '.post-author', '.byline', '.article-author',\n                '.date', '.post-date', '.published-date', '.timestamp', 'time',\n                '.article-meta', '.meta', '.post-meta', '.entry-meta',\n                '.social-share', '.share-buttons', '.share-links',\n                '.tags', '.post-tags', '.article-tags',\n                '.article-teaser', '.teaser', '.preview', '.excerpt'\n            ]\n            for selector in metadata_selectors:\n                for element in content_elem.select(selector):\n                    element.decompose()\n            \n            # Extract all paragraph text for better content capture\n            paragraphs = content_elem.select('p')\n            if paragraphs and len(paragraphs) > 2:\n                # Use paragraph-based extraction for better quality\n                content_parts = []\n                for p in paragraphs:\n                    text = p.get_text(strip=True)\n                    if text and len(text) > 20:  # Skip very short paragraphs\n                        content_parts.append(text)\n                content = '\\n\\n'.join(content_parts)\n            else:\n                # Fallback to full text extraction\n                content = content_elem.get_text(separator='\\n', strip=True)\n            \n            # Check content length and warn if too short\n            if len(content) < 200:\n                logger.warning(f\"  Just Drinks article too short ({len(content)} chars): {title[:50]}...\")\n                # Try trafilatura as fallback\n                try:\n                    import trafilatura\n                    downloaded = trafilatura.fetch_url(url)\n                    if downloaded:\n                        traf_content = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n                        if traf_content and len(traf_content) > len(content):\n                            logger.info(f\"  Trafilatura recovered more content: {len(traf_content)} chars\")\n                            content = traf_content\n                except Exception:\n                    pass\n            \n            # Clean content (remove metadata patterns, fix formatting)\n            content = self._clean_content(content, title)\n            \n            return content\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_content(self, content: str, title: str) -> str:\n        \"\"\"Clean Just Drinks article content - remove author, dates, fix formatting\"\"\"\n        import re\n        \n        if not content:\n            return \"\"\n        \n        # Remove duplicate title if present at start (within first 500 chars)\n        if title and len(title) > 15:\n            if content.startswith(title):\n                content = content[len(title):].strip()\n            else:\n                title_pos = content[:500].find(title)\n                if title_pos >= 0:\n                    content = content[:title_pos] + content[title_pos + len(title):]\n                    content = content.strip()\n        \n        # Split into lines\n        lines = content.split('\\n')\n        cleaned_lines = []\n        \n        # English month names for date detection\n        months = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)'\n        \n        for line in lines:\n            line = line.strip()\n            \n            # Skip empty lines\n            if not line:\n                continue\n            \n            # Skip \"By <Author>\" bylines (e.g., \"By Fiona Holland\", \"By John Smith in News\")\n            if re.match(r'^By\\s+[A-Z][a-z]+(\\s+[A-Z][a-z\\-]+)*(\\s+(in|for|at|from)\\s+.*)?$', line, re.IGNORECASE):\n                continue\n            \n            # Skip author names (2-4 capitalized words, short line, no punctuation)\n            words = line.split()\n            if 1 <= len(words) <= 4 and len(line) < 40:\n                # Check if it looks like an author name (capitalized words, allowing small connectors)\n                name_words = [w for w in words if len(w) > 2]  # Ignore small words like \"in\", \"at\"\n                if name_words and all(w[0].isupper() for w in name_words if w and w[0].isalpha()):\n                    # But don't skip if it ends with punctuation (likely a sentence)\n                    if not line.endswith(('.', '!', '?', ':')):\n                        continue\n            \n            # Skip date patterns: \"November 24, 2025\", \"24 November 2025\", etc.\n            if re.match(rf'^{months}\\s+\\d{{1,2}},?\\s+\\d{{4}}$', line, re.IGNORECASE):\n                continue\n            if re.match(rf'^\\d{{1,2}}\\s+{months},?\\s+\\d{{4}}$', line, re.IGNORECASE):\n                continue\n            if re.match(r'^\\d{1,2}/\\d{1,2}/\\d{4}$', line):\n                continue\n            if re.match(r'^\\d{4}-\\d{2}-\\d{2}$', line):\n                continue\n            \n            # Skip very short lines that are likely navigation/UI (but keep source tag)\n            if len(line) < 10 and not line.endswith('.') and 'Just Drinks' not in line:\n                continue\n            \n            # Skip social sharing text\n            if any(social in line.lower() for social in ['share this', 'tweet', 'linkedin', 'facebook', 'email this']):\n                if len(line) < 30:\n                    continue\n            \n            # Skip Just Drinks promotional/subscription content\n            promo_phrases = [\n                'stay ahead with unbiased news',\n                'combine business intelligence and editorial excellence',\n                'as a trusted provider of data and insights',\n                'gain a deeper understanding of the drinks industry',\n                'ready to stay informed',\n                'subscribeto',  # No space version\n                'subscribe to',\n                'the gold standard of business intelligence',\n                'reach engaged professionals across',\n                'leading media platforms',\n                'unique thought leadership and analysis',\n                'priorities shaping the profession',\n                'just drinks collaborates closely with industry leaders',\n                'sign up for our newsletter',\n                'get the latest news delivered',\n                'unlock exclusive content',\n                'already a subscriber',\n                'sign into access your account',\n                'complete this form',\n                'request more information',\n                'representative will be in touch',\n                'don\\'t let policy changes catch you',\n                'stay proactive with real-time data',\n                'gain the recognition you deserve',\n                'just drinks excellence awards',\n                'celebrate innovation, leadership',\n                'elevate your industry profile',\n                'showcase your achievements',\n            ]\n            line_lower = line.lower()\n            if any(phrase in line_lower for phrase in promo_phrases):\n                continue\n            \n            cleaned_lines.append(line)\n        \n        # Join with double newline for paragraph separation\n        content = '\\n\\n'.join(cleaned_lines)\n        \n        # Fix paragraph spacing at sentence boundaries (for run-on text)\n        content = re.sub(r'\\.(\\s*)([A-Z])', r'.\\n\\n\\2', content)\n        \n        # Remove excessive newlines\n        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n        \n        # Remove non-breaking spaces and zero-width spaces\n        content = content.replace('\\xa0', ' ')\n        content = content.replace('\\u200b', '')\n        \n        # Remove multiple spaces\n        content = re.sub(r' +', ' ', content)\n        \n        return content.strip()\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Legacy method - redirects to _clean_content\"\"\"\n        return self._clean_content(text, \"\")\n","path":null,"size_bytes":18981,"size_tokens":null},"backend/services/analytics_tracker.py":{"content":"import os\nimport requests\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom models import SessionLocal\nfrom models.content import ContentQueue\n\nlogger = logging.getLogger(__name__)\n\nclass AnalyticsTracker:\n    def __init__(self):\n        self.page_access_token = os.getenv('FACEBOOK_PAGE_ACCESS_TOKEN')\n        self.page_id = os.getenv('FACEBOOK_PAGE_ID')\n        self.graph_api_version = 'v18.0'\n    \n    def get_post_insights(self, post_id: str) -> Dict:\n        \"\"\"\n        Get engagement metrics for a Facebook post\n        \n        Args:\n            post_id: Facebook post ID (page_id_post_id format)\n            \n        Returns:\n            Dict with likes, comments, shares, reach, impressions, engagement_rate\n        \"\"\"\n        if not self.page_access_token:\n            logger.error(\"Facebook token not configured\")\n            return {\"error\": \"Token not configured\"}\n        \n        url = f\"https://graph.facebook.com/{self.graph_api_version}/{post_id}\"\n        params = {\n            'fields': 'likes.summary(true),comments.summary(true),shares,insights.metric(post_impressions,post_engaged_users,post_clicks)',\n            'access_token': self.page_access_token\n        }\n        \n        try:\n            response = requests.get(url, params=params, timeout=15)\n            result = response.json()\n            \n            if 'error' in result:\n                logger.error(f\"Analytics error for {post_id}: {result['error']}\")\n                return {\"error\": result['error'].get('message', 'Unknown error')}\n            \n            metrics = {\n                'post_id': post_id,\n                'likes': result.get('likes', {}).get('summary', {}).get('total_count', 0),\n                'comments': result.get('comments', {}).get('summary', {}).get('total_count', 0),\n                'shares': result.get('shares', {}).get('count', 0),\n                'engagement_rate': 0,\n                'impressions': 0,\n                'reach': 0,\n                'clicks': 0,\n                'collected_at': datetime.now().isoformat()\n            }\n            \n            insights = result.get('insights', {}).get('data', [])\n            for insight in insights:\n                metric_name = insight.get('name')\n                values = insight.get('values', [])\n                value = values[0].get('value', 0) if values else 0\n                \n                if metric_name == 'post_impressions':\n                    metrics['impressions'] = value\n                elif metric_name == 'post_engaged_users':\n                    metrics['reach'] = value\n                elif metric_name == 'post_clicks':\n                    metrics['clicks'] = value\n            \n            if metrics['impressions'] > 0:\n                total_engagement = metrics['likes'] + metrics['comments'] + metrics['shares']\n                metrics['engagement_rate'] = round((total_engagement / metrics['impressions']) * 100, 2)\n            \n            logger.info(f\"Collected metrics for {post_id}: {metrics['likes']} likes, {metrics['comments']} comments\")\n            \n            return metrics\n            \n        except Exception as e:\n            logger.error(f\"Failed to get insights for {post_id}: {e}\")\n            return {\"error\": str(e)}\n    \n    def get_best_posting_times(self, days: int = 30) -> Dict:\n        \"\"\"\n        Analyze historical posts to find best posting times\n        \n        Args:\n            days: Number of days to analyze (default 30)\n            \n        Returns:\n            Dict with best hours, best days, and statistics\n        \"\"\"\n        db = SessionLocal()\n        \n        try:\n            cutoff_date = datetime.now() - timedelta(days=days)\n            \n            posts = db.query(ContentQueue).filter(\n                ContentQueue.status == 'posted',\n                ContentQueue.created_at >= cutoff_date,\n                ContentQueue.extra_metadata.isnot(None)\n            ).all()\n            \n            posts = [p for p in posts if p.extra_metadata and 'fb_post_id' in p.extra_metadata]\n            \n            if not posts:\n                return {\n                    \"message\": \"Not enough data yet. Need at least 1 posted article.\",\n                    \"posts_analyzed\": 0\n                }\n            \n            performance_by_hour = {}\n            performance_by_day = {}\n            total_engagement = 0\n            posts_with_metrics = 0\n            \n            for post in posts:\n                post_id = post.extra_metadata.get('fb_post_id')\n                \n                if 'analytics' in post.extra_metadata:\n                    metrics = post.extra_metadata['analytics']\n                else:\n                    metrics = self.get_post_insights(post_id)\n                    \n                    if 'error' not in metrics:\n                        if not post.extra_metadata:\n                            post.extra_metadata = {}\n                        post.extra_metadata['analytics'] = metrics\n                \n                if 'error' in metrics:\n                    continue\n                \n                posts_with_metrics += 1\n                engagement = metrics.get('likes', 0) + metrics.get('comments', 0) + metrics.get('shares', 0)\n                total_engagement += engagement\n                \n                hour = post.created_at.hour\n                if hour not in performance_by_hour:\n                    performance_by_hour[hour] = {'count': 0, 'total_engagement': 0}\n                \n                performance_by_hour[hour]['count'] += 1\n                performance_by_hour[hour]['total_engagement'] += engagement\n                \n                day = post.created_at.strftime('%A')\n                if day not in performance_by_day:\n                    performance_by_day[day] = {'count': 0, 'total_engagement': 0}\n                \n                performance_by_day[day]['count'] += 1\n                performance_by_day[day]['total_engagement'] += engagement\n            \n            db.commit()\n            \n            best_hours = []\n            for hour, data in performance_by_hour.items():\n                avg = data['total_engagement'] / data['count'] if data['count'] > 0 else 0\n                best_hours.append({\n                    'hour': f\"{hour:02d}:00\",\n                    'avg_engagement': round(avg, 2),\n                    'posts_count': data['count'],\n                    'total_engagement': data['total_engagement']\n                })\n            \n            best_hours = sorted(best_hours, key=lambda x: x['avg_engagement'], reverse=True)\n            \n            best_days = []\n            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n            for day in day_order:\n                if day in performance_by_day:\n                    data = performance_by_day[day]\n                    avg = data['total_engagement'] / data['count'] if data['count'] > 0 else 0\n                    best_days.append({\n                        'day': day,\n                        'avg_engagement': round(avg, 2),\n                        'posts_count': data['count'],\n                        'total_engagement': data['total_engagement']\n                    })\n            \n            best_days = sorted(best_days, key=lambda x: x['avg_engagement'], reverse=True)\n            \n            avg_engagement_per_post = round(total_engagement / posts_with_metrics, 2) if posts_with_metrics > 0 else 0\n            \n            return {\n                'summary': {\n                    'posts_analyzed': posts_with_metrics,\n                    'date_range_days': days,\n                    'total_engagement': total_engagement,\n                    'avg_engagement_per_post': avg_engagement_per_post\n                },\n                'best_hours': best_hours[:5],\n                'best_days': best_days,\n                'recommendations': self._generate_recommendations(best_hours, best_days, posts_with_metrics)\n            }\n            \n        finally:\n            db.close()\n    \n    def _generate_recommendations(self, best_hours: List, best_days: List, posts_count: int) -> List[str]:\n        \"\"\"Generate actionable recommendations based on data\"\"\"\n        recommendations = []\n        \n        if posts_count < 5:\n            recommendations.append(f\"‚ö†Ô∏è Limited data: Only {posts_count} posts analyzed. Need at least 10 posts for reliable insights.\")\n        \n        if best_hours:\n            top_hour = best_hours[0]\n            recommendations.append(f\"üïê Best posting time: {top_hour['hour']} (avg {top_hour['avg_engagement']} engagement)\")\n        \n        if best_days:\n            top_day = best_days[0]\n            recommendations.append(f\"üìÖ Best posting day: {top_day['day']} (avg {top_day['avg_engagement']} engagement)\")\n        \n        if len(best_hours) >= 3:\n            top_3_hours = [h['hour'] for h in best_hours[:3]]\n            recommendations.append(f\"‚≠ê Optimal posting windows: {', '.join(top_3_hours)}\")\n        \n        return recommendations\n    \n    def get_recent_posts_performance(self, limit: int = 10) -> List[Dict]:\n        \"\"\"Get performance summary for recent posts\"\"\"\n        db = SessionLocal()\n        \n        try:\n            posts = db.query(ContentQueue).filter(\n                ContentQueue.status == 'posted',\n                ContentQueue.extra_metadata.isnot(None)\n            ).order_by(ContentQueue.created_at.desc()).limit(limit).all()\n            \n            results = []\n            \n            for post in posts:\n                if not post.extra_metadata or 'fb_post_id' not in post.extra_metadata:\n                    continue\n                \n                post_id = post.extra_metadata['fb_post_id']\n                \n                if 'analytics' in post.extra_metadata:\n                    metrics = post.extra_metadata['analytics']\n                else:\n                    metrics = self.get_post_insights(post_id)\n                    if 'error' not in metrics:\n                        if not post.extra_metadata:\n                            post.extra_metadata = {}\n                        post.extra_metadata['analytics'] = metrics\n                \n                if 'error' in metrics:\n                    continue\n                \n                results.append({\n                    'id': post.id,\n                    'title': post.translated_title or 'Untitled',\n                    'created_at': post.created_at.isoformat() if post.created_at else None,\n                    'post_url': post.extra_metadata.get('fb_post_url', ''),\n                    'metrics': {\n                        'likes': metrics.get('likes', 0),\n                        'comments': metrics.get('comments', 0),\n                        'shares': metrics.get('shares', 0),\n                        'engagement_rate': metrics.get('engagement_rate', 0),\n                        'impressions': metrics.get('impressions', 0)\n                    }\n                })\n            \n            db.commit()\n            return results\n            \n        finally:\n            db.close()\n\nanalytics_tracker = AnalyticsTracker()\n","path":null,"size_bytes":11164,"size_tokens":null},"backend/services/scrapers/__init__.py":{"content":"\"\"\"\nMulti-source scraping architecture\nSupports both English and Ukrainian news sources\n\"\"\"\n\nfrom .base import ScraperBase, ArticlePayload\nfrom .spirits_business import SpiritsBusinessScraper\nfrom .delo_ua import DeloUaScraper\nfrom .minfin_ua import MinFinUaScraper\nfrom .just_drinks import JustDrinksScraper\nfrom .restorator_ua import RestoratorUaScraper\nfrom .drinks_report import DrinksReportScraper\nfrom .manager import ScraperManager\n\n__all__ = [\n    'ScraperBase',\n    'ArticlePayload',\n    'SpiritsBusinessScraper',\n    'DeloUaScraper',\n    'MinFinUaScraper',\n    'JustDrinksScraper',\n    'RestoratorUaScraper',\n    'DrinksReportScraper',\n    'ScraperManager'\n]\n","path":null,"size_bytes":669,"size_tokens":null},"backend/services/scrapers/drinks_report.py":{"content":"\"\"\"\nThe Drinks Report scraper - English source, NEEDS translation\nScrapes quick news bites and industry updates from thedrinksreport.com\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Optional\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass DrinksReportScraper(ScraperBase):\n    \"\"\"Scraper for The Drinks Business - Leading drinks industry news (English source)\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"The Drinks Business\"\n    \n    def get_language(self) -> str:\n        return \"en\"\n    \n    def get_needs_translation(self) -> bool:\n        return True\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape articles from The Drinks Business homepage\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name} (English)...\")\n            news_url = \"https://www.thedrinksbusiness.com/\"\n            \n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(news_url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article cards\n            article_elements = (\n                soup.select('div[class*=\"post\"]') or\n                soup.select('article') or\n                soup.select('.post') or\n                soup.select('.news-item') or\n                soup.select('.article-card') or\n                soup.select('[class*=\"article\"]')\n            )\n            \n            logger.info(f\"  Found {len(article_elements)} potential articles\")\n            \n            for element in article_elements[:limit * 2]:\n                if len(articles) >= limit:\n                    break\n                \n                try:\n                    article_data = self._parse_article_card(element)\n                    \n                    if not article_data:\n                        continue\n                    \n                    # Fetch full article content\n                    content = self._fetch_article_content(article_data['url'])\n                    \n                    if content and len(content) > 100:\n                        article = ArticlePayload(\n                            source_name=self.source_name,\n                            language=self.language,\n                            needs_translation=self.needs_translation,\n                            url=article_data['url'],\n                            title=article_data['title'],\n                            content=content,\n                            published_at=article_data.get('published_date'),\n                            author=article_data.get('author'),\n                            image_url=article_data.get('image_url')\n                        )\n                        articles.append(article)\n                        logger.info(f\"  ‚úÖ {article_data['title'][:50]}...\")\n                        \n                except Exception as e:\n                    logger.error(f\"  Error parsing article element: {e}\")\n                    continue\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} English articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _parse_article_card(self, element) -> Optional[dict]:\n        \"\"\"Parse article card from listing page\"\"\"\n        try:\n            # Find title\n            title_elem = (\n                element.select_one('h2') or\n                element.select_one('h3') or\n                element.select_one('.title') or\n                element.select_one('.headline') or\n                element.select_one('[class*=\"title\"]')\n            )\n            \n            if not title_elem:\n                return None\n            \n            title = title_elem.get_text(strip=True)\n            \n            if len(title) < 10:\n                return None\n            \n            # Find link\n            link_elem = title_elem.find_parent('a') or element.select_one('a')\n            \n            if not link_elem:\n                return None\n            \n            url = link_elem.get('href')\n            if not url:\n                return None\n            \n            # Make URL absolute\n            if not url.startswith('http'):\n                base_url = \"https://www.thedrinksreport.com\"\n                url = base_url + url if url.startswith('/') else f\"{base_url}/{url}\"\n            \n            # Find image (optional)\n            img_elem = element.select_one('img')\n            image_url = None\n            if img_elem:\n                image_url = img_elem.get('src') or img_elem.get('data-src')\n                if image_url and not image_url.startswith('http'):\n                    if image_url.startswith('//'):\n                        image_url = f\"https:{image_url}\"\n                    else:\n                        image_url = f\"https://www.thedrinksreport.com{image_url}\"\n            \n            # Find date (optional)\n            date_elem = element.select_one('.date') or element.select_one('time') or element.select_one('.published')\n            published_date = None\n            if date_elem:\n                published_date = date_elem.get_text(strip=True)\n                if not published_date and date_elem.has_attr('datetime'):\n                    published_date = date_elem['datetime']\n            \n            # Find author (optional)\n            author_elem = element.select_one('.author') or element.select_one('.byline')\n            author = None\n            if author_elem:\n                author = author_elem.get_text(strip=True)\n            \n            return {\n                'title': title,\n                'url': url,\n                'image_url': image_url,\n                'published_date': published_date,\n                'author': author\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article card: {e}\")\n            return None\n    \n    def _fetch_article_content(self, url: str) -> Optional[str]:\n        \"\"\"Fetch full article content from article page\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article content\n            content_elem = (\n                soup.select_one('.article-body') or\n                soup.select_one('.entry-content') or\n                soup.select_one('.post-content') or\n                soup.select_one('article .content') or\n                soup.select_one('article') or\n                soup.select_one('[class*=\"content\"]')\n            )\n            \n            if not content_elem:\n                logger.warning(f\"  Could not find content container for: {url}\")\n                return None\n            \n            # Remove unwanted elements\n            for unwanted in content_elem.select('script, style, aside, .ads, .advertisement, .social-share, nav, footer, .related, .comments'):\n                unwanted.decompose()\n            \n            # Extract text\n            content = content_elem.get_text(separator='\\n', strip=True)\n            \n            # Clean text\n            content = self._clean_text(content)\n            \n            return content\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Clean scraped text\"\"\"\n        if not text:\n            return \"\"\n        \n        # Remove extra whitespace\n        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n        text = '\\n\\n'.join(lines)\n        \n        # Remove non-breaking spaces\n        text = text.replace('\\xa0', ' ')\n        text = text.replace('\\u200b', '')\n        \n        # Remove very short lines (likely UI elements)\n        lines = text.split('\\n\\n')\n        lines = [line for line in lines if len(line) > 15]\n        text = '\\n\\n'.join(lines)\n        \n        # Remove multiple spaces\n        import re\n        text = re.sub(r' +', ' ', text)\n        \n        return text.strip()\n","path":null,"size_bytes":8511,"size_tokens":null},"frontend/postcss.config.js":{"content":"export default {\n  plugins: {\n    '@tailwindcss/postcss': {},\n    autoprefixer: {},\n  },\n}\n","path":null,"size_bytes":91,"size_tokens":null},"backend/services/scrapers/delo_ua.py":{"content":"\"\"\"\nDelo.ua scraper - Ukrainian source, NO translation needed\nScrapes alcohol/business news from delo.ua using Playwright for JavaScript rendering\n\"\"\"\n\nimport logging\nfrom typing import List, Optional\nfrom playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError\nfrom bs4 import BeautifulSoup\nimport requests\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass DeloUaScraper(ScraperBase):\n    \"\"\"Scraper for Delo.ua alcohol/business section (Ukrainian source)\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"Delo.ua\"\n    \n    def get_language(self) -> str:\n        return \"uk\"\n    \n    def get_needs_translation(self) -> bool:\n        return False\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape articles from Delo.ua retail section using Playwright\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name} (Ukrainian) with Playwright...\")\n            section_url = \"https://delo.ua/business/retail/\"\n            \n            with sync_playwright() as p:\n                # Launch browser in headless mode\n                browser = p.chromium.launch(headless=True)\n                page = browser.new_page(user_agent=self.user_agent)\n                \n                try:\n                    # Navigate to the page and wait for content to load\n                    page.goto(section_url, wait_until='networkidle', timeout=30000)\n                    \n                    # Wait for article elements to appear (try multiple selectors)\n                    try:\n                        page.wait_for_selector('article, .article-item, .news-item, a[href*=\"/business/\"]', timeout=10000)\n                    except PlaywrightTimeoutError:\n                        logger.warning(f\"  Timeout waiting for articles to load\")\n                    \n                    # Get the fully rendered HTML\n                    html_content = page.content()\n                    \n                finally:\n                    browser.close()\n            \n            # Parse with BeautifulSoup\n            soup = BeautifulSoup(html_content, 'html.parser')\n            \n            # Find article headings (h2, h3)\n            headings = soup.select('h2, h3')\n            \n            logger.info(f\"  Found {len(headings)} potential articles\")\n            \n            for heading in headings[:limit * 2]:  # Get more than needed to account for filtering\n                if len(articles) >= limit:\n                    break\n                \n                try:\n                    article_data = self._parse_article_heading(heading)\n                    \n                    if not article_data:\n                        continue\n                    \n                    # Skip navigation links (too short URLs or category pages)\n                    if article_data['url'].endswith(('/business/', '/retail/', '/news-feed/', '/articles/')):\n                        continue\n                    \n                    # Fetch full article content (pass title for duplicate removal)\n                    content = self._fetch_article_content(article_data['url'], article_data['title'])\n                    \n                    if content and len(content) > 100:  # Minimum content length\n                        article = ArticlePayload(\n                            source_name=self.source_name,\n                            language=self.language,\n                            needs_translation=self.needs_translation,\n                            url=article_data['url'],\n                            title=article_data['title'],\n                            content=content,\n                            published_at=article_data.get('published_date'),\n                            image_url=article_data.get('image_url')\n                        )\n                        articles.append(article)\n                        logger.info(f\"  ‚úÖ {article_data['title'][:50]}...\")\n                        \n                except Exception as e:\n                    logger.error(f\"  Error parsing article heading: {e}\")\n                    continue\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} Ukrainian articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _parse_article_heading(self, heading) -> Optional[dict]:\n        \"\"\"Parse article data from heading element\"\"\"\n        try:\n            # Get title from heading\n            title = heading.get_text(strip=True)\n            \n            # Remove merged category/metadata prefixes\n            title = title.replace('–ê–∫—Ç—É–∞–ª—å–Ω–æ–ê–∫—Ç—É–∞–ª—å–Ω–æ', '').strip()\n            title = title.replace('–ê–∫—Ç—É–∞–ª—å–Ω–æ', '').strip()\n            \n            # Remove \"–ù–æ–≤–∏–Ω–∏ –∫–æ–º–ø–∞–Ω—ñ–π\" and similar prefixes (with or without space)\n            import re\n            title = re.sub(r'^–ù–æ–≤–∏–Ω[–∏—ñ]\\s*–∫–æ–º–ø–∞–Ω—ñ–π\\s*', '', title).strip()\n            title = re.sub(r'^–ù–æ–≤–∏–Ω–∏\\s*', '', title).strip()\n            title = re.sub(r'^–ö–∞—Ç–µ–≥–æ—Ä—ñ—è\\s*', '', title).strip()\n            \n            if len(title) < 15:  # Too short to be a real article title\n                return None\n            \n            # Find link - check if heading is inside a link or has a link child\n            link_elem = heading.find_parent('a') or heading.find('a')\n            \n            if not link_elem:\n                return None\n            \n            url = link_elem.get('href')\n            if not url:\n                return None\n            \n            # Make URL absolute\n            if not url.startswith('http'):\n                base_url = \"https://delo.ua\"\n                url = base_url + url if url.startswith('/') else f\"{base_url}/{url}\"\n            \n            # Find image (optional) - look in parent container\n            parent_container = heading.find_parent('div') or heading.find_parent('article')\n            img_elem = None\n            if parent_container:\n                img_elem = parent_container.select_one('img')\n            \n            image_url = None\n            if img_elem:\n                image_url = img_elem.get('src') or img_elem.get('data-src')\n                if image_url and not image_url.startswith('http'):\n                    image_url = f\"https:{image_url}\" if image_url.startswith('//') else f\"https://delo.ua{image_url}\"\n            \n            # Find date (optional)\n            date_elem = None\n            if parent_container:\n                date_elem = parent_container.select_one('.date') or parent_container.select_one('.time') or parent_container.select_one('time')\n            \n            published_date = None\n            if date_elem:\n                published_date = date_elem.get_text(strip=True)\n            \n            return {\n                'title': title,\n                'url': url,\n                'image_url': image_url,\n                'published_date': published_date\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article heading: {e}\")\n            return None\n    \n    def _fetch_article_content(self, url: str, title: str = \"\") -> Optional[str]:\n        \"\"\"Fetch full article content from article page\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article content\n            content_elem = (\n                soup.select_one('.article-content') or\n                soup.select_one('.post-content') or\n                soup.select_one('.entry-content') or\n                soup.select_one('article') or\n                soup.select_one('.article-body') or\n                soup.select_one('[class*=\"content\"]')\n            )\n            \n            if not content_elem:\n                logger.warning(f\"  Could not find content container for: {url}\")\n                return None\n            \n            # Remove unwanted elements (ads, scripts, social, etc.)\n            for unwanted in content_elem.select('script, style, aside, .ads, .advertisement, .related, .comments, .share, .social'):\n                unwanted.decompose()\n            \n            # Remove metadata elements BEFORE extracting text\n            metadata_selectors = [\n                '.article-meta', '.meta', '.metadata', '.post-meta', '.entry-meta',\n                '.category', '.post-category', '.article-category',\n                '.date', '.post-date', '.published-date', '.article-date',\n                '.author', '.post-author', '.article-author',\n                '.tags', '.post-tags', '.article-tags',\n                'time', '.time', '.timestamp'\n            ]\n            for selector in metadata_selectors:\n                for element in content_elem.select(selector):\n                    element.decompose()\n            \n            # Extract text\n            content = content_elem.get_text(separator='\\n', strip=True)\n            \n            # Clean content (remove metadata patterns, duplicate title, fix formatting)\n            content = self._clean_content(content, title)\n            \n            return content\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_content(self, content: str, title: str) -> str:\n        \"\"\"Clean Delo.ua content with aggressive line joining to fix chaotic breaks\"\"\"\n        import re\n        \n        if not content:\n            return \"\"\n        \n        # Step 1: Remove merged category pattern at very start\n        # Pattern: \"–ù–æ–≤–∏–Ω–∏ –∫–æ–º–ø–∞–Ω—ñ–πTitle\" or \"–ù–æ–≤–∏–Ω—ñ –∫–æ–º–ø–∞–Ω—ñ–πTitle\" (typo variant)\n        content = re.sub(r'^–ù–æ–≤–∏–Ω[–∏—ñ] –∫–æ–º–ø–∞–Ω—ñ–π\\s*' + re.escape(title) if title else '', title if title else '', content)\n        content = re.sub(r'^–ù–æ–≤–∏–Ω[–∏—ñ] –∫–æ–º–ø–∞–Ω—ñ–π\\s*', '', content)\n        \n        # Step 2: Remove metadata patterns\n        metadata_patterns = [\n            r'^–ö–∞—Ç–µ–≥–æ—Ä—ñ—è\\s*\\n\\s*[^\\n]*\\n',\n            r'^–î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó\\s*\\n\\s*[^\\n]*\\n',\n            r'^–ù–æ–≤–∏–Ω–∏\\s*\\n',\n            r'^\\d{1,2}\\s+\\w+\\s+\\d{2}:\\d{2}\\s*\\n',\n            r'^\\d{1,2}\\s+(—Å—ñ—á–Ω—è|–ª—é—Ç–æ–≥–æ|–±–µ—Ä–µ–∑–Ω—è|–∫–≤—ñ—Ç–Ω—è|—Ç—Ä–∞–≤–Ω—è|—á–µ—Ä–≤–Ω—è|–ª–∏–ø–Ω—è|—Å–µ—Ä–ø–Ω—è|–≤–µ—Ä–µ—Å–Ω—è|–∂–æ–≤—Ç–Ω—è|–ª–∏—Å—Ç–æ–ø–∞–¥–∞|–≥—Ä—É–¥–Ω—è)\\s+\\d{2}:\\d{2}\\s*\\n',\n            r'^\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\s*\\n',\n            r'–ó–º—ñ–Ω–∏—Ç–∏ –º–æ–≤—É.*?\\n',\n            r'–ß–∏—Ç–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º.*?\\n',\n            r'^–ê–≤—Ç–æ—Ä:?\\s*[^\\n]*\\n',\n            r'^–î–∂–µ—Ä–µ–ª–æ:?\\s*[^\\n]*\\n',\n            r'^–§–æ—Ç–æ:?\\s*[^\\n]*\\n',\n        ]\n        \n        for pattern in metadata_patterns:\n            content = re.sub(pattern, '', content, flags=re.MULTILINE | re.IGNORECASE)\n        \n        # Words/phrases to skip as standalone lines (metadata/UI)\n        skip_words = {\n            '–ö–∞—Ç–µ–≥–æ—Ä—ñ—è', '–ù–æ–≤–∏–Ω–∏', '–ù–æ–≤–∏–Ω–∏ –∫–æ–º–ø–∞–Ω—ñ–π', '–ù–æ–≤–∏–Ω—ñ –∫–æ–º–ø–∞–Ω—ñ–π',\n            '–î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó', '–ê–≤—Ç–æ—Ä', '–î–∂–µ—Ä–µ–ª–æ', '–§–æ—Ç–æ', '–¢–µ–≥–∏', \n            '–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂', '–ü–æ–¥—ñ–ª–∏—Ç–∏—Å—è', 'Share', 'Facebook', 'Twitter', \n            'Telegram', 'Viber', '–ó–º—ñ–Ω–∏—Ç–∏ –º–æ–≤—É', '–ß–∏—Ç–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º', \n            '–ß–∏—Ç–∞—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é', '–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ', '–ü—ñ–¥–ø–∏—Å–∞—Ç–∏—Å—è', '–†–µ–∫–ª–∞–º–∞', \n            '–ë—ñ–ª—å—à–µ –Ω–æ–≤–∏–Ω', '–ü–æ–ø—É–ª—è—Ä–Ω–µ', '–ê–∫—Ç—É–∞–ª—å–Ω–æ'\n        }\n        \n        # Step 3: Get all lines, remove metadata, COMPLETELY REMOVE duplicate titles\n        # Title is added separately during posting, so we remove ALL occurrences from content\n        lines = content.split('\\n')\n        filtered_lines = []\n        \n        for line in lines:\n            line_stripped = line.strip()\n            \n            # Skip empty lines\n            if not line_stripped:\n                continue\n            \n            # Skip metadata words\n            if line_stripped in skip_words:\n                continue\n            \n            # Skip lines containing only metadata\n            skip_line = False\n            for skip_word in skip_words:\n                if skip_word in line_stripped and len(line_stripped) < 60:\n                    skip_line = True\n                    break\n            if skip_line:\n                continue\n            \n            # Skip date-only lines\n            if re.match(r'^\\d{1,2}\\s+\\w+\\s+\\d{2}:\\d{2}$', line_stripped):\n                continue\n            \n            # Skip very short non-sentence lines\n            if len(line_stripped) < 8 and not line_stripped.endswith(('.', '!', '?', '\"', '¬ª')):\n                continue\n            \n            # REMOVE ALL occurrences of the exact title (title is added separately during posting)\n            if title and line_stripped == title:\n                continue\n            \n            filtered_lines.append(line_stripped)\n        \n        # Step 4: AGGRESSIVE LINE JOINING - join ALL lines into single text block\n        full_text = ' '.join(filtered_lines)\n        \n        # Step 4.5: Remove title if it appears at the very start of content\n        # This handles cases like \"Title –¶—å–æ–≥–æ—Ä—ñ—á–Ω–∞...\" where title is merged with first sentence\n        if title and full_text.startswith(title):\n            full_text = full_text[len(title):].lstrip()\n        \n        # Step 4.6: Remove image captions (pattern: \"Caption text / Photo credit\")\n        # Only remove when it's clearly a standalone caption, not inline mention\n        # Pattern must be: short caption (no punctuation) + \" / \" + credit at END of caption\n        # This avoids matching legitimate sentences like \"–ó–∞ –¥–∞–Ω–∏–º–∏ Reuters / AFP...\"\n        photo_credits = ['Depositphotos', 'Getty Images', 'Unsplash', '–£–ù–Ü–ê–ù', 'UNIAN', 'Shutterstock', 'iStock']\n        for credit in photo_credits:\n            # Only match at the very start: \"Caption / Credit \" followed by capital letter\n            # This targets patterns like \"–Ü–Ω–¥–µ–∫—Å —Å–∞–º–æ–ø–æ—á—É—Ç—Ç—è —Ä–∏—Ç–µ–π–ª—É –∑—Ä—ñ—Å / Depositphotos –Ü–Ω–¥–µ–∫—Å...\"\n            caption_pattern = f'^[–ê-–Ø–Ü–á–Ñ“ê–∞-—è—ñ—ó—î“ëA-Za-z0-9\\\\s]{{5,60}}\\\\s*/\\\\s*{re.escape(credit)}\\\\s+(?=[–ê-–Ø–Ü–á–Ñ“êA-Z])'\n            full_text = re.sub(caption_pattern, '', full_text, flags=re.IGNORECASE)\n        \n        # Step 5: Fix spacing issues\n        full_text = full_text.replace('\\xa0', ' ')\n        full_text = full_text.replace('\\u200b', '')\n        full_text = re.sub(r'  +', ' ', full_text)\n        \n        # Fix spacing around quotes\n        full_text = re.sub(r'\\s+\"', ' \"', full_text)\n        full_text = re.sub(r'\"\\s+', '\" ', full_text)\n        full_text = re.sub(r'\\s+¬´', ' ¬´', full_text)\n        full_text = re.sub(r'¬ª\\s+', '¬ª ', full_text)\n        \n        # Step 6: Rebuild paragraph structure at proper sentence boundaries\n        # After quote + attribution + punctuation\n        full_text = re.sub(r'([.!?][¬ª\"]\\s*,?\\s*)([A-Z–ê-–Ø–Ü–á–Ñ“ê])', r'\\1\\n\\n\\2', full_text)\n        \n        # After period/question/exclamation + space + capital letter\n        full_text = re.sub(r'([.!?]\\s+)([A-Z–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ëa-z])', r'\\1\\n\\n\\2', full_text)\n        \n        # Before section headers (capital word + colon)\n        full_text = re.sub(r'\\.(\\s+)([–ê-–Ø–Ü–á–Ñ“ê][^\\n]{10,80}:)', r'.\\n\\n\\2', full_text)\n        \n        # Step 7: Group into reasonable paragraphs (2-3 sentences each)\n        paragraphs = full_text.split('\\n\\n')\n        grouped_paragraphs = []\n        current_group = []\n        \n        for para in paragraphs:\n            para = para.strip()\n            if not para:\n                continue\n            \n            current_group.append(para)\n            \n            # Create paragraph break after 2 sentences or if long enough\n            total_len = sum(len(p) for p in current_group)\n            if len(current_group) >= 2 or total_len > 350:\n                grouped_paragraphs.append(' '.join(current_group))\n                current_group = []\n        \n        # Add remaining\n        if current_group:\n            grouped_paragraphs.append(' '.join(current_group))\n        \n        full_text = '\\n\\n'.join(grouped_paragraphs)\n        \n        # Step 8: Clean up\n        full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n        full_text = re.sub(r'  +', ' ', full_text)\n        \n        # Step 9: Remove trailing incomplete sentences\n        lines = full_text.split('\\n')\n        if lines:\n            last_line = lines[-1].strip()\n            if len(last_line) < 50 and not last_line.endswith(('.', '!', '?', '\"', '¬ª')):\n                lines = lines[:-1]\n        \n        full_text = '\\n'.join(lines)\n        \n        return full_text.strip()\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Legacy method - redirects to _clean_content\"\"\"\n        return self._clean_content(text, \"\")\n","path":null,"size_bytes":17277,"size_tokens":null},"replit.md":{"content":"# Gradus Media AI Agent\n\n## Overview\nThis project is an intelligent content management system designed to automate and streamline social media content creation, translation, image generation, and publishing for Gradus Media. It integrates Claude AI for content and translation, and DALL-E 3 for image generation. The system features a React dashboard for human-in-the-loop content review and approval, a PostgreSQL database, and integrations with Facebook, LinkedIn, and Telegram. Its primary goal is to ensure high-quality content delivery across social platforms with efficient workflow automation and human oversight.\n\n## User Preferences\n- Language: Ukrainian for content output\n- Tech stack: Python (FastAPI), React, PostgreSQL\n- Deployment: Replit native (no Docker)\n- Focus: Content quality with approval workflow\n\n## System Architecture\nThe system employs a FastAPI backend and a React frontend to manage a sophisticated content pipeline.\n\n**UI/UX Decisions:**\n- **Frontend:** React with Vite, styled using Tailwind CSS for a modern, responsive dashboard experience.\n- **Dashboard:** Centralized interface for statistics, AI chat interaction, and content approval workflows.\n\n**Technical Implementations:**\n- **Backend:** FastAPI for high-performance, asynchronous API operations.\n- **Database:** PostgreSQL with SQLAlchemy ORM for robust data persistence.\n- **Human-in-the-loop Workflow:** Critical for content quality control, enabling review and approval before publishing.\n- **Service-Oriented Architecture:** Modular design supporting extensibility and maintainability.\n- **Multi-Source Scraping Architecture:** Modular scraper system with ScraperManager coordinating 5 active sources:\n    - **English sources (need translation):**\n      - The Spirits Business - Professional industry news\n      - Just Drinks - Lighter, more accessible drinks industry content\n      - Drinks International - Vodka and spirits industry news\n    - **Ukrainian sources (no translation):**\n      - Delo.ua - Ukrainian retail and business news (uses Playwright for JavaScript rendering)\n      - HoReCa-–£–∫—Ä–∞—ó–Ω–∞ - Ukrainian HoReCa industry news\n- **Automated Content Pipeline:** 24/7 automation via APScheduler with platform-optimized scheduling:\n    - **Platform-Specific Scraping:**\n      - LinkedIn sources (Mon/Wed/Fri 1:00 AM): The Spirits Business, Drinks International\n      - Facebook sources (Daily 2:00 AM): Delo.ua, HoReCa-–£–∫—Ä–∞—ó–Ω–∞, Just Drinks\n    - **Startup Catch-Up:** On backend restart, automatically checks for missed scraping:\n      - Facebook: catch-up if >24h since last scrape\n      - LinkedIn: catch-up if >48h since last scrape (runs any day if overdue)\n      - Per-platform tracking ensures one platform's recent activity doesn't suppress another\n    - **Processing:**\n      - AI-driven translation (3x/day at 6am, 2pm, 8pm - only for English sources)\n      - Image generation (3x/day at 6:15am, 2:15pm, 8:15pm - for both languages)\n    - **Maintenance:**\n      - Daily cleanup of rejected content (3:00 AM)\n      - API monitoring (8:00 AM)\n- **Telegram Quick Approval:** Allows one-click content approval/rejection directly from Telegram notifications, including image previews.\n- **Image Generation:** Utilizes Claude AI to craft DALL-E 3 prompts for 1024x1024, text-free, professional social media images.\n- **Permanent Image Storage:** DALL-E generated images are downloaded and stored locally in `attached_assets/generated_images/` to prevent link expiration.\n- **Scheduled Posting:** Approved content is automatically posted at optimal engagement times:\n    - **Facebook:** Daily at 18:00.\n    - **LinkedIn:** Monday, Wednesday, Friday at 09:00.\n- **LinkedIn Integration:** Supports organization page posting with robust asset upload, local image fallback, and graceful degradation to text-only posts.\n- **API Monitoring:** Daily health checks for all external services with proactive Telegram alerts for issues.\n\n**Feature Specifications:**\n- **Content Management:** API for managing pending, approved, and rejected content with editing and history.\n- **AI Services:** Endpoints for Claude AI chat and English-to-Ukrainian translation.\n- **News Scraper:** Extracts clean content and metadata, with year-agnostic URL matching and duplicate detection. Includes Playwright headless browser support for JavaScript-rendered sites.\n- **Notifications:** Telegram notifications for content status, approval, and rejection.\n- **Database Schema:** `ContentQueue` for reviewable content and `ApprovalLog` for audit trails, storing comprehensive metadata.\n\n## External Dependencies\n- **Claude AI (Anthropic):** Content generation, DALL-E prompt creation, English-to-Ukrainian translation.\n- **DALL-E 3 (OpenAI):** AI-powered image generation.\n- **PostgreSQL:** Primary database for all system data.\n- **Telegram Bot API:** Notifications, quick approval via inline keyboard, webhook callbacks.\n- **Facebook Graph API:** Authentication, testing, and scheduled posting to Facebook Pages.\n- **LinkedIn API v2:** OAuth 2.0 authentication and scheduled posting to LinkedIn organization pages.\n- **Trafilatura:** Used for clean content extraction by the news scraper.","path":null,"size_bytes":5183,"size_tokens":null},"frontend/src/pages/ChatPage.jsx":{"content":"import { useState } from 'react'\nimport { Send, Languages } from 'lucide-react'\nimport axios from 'axios'\nimport { API_URL } from '../lib/api'\n\nfunction ChatPage() {\n  const [message, setMessage] = useState('')\n  const [textToTranslate, setTextToTranslate] = useState('')\n  const [chatResponse, setChatResponse] = useState('')\n  const [translation, setTranslation] = useState('')\n  const [loading, setLoading] = useState(false)\n\n  const handleChat = async (e) => {\n    e.preventDefault()\n    if (!message.trim()) return\n\n    setLoading(true)\n    try {\n      const response = await axios.post(`${API_URL}/chat`, {\n        message: message\n      })\n      setChatResponse(response.data.response)\n    } catch (error) {\n      setChatResponse(`Error: ${error.response?.data?.detail || error.message}`)\n    }\n    setLoading(false)\n  }\n\n  const handleTranslate = async (e) => {\n    e.preventDefault()\n    if (!textToTranslate.trim()) return\n\n    setLoading(true)\n    try {\n      const response = await axios.post(`${API_URL}/translate`, {\n        text: textToTranslate\n      })\n      setTranslation(response.data.translation)\n    } catch (error) {\n      setTranslation(`Error: ${error.response?.data?.detail || error.message}`)\n    }\n    setLoading(false)\n  }\n\n  return (\n    <div>\n      <h1 className=\"text-3xl font-bold text-gray-900 mb-6\">Chat & Translation</h1>\n\n      <div className=\"grid grid-cols-1 lg:grid-cols-2 gap-6\">\n        <div className=\"bg-white rounded-lg shadow p-6\">\n          <div className=\"flex items-center space-x-2 mb-4\">\n            <Send className=\"text-purple-600\" />\n            <h2 className=\"text-xl font-semibold\">Chat with Claude</h2>\n          </div>\n\n          <form onSubmit={handleChat} className=\"space-y-4\">\n            <div>\n              <label className=\"block text-sm font-medium text-gray-700 mb-2\">\n                Your Message\n              </label>\n              <textarea\n                value={message}\n                onChange={(e) => setMessage(e.target.value)}\n                className=\"w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent\"\n                rows=\"4\"\n                placeholder=\"Type your message here...\"\n              />\n            </div>\n\n            <button\n              type=\"submit\"\n              disabled={loading}\n              className=\"w-full bg-purple-600 text-white py-2 px-4 rounded-lg hover:bg-purple-700 disabled:bg-gray-400 transition\"\n            >\n              {loading ? 'Processing...' : 'Send Message'}\n            </button>\n          </form>\n\n          {chatResponse && (\n            <div className=\"mt-4 p-4 bg-gray-50 rounded-lg\">\n              <p className=\"text-sm font-medium text-gray-700 mb-2\">Response:</p>\n              <p className=\"text-gray-900 whitespace-pre-wrap\">{chatResponse}</p>\n            </div>\n          )}\n        </div>\n\n        <div className=\"bg-white rounded-lg shadow p-6\">\n          <div className=\"flex items-center space-x-2 mb-4\">\n            <Languages className=\"text-purple-600\" />\n            <h2 className=\"text-xl font-semibold\">English ‚Üí Ukrainian</h2>\n          </div>\n\n          <form onSubmit={handleTranslate} className=\"space-y-4\">\n            <div>\n              <label className=\"block text-sm font-medium text-gray-700 mb-2\">\n                English Text\n              </label>\n              <textarea\n                value={textToTranslate}\n                onChange={(e) => setTextToTranslate(e.target.value)}\n                className=\"w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent\"\n                rows=\"4\"\n                placeholder=\"Enter English text to translate...\"\n              />\n            </div>\n\n            <button\n              type=\"submit\"\n              disabled={loading}\n              className=\"w-full bg-purple-600 text-white py-2 px-4 rounded-lg hover:bg-purple-700 disabled:bg-gray-400 transition\"\n            >\n              {loading ? 'Translating...' : 'Translate to Ukrainian'}\n            </button>\n          </form>\n\n          {translation && (\n            <div className=\"mt-4 p-4 bg-gray-50 rounded-lg\">\n              <p className=\"text-sm font-medium text-gray-700 mb-2\">Ukrainian Translation:</p>\n              <p className=\"text-gray-900 whitespace-pre-wrap text-lg\">{translation}</p>\n            </div>\n          )}\n        </div>\n      </div>\n    </div>\n  )\n}\n\nexport default ChatPage\n","path":null,"size_bytes":4483,"size_tokens":null},"backend/services/api_token_monitor.py":{"content":"import os\nimport requests\nimport logging\nfrom typing import Dict, List, Optional\nfrom datetime import datetime\nfrom anthropic import Anthropic\nfrom openai import OpenAI\n\nlogger = logging.getLogger(__name__)\n\nclass APITokenMonitor:\n    \"\"\"\n    Monitor all API tokens, quotas, and expiration dates\n    Send proactive Telegram alerts before issues occur\n    \"\"\"\n    \n    def __init__(self):\n        self.telegram_bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n        self.telegram_chat_id = os.getenv('TELEGRAM_CHAT_ID')\n        \n        self.days_warning = 7\n        self.quota_warning_percent = 80\n        \n    def check_all_services(self) -> Dict:\n        \"\"\"\n        Check all API services\n        Returns summary of all checks\n        \"\"\"\n        \n        results = {\n            'timestamp': datetime.now().isoformat(),\n            'services': {},\n            'warnings': [],\n            'errors': []\n        }\n        \n        services_to_check = [\n            ('anthropic', self.check_anthropic_api),\n            ('openai', self.check_openai_api),\n            ('facebook', self.check_facebook_token),\n            ('telegram', self.check_telegram_bot)\n        ]\n        \n        for service_name, check_function in services_to_check:\n            try:\n                service_status = check_function()\n                results['services'][service_name] = service_status\n                \n                if service_status.get('warning'):\n                    results['warnings'].append({\n                        'service': service_name,\n                        'message': service_status.get('warning_message')\n                    })\n                \n                if service_status.get('status') == 'error':\n                    results['errors'].append({\n                        'service': service_name,\n                        'message': service_status.get('error_message')\n                    })\n                    \n            except Exception as e:\n                logger.error(f\"Error checking {service_name}: {e}\")\n                results['errors'].append({\n                    'service': service_name,\n                    'message': str(e)\n                })\n        \n        if results['warnings'] or results['errors']:\n            self._send_alert_notification(results)\n        \n        return results\n    \n    def check_anthropic_api(self) -> Dict:\n        \"\"\"Check Claude API status and quota\"\"\"\n        \n        api_key = os.getenv('ANTHROPIC_API_KEY')\n        \n        if not api_key:\n            return {\n                'status': 'error',\n                'service_name': 'Claude AI (Anthropic)',\n                'error_message': 'No Anthropic API key configured'\n            }\n        \n        try:\n            client = Anthropic(api_key=api_key)\n            \n            response = client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            return {\n                'status': 'healthy',\n                'service_name': 'Claude AI (Anthropic)',\n                'is_valid': True,\n                'last_checked': datetime.now().isoformat(),\n                'console_url': 'https://console.anthropic.com/settings/usage',\n                'note': 'API key valid. Check usage at console.anthropic.com'\n            }\n            \n        except Exception as e:\n            error_msg = str(e)\n            \n            if 'invalid' in error_msg.lower() or 'authentication' in error_msg.lower():\n                return {\n                    'status': 'error',\n                    'service_name': 'Claude AI (Anthropic)',\n                    'is_valid': False,\n                    'error_message': 'API key invalid or expired',\n                    'action_required': 'Update ANTHROPIC_API_KEY in Replit Secrets',\n                    'console_url': 'https://console.anthropic.com/settings/keys'\n                }\n            elif 'quota' in error_msg.lower() or 'limit' in error_msg.lower():\n                return {\n                    'status': 'warning',\n                    'service_name': 'Claude AI (Anthropic)',\n                    'warning': True,\n                    'warning_message': 'API quota may be exhausted',\n                    'action_required': 'Add credits at console.anthropic.com',\n                    'console_url': 'https://console.anthropic.com/settings/usage'\n                }\n            else:\n                return {\n                    'status': 'error',\n                    'service_name': 'Claude AI (Anthropic)',\n                    'error_message': error_msg,\n                    'console_url': 'https://console.anthropic.com'\n                }\n    \n    def check_openai_api(self) -> Dict:\n        \"\"\"Check OpenAI (DALL-E) API status and quota\"\"\"\n        \n        api_key = os.getenv('OPENAI_API_KEY')\n        \n        if not api_key:\n            return {\n                'status': 'error',\n                'service_name': 'OpenAI (DALL-E)',\n                'error_message': 'No OpenAI API key configured'\n            }\n        \n        try:\n            client = OpenAI(api_key=api_key)\n            \n            models = client.models.list()\n            \n            dalle_available = any('dall-e' in model.id for model in models.data)\n            \n            return {\n                'status': 'healthy',\n                'service_name': 'OpenAI (DALL-E)',\n                'is_valid': True,\n                'dalle_available': dalle_available,\n                'last_checked': datetime.now().isoformat(),\n                'console_url': 'https://platform.openai.com/usage',\n                'billing_url': 'https://platform.openai.com/settings/organization/billing',\n                'note': 'API key valid. Check usage and add credits at platform.openai.com'\n            }\n            \n        except Exception as e:\n            error_msg = str(e)\n            \n            if 'invalid' in error_msg.lower() or 'authentication' in error_msg.lower():\n                return {\n                    'status': 'error',\n                    'service_name': 'OpenAI (DALL-E)',\n                    'is_valid': False,\n                    'error_message': 'API key invalid or expired',\n                    'action_required': 'Update OPENAI_API_KEY in Replit Secrets',\n                    'console_url': 'https://platform.openai.com/api-keys'\n                }\n            elif 'quota' in error_msg.lower() or 'insufficient' in error_msg.lower():\n                return {\n                    'status': 'warning',\n                    'service_name': 'OpenAI (DALL-E)',\n                    'warning': True,\n                    'warning_message': 'API quota exhausted or payment issue',\n                    'action_required': 'Add credits at platform.openai.com/billing',\n                    'console_url': 'https://platform.openai.com/settings/organization/billing'\n                }\n            else:\n                return {\n                    'status': 'error',\n                    'service_name': 'OpenAI (DALL-E)',\n                    'error_message': error_msg,\n                    'console_url': 'https://platform.openai.com'\n                }\n    \n    def check_facebook_token(self) -> Dict:\n        \"\"\"Check Facebook Page Access Token\"\"\"\n        \n        page_access_token = os.getenv('FACEBOOK_PAGE_ACCESS_TOKEN')\n        \n        if not page_access_token:\n            return {\n                'status': 'error',\n                'service_name': 'Facebook Page Token',\n                'error_message': 'No Facebook token configured'\n            }\n        \n        try:\n            url = f'https://graph.facebook.com/v18.0/me'\n            params = {'access_token': page_access_token}\n            \n            response = requests.get(url, params=params, timeout=10)\n            result = response.json()\n            \n            if 'error' in result:\n                error = result['error']\n                if error.get('code') == 190:\n                    return {\n                        'status': 'error',\n                        'service_name': 'Facebook Page Token',\n                        'is_valid': False,\n                        'error_message': 'Token is invalid or expired',\n                        'action_required': 'Generate new Page Access Token',\n                        'console_url': 'https://developers.facebook.com/'\n                    }\n                else:\n                    return {\n                        'status': 'error',\n                        'service_name': 'Facebook Page Token',\n                        'error_message': error.get('message', 'Unknown error'),\n                        'console_url': 'https://developers.facebook.com/'\n                    }\n            \n            url_debug = f'https://graph.facebook.com/v18.0/debug_token'\n            params_debug = {\n                'input_token': page_access_token,\n                'access_token': page_access_token\n            }\n            \n            response_debug = requests.get(url_debug, params=params_debug, timeout=10)\n            debug_data = response_debug.json()\n            \n            if 'data' in debug_data:\n                token_data = debug_data['data']\n                expires_at = token_data.get('expires_at')\n                \n                if expires_at == 0:\n                    return {\n                        'status': 'healthy',\n                        'service_name': 'Facebook Page Token',\n                        'is_valid': True,\n                        'expires': 'Never',\n                        'last_checked': datetime.now().isoformat(),\n                        'console_url': 'https://developers.facebook.com/'\n                    }\n                else:\n                    expiry_date = datetime.fromtimestamp(expires_at)\n                    days_remaining = (expiry_date - datetime.now()).days\n                    \n                    if days_remaining < self.days_warning:\n                        return {\n                            'status': 'warning',\n                            'service_name': 'Facebook Page Token',\n                            'warning': True,\n                            'warning_message': f'Token expires in {days_remaining} days',\n                            'expires_at': expiry_date.isoformat(),\n                            'action_required': 'Renew Facebook token soon',\n                            'console_url': 'https://developers.facebook.com/'\n                        }\n                    else:\n                        return {\n                            'status': 'healthy',\n                            'service_name': 'Facebook Page Token',\n                            'is_valid': True,\n                            'days_remaining': days_remaining,\n                            'expires_at': expiry_date.isoformat(),\n                            'last_checked': datetime.now().isoformat(),\n                            'console_url': 'https://developers.facebook.com/'\n                        }\n            \n            return {\n                'status': 'healthy',\n                'service_name': 'Facebook Page Token',\n                'is_valid': True,\n                'last_checked': datetime.now().isoformat(),\n                'console_url': 'https://developers.facebook.com/'\n            }\n                \n        except Exception as e:\n            return {\n                'status': 'error',\n                'service_name': 'Facebook Page Token',\n                'error_message': str(e),\n                'console_url': 'https://developers.facebook.com/'\n            }\n    \n    def check_telegram_bot(self) -> Dict:\n        \"\"\"Check Telegram Bot token validity\"\"\"\n        \n        if not self.telegram_bot_token:\n            return {\n                'status': 'error',\n                'service_name': 'Telegram Bot',\n                'error_message': 'No Telegram bot token configured'\n            }\n        \n        try:\n            url = f\"https://api.telegram.org/bot{self.telegram_bot_token}/getMe\"\n            response = requests.get(url, timeout=10)\n            \n            if response.status_code == 200:\n                bot_info = response.json()\n                return {\n                    'status': 'healthy',\n                    'service_name': 'Telegram Bot',\n                    'is_valid': True,\n                    'bot_username': bot_info['result'].get('username'),\n                    'last_checked': datetime.now().isoformat()\n                }\n            else:\n                return {\n                    'status': 'error',\n                    'service_name': 'Telegram Bot',\n                    'is_valid': False,\n                    'error_message': 'Invalid bot token',\n                    'action_required': 'Update TELEGRAM_BOT_TOKEN in Replit Secrets'\n                }\n                \n        except Exception as e:\n            return {\n                'status': 'error',\n                'service_name': 'Telegram Bot',\n                'error_message': str(e)\n            }\n    \n    def _send_alert_notification(self, results: Dict):\n        \"\"\"Send Telegram alert for warnings or errors\"\"\"\n        \n        if not self.telegram_bot_token or not self.telegram_chat_id:\n            logger.warning(\"Telegram not configured, skipping alert notification\")\n            return\n        \n        warnings = results.get('warnings', [])\n        errors = results.get('errors', [])\n        \n        if not warnings and not errors:\n            return\n        \n        message_parts = [\"üö® <b>API Token Monitor Alert</b>\\n\"]\n        \n        if errors:\n            message_parts.append(f\"\\n‚ùå <b>ERRORS ({len(errors)}):</b>\")\n            for error in errors:\n                service = error['service'].upper()\n                msg = error['message']\n                message_parts.append(f\"‚Ä¢ {service}: {msg}\")\n        \n        if warnings:\n            message_parts.append(f\"\\n‚ö†Ô∏è <b>WARNINGS ({len(warnings)}):</b>\")\n            for warning in warnings:\n                service = warning['service'].upper()\n                msg = warning['message']\n                message_parts.append(f\"‚Ä¢ {service}: {msg}\")\n        \n        message_parts.append(\"\\n\\nüìä <b>Quick Links:</b>\")\n        message_parts.append(\"‚Ä¢ Claude: https://console.anthropic.com/settings/usage\")\n        message_parts.append(\"‚Ä¢ OpenAI: https://platform.openai.com/usage\")\n        message_parts.append(\"‚Ä¢ Facebook: https://developers.facebook.com/\")\n        \n        message_parts.append(f\"\\n‚è∞ Checked: {datetime.now().strftime('%H:%M, %d %b %Y')}\")\n        \n        message = \"\\n\".join(message_parts)\n        \n        try:\n            url = f\"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage\"\n            payload = {\n                \"chat_id\": self.telegram_chat_id,\n                \"text\": message,\n                \"parse_mode\": \"HTML\",\n                \"disable_web_page_preview\": True\n            }\n            \n            response = requests.post(url, json=payload, timeout=10)\n            \n            if response.status_code == 200:\n                logger.info(\"API monitor alert sent successfully to Telegram\")\n            else:\n                logger.error(f\"Failed to send Telegram alert: {response.text}\")\n                \n        except Exception as e:\n            logger.error(f\"Error sending Telegram alert: {e}\")\n    \n    def send_success_notification(self, results: Dict):\n        \"\"\"Send success notification (all services healthy)\"\"\"\n        \n        if not self.telegram_bot_token or not self.telegram_chat_id:\n            return\n        \n        services = results.get('services', {})\n        healthy_count = sum(1 for s in services.values() if s.get('status') == 'healthy')\n        \n        message = f\"\"\"‚úÖ <b>API Monitor: All Systems Operational</b>\n\nüìä Status: {healthy_count}/{len(services)} services healthy\n\n<b>Services Checked:</b>\n‚Ä¢ Claude AI (Anthropic) ‚úÖ\n‚Ä¢ OpenAI (DALL-E) ‚úÖ\n‚Ä¢ Facebook Page Token ‚úÖ\n‚Ä¢ Telegram Bot ‚úÖ\n\n‚è∞ {datetime.now().strftime('%H:%M, %d %b %Y')}\"\"\"\n        \n        try:\n            url = f\"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage\"\n            payload = {\n                \"chat_id\": self.telegram_chat_id,\n                \"text\": message,\n                \"parse_mode\": \"HTML\"\n            }\n            \n            requests.post(url, json=payload, timeout=10)\n            logger.info(\"API monitor success notification sent\")\n            \n        except Exception as e:\n            logger.error(f\"Error sending success notification: {e}\")\n\napi_token_monitor = APITokenMonitor()\n","path":null,"size_bytes":16642,"size_tokens":null},"frontend/tailwind.config.js":{"content":"/** @type {import('tailwindcss').Config} */\nexport default {\n  content: [\n    \"./index.html\",\n    \"./src/**/*.{js,ts,jsx,tsx}\",\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n","path":null,"size_bytes":182,"size_tokens":null},"backend/services/scrapers/drinks_international.py":{"content":"\"\"\"\nDrinks International scraper - English source, NEEDS translation\nScrapes vodka and spirits news from drinksint.com\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Optional\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass DrinksInternationalScraper(ScraperBase):\n    \"\"\"Scraper for Drinks International vodka news (English source)\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"Drinks International\"\n    \n    def get_language(self) -> str:\n        return \"en\"\n    \n    def get_needs_translation(self) -> bool:\n        return True\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape articles from Drinks International vodka news section\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name} (English)...\")\n            vodka_news_url = \"https://drinksint.com/news/categoryfront.php/id/209/Vodka_news.html\"\n            \n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(vodka_news_url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Find all links to articles (fullstory.php pattern)\n            all_links = soup.find_all('a', href=True)\n            article_links = []\n            \n            for link in all_links:\n                href = link.get('href', '')\n                if 'fullstory.php' in href:\n                    text = link.get_text(strip=True)\n                    if len(text) > 20:  # Real article titles are longer\n                        article_links.append(link)\n            \n            logger.info(f\"  Found {len(article_links)} potential articles\")\n            \n            for link_elem in article_links[:limit * 2]:  # Get more than needed for filtering\n                if len(articles) >= limit:\n                    break\n                \n                try:\n                    article_data = self._parse_article_link(link_elem)\n                    \n                    if not article_data:\n                        continue\n                    \n                    # Fetch full article content\n                    content = self._fetch_article_content(article_data['url'])\n                    \n                    if content and len(content) > 100:\n                        article = ArticlePayload(\n                            source_name=self.source_name,\n                            language=self.language,\n                            needs_translation=self.needs_translation,\n                            url=article_data['url'],\n                            title=article_data['title'],\n                            content=content,\n                            published_at=article_data.get('published_date'),\n                            image_url=article_data.get('image_url')\n                        )\n                        articles.append(article)\n                        logger.info(f\"  ‚úÖ {article_data['title'][:50]}...\")\n                        \n                except Exception as e:\n                    logger.error(f\"  Error parsing article link: {e}\")\n                    continue\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} English articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _parse_article_link(self, link_elem) -> Optional[dict]:\n        \"\"\"Parse article data from link element\"\"\"\n        try:\n            # Get title from link text\n            title = link_elem.get_text(strip=True)\n            \n            if len(title) < 20:  # Too short to be a real article title\n                return None\n            \n            # Get URL\n            url = link_elem.get('href')\n            if not url:\n                return None\n            \n            # Make URL absolute\n            if not url.startswith('http'):\n                base_url = \"https://drinksint.com\"\n                url = base_url + url if url.startswith('/') else f\"{base_url}/{url}\"\n            \n            # Find image (optional) - look in parent container\n            parent_container = link_elem.find_parent('div') or link_elem.find_parent('td')\n            img_elem = None\n            if parent_container:\n                img_elem = parent_container.select_one('img')\n            \n            image_url = None\n            if img_elem:\n                image_url = img_elem.get('src')\n                if image_url and not image_url.startswith('http'):\n                    if image_url.startswith('//'):\n                        image_url = f\"https:{image_url}\"\n                    else:\n                        image_url = f\"https://drinksint.com{image_url}\"\n            \n            return {\n                'title': title,\n                'url': url,\n                'image_url': image_url,\n                'published_date': None  # Date extraction can be added later if needed\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article link: {e}\")\n            return None\n    \n    def _fetch_article_content(self, url: str) -> Optional[str]:\n        \"\"\"Fetch full article content from article page\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article content\n            content_elem = (\n                soup.select_one('.article-body') or\n                soup.select_one('.story-body') or\n                soup.select_one('.entry-content') or\n                soup.select_one('.post-content') or\n                soup.select_one('article') or\n                soup.select_one('[class*=\"content\"]') or\n                soup.select_one('div[class*=\"story\"]')\n            )\n            \n            if not content_elem:\n                # Fallback: try to find main content area\n                content_elem = soup.find('td', {'class': 'mainContent'})\n            \n            if not content_elem:\n                logger.warning(f\"  Could not find content container for: {url}\")\n                return None\n            \n            # Remove unwanted elements\n            for unwanted in content_elem.select('script, style, aside, .ads, .advertisement, .social-share, .related, nav, footer, .comments, .share'):\n                unwanted.decompose()\n            \n            # Extract text\n            content = content_elem.get_text(separator='\\n', strip=True)\n            \n            # Clean text\n            content = self._clean_text(content)\n            \n            return content\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Clean scraped text\"\"\"\n        if not text:\n            return \"\"\n        \n        # Remove extra whitespace\n        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n        text = '\\n\\n'.join(lines)\n        \n        # Remove non-breaking spaces\n        text = text.replace('\\xa0', ' ')\n        text = text.replace('\\u200b', '')\n        \n        # Remove very short lines (likely UI elements)\n        lines = text.split('\\n\\n')\n        lines = [line for line in lines if len(line) > 15]\n        text = '\\n\\n'.join(lines)\n        \n        # Remove multiple spaces\n        import re\n        text = re.sub(r' +', ' ', text)\n        \n        return text.strip()\n","path":null,"size_bytes":7838,"size_tokens":null},"backend/services/translation_service.py":{"content":"import os\nfrom anthropic import Anthropic\nfrom typing import Dict, Optional\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TranslationService:\n    def __init__(self):\n        self.claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n        self.client = Anthropic(api_key=self.claude_api_key) if self.claude_api_key else None\n        \n        from services.notification_service import notification_service\n        self.notification_service = notification_service\n    \n    def translate_article(self, article_data: Dict) -> Dict[str, str]:\n        \"\"\"\n        Translate article title and content separately\n        \n        Args:\n            article_data: Dict with 'title' and 'content' or 'summary'\n            \n        Returns:\n            Dict with 'title' and 'content' keys\n        \"\"\"\n        if not self.client:\n            logger.error(\"Claude API client not initialized\")\n            return {\"title\": \"\", \"content\": \"\"}\n        \n        title = article_data.get('title', '')\n        content = article_data.get('content') or article_data.get('summary', '')\n        \n        title_prompt = f\"\"\"–ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —è–∑—ã–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ:\n\n{title}\n\n–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤.\"\"\"\n\n        content_prompt = f\"\"\"–ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —è–∑—ã–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ:\n\n{content}\n\n–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n- –°–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ä–º–∏–Ω—ã –∏ –±—Ä–µ–Ω–¥—ã –∫–∞–∫ –µ—Å—Ç—å\n- –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —è–∑—ã–∫\n- –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∏–ª—å\n\n–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\"\"\"\n\n        try:\n            title_message = self.client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=200,\n                messages=[{\"role\": \"user\", \"content\": title_prompt}]\n            )\n            translated_title = title_message.content[0].text.strip()\n            \n            content_message = self.client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=4000,\n                messages=[{\"role\": \"user\", \"content\": content_prompt}]\n            )\n            translated_content = content_message.content[0].text.strip()\n            \n            logger.info(f\"Translated article: {title[:50]}...\")\n            \n            return {\n                \"title\": translated_title,\n                \"content\": translated_content\n            }\n            \n        except Exception as e:\n            logger.error(f\"Translation failed: {e}\")\n            return {\"title\": \"\", \"content\": \"\"}\n    \n    def translate_article_with_notification(self, article_data: Dict, article_id: int, image_url: str = None) -> tuple[Dict[str, str], bool]:\n        \"\"\"\n        Translate article and send Telegram notification with image\n        \n        Args:\n            article_data: Dict with article info\n            article_id: Database ID of the article\n            image_url: Optional image URL to include in notification\n            \n        Returns:\n            Tuple of (Dict with 'title' and 'content', notification_sent boolean)\n        \"\"\"\n        translation = self.translate_article(article_data)\n        notification_sent = False\n        \n        if translation and translation.get('title') and translation.get('content'):\n            notification_data = {\n                'id': article_id,\n                'source': 'The Spirits Business',\n                'title': translation['title'],\n                'translated_text': translation['content'],\n                'image_url': image_url,\n                'created_at': datetime.now().strftime('%Y-%m-%d %H:%M')\n            }\n            \n            try:\n                result = self.notification_service.send_approval_notification(notification_data)\n                if result:\n                    notification_sent = True\n                    logger.info(f\"Notification sent for article {article_id}\")\n                else:\n                    logger.warning(f\"Notification failed for article {article_id}\")\n            except Exception as e:\n                logger.error(f\"Failed to send notification: {e}\")\n        \n        return translation, notification_sent\n    \n    def translate_batch(self, articles: list) -> Dict[int, str]:\n        \"\"\"\n        Translate multiple articles\n        \n        Returns:\n            Dict mapping article IDs to translations\n        \"\"\"\n        translations = {}\n        \n        for article in articles:\n            article_id = article.get('id')\n            translation = self.translate_article(article)\n            \n            if translation:\n                translations[article_id] = translation\n        \n        return translations\n\ntranslation_service = TranslationService()\n","path":null,"size_bytes":4946,"size_tokens":null},"backend/services/facebook_token_manager.py":{"content":"import os\nimport requests\nimport logging\nfrom datetime import datetime\nfrom typing import Dict\n\nlogger = logging.getLogger(__name__)\n\nclass FacebookTokenManager:\n    def __init__(self):\n        self.app_id = os.getenv('FACEBOOK_APP_ID', '1033029015627449')\n        self.app_secret = os.getenv('FACEBOOK_APP_SECRET')\n        self.page_id = os.getenv('FACEBOOK_PAGE_ID')\n        self.current_token = os.getenv('FACEBOOK_PAGE_ACCESS_TOKEN')\n        self.graph_api_version = 'v18.0'\n    \n    def check_token_expiration(self) -> Dict:\n        \"\"\"\n        Check when current token expires\n        Returns dict with expiration info\n        \"\"\"\n        if not self.current_token or not self.app_secret:\n            return {\"error\": \"Token or app secret not configured\"}\n        \n        url = f\"https://graph.facebook.com/{self.graph_api_version}/debug_token\"\n        params = {\n            'input_token': self.current_token,\n            'access_token': f\"{self.app_id}|{self.app_secret}\"\n        }\n        \n        try:\n            response = requests.get(url, params=params, timeout=10)\n            result = response.json()\n            \n            if 'data' in result:\n                token_data = result['data']\n                expires_at = token_data.get('expires_at', 0)\n                data_access_expires_at = token_data.get('data_access_expires_at', 0)\n                \n                if expires_at == 0:\n                    status = \"never_expires\"\n                    days_remaining = None\n                else:\n                    expiry_date = datetime.fromtimestamp(expires_at)\n                    days_remaining = (expiry_date - datetime.now()).days\n                    status = \"expires_soon\" if days_remaining < 7 else \"healthy\"\n                \n                return {\n                    \"status\": status,\n                    \"is_valid\": token_data.get('is_valid', False),\n                    \"expires_at\": expires_at,\n                    \"data_access_expires_at\": data_access_expires_at,\n                    \"days_remaining\": days_remaining,\n                    \"needs_refresh\": days_remaining is not None and days_remaining < 7\n                }\n            else:\n                return {\"error\": \"Invalid token response\", \"details\": result}\n                \n        except Exception as e:\n            logger.error(f\"Failed to check token: {e}\")\n            return {\"error\": str(e)}\n    \n    def send_expiration_alert(self, days_remaining: int):\n        \"\"\"Send Telegram alert when token is expiring soon\"\"\"\n        message = f\"\"\"üö® <b>Facebook Token Alert</b>\n\n‚è∞ Your Facebook Page Access Token expires in <b>{days_remaining} days</b>!\n\nüìã Action needed:\n1. Go to: developers.facebook.com/tools/accesstoken\n2. Generate new Page Token for Gradus Media\n3. Update FACEBOOK_PAGE_ACCESS_TOKEN in Replit Secrets\n\nüîó Quick link: https://developers.facebook.com/tools/accesstoken\n\nWithout renewal, Facebook posting will stop working!\"\"\"\n        \n        try:\n            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n            chat_id = os.getenv('TELEGRAM_CHAT_ID')\n            \n            if bot_token and chat_id:\n                url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n                payload = {\n                    \"chat_id\": chat_id,\n                    \"text\": message,\n                    \"parse_mode\": \"HTML\"\n                }\n                requests.post(url, json=payload, timeout=10)\n                logger.info(f\"Token expiration alert sent ({days_remaining} days)\")\n        except Exception as e:\n            logger.error(f\"Failed to send expiration alert: {e}\")\n\nfacebook_token_manager = FacebookTokenManager()\n","path":null,"size_bytes":3666,"size_tokens":null},"backend/services/linkedin_poster.py":{"content":"import os\nimport requests\nimport logging\nfrom typing import Dict, Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass LinkedInPoster:\n    \"\"\"\n    LinkedIn Organization Page posting service\n    Requires OAuth 2.0 access token with w_organization_social scope\n    \"\"\"\n    \n    def __init__(self):\n        self.access_token = os.getenv('LINKEDIN_ACCESS_TOKEN')\n        self.organization_urn = os.getenv('LINKEDIN_ORGANIZATION_URN')\n        self.api_version = 'v2'\n        self.base_url = 'https://api.linkedin.com'\n        \n    def format_post_text(self, article_data: Dict) -> str:\n        \"\"\"\n        Format post text for LinkedIn with Ukrainian content\n        \n        Args:\n            article_data: Dict with title, text, source, url\n            \n        Returns:\n            Formatted post text\n        \"\"\"\n        title = article_data.get('title', '')\n        text = article_data.get('text', '')\n        source = article_data.get('source', 'The Spirits Business')\n        source_url = article_data.get('source_url', '')\n        \n        post_text = f\"\"\"{title}\n\n{text}\n\nüì∞ {source}\"\"\"\n        \n        if source_url:\n            post_text += f\"\\nüîó {source_url}\"\n        \n        return post_text\n    \n    def post_to_linkedin(self, article_data: Dict) -> Dict:\n        \"\"\"\n        Post content to LinkedIn organization page\n        \n        Args:\n            article_data: Dict with title, text, source_url, image_url, local_image_path (optional)\n            \n        Returns:\n            Dict with status, post_id, post_url\n        \"\"\"\n        if not self.access_token:\n            logger.error(\"LinkedIn access token not configured\")\n            return {\n                'status': 'error',\n                'message': 'LinkedIn access token missing',\n                'action_required': 'Set LINKEDIN_ACCESS_TOKEN in Replit Secrets'\n            }\n        \n        if not self.organization_urn:\n            logger.error(\"LinkedIn organization URN not configured\")\n            return {\n                'status': 'error',\n                'message': 'LinkedIn organization URN missing',\n                'action_required': 'Set LINKEDIN_ORGANIZATION_URN in Replit Secrets'\n            }\n        \n        post_text = self.format_post_text(article_data)\n        image_url = article_data.get('image_url')\n        local_image_path = article_data.get('local_image_path')\n        \n        if image_url or local_image_path:\n            return self._post_with_image(post_text, image_url, local_image_path)\n        else:\n            return self._post_text_only(post_text)\n    \n    def _post_text_only(self, text: str) -> Dict:\n        \"\"\"Post text-only content to LinkedIn\"\"\"\n        \n        url = f\"{self.base_url}/{self.api_version}/ugcPosts\"\n        \n        payload = {\n            \"author\": self.organization_urn,\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\n                        \"text\": text\n                    },\n                    \"shareMediaCategory\": \"NONE\"\n                }\n            },\n            \"visibility\": {\n                \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n            }\n        }\n        \n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\",\n            \"Content-Type\": \"application/json\",\n            \"X-Restli-Protocol-Version\": \"2.0.0\"\n        }\n        \n        try:\n            logger.info(\"Posting text to LinkedIn...\")\n            response = requests.post(url, json=payload, headers=headers, timeout=30)\n            \n            return self._parse_response(response)\n            \n        except Exception as e:\n            logger.error(f\"Failed to post to LinkedIn: {e}\")\n            return {\n                'status': 'error',\n                'message': str(e)\n            }\n    \n    def _register_upload(self) -> Optional[Dict]:\n        \"\"\"\n        Step 1: Register image upload with LinkedIn\n        Returns upload URL and asset URN\n        \"\"\"\n        url = f\"{self.base_url}/{self.api_version}/assets?action=registerUpload\"\n        \n        payload = {\n            \"registerUploadRequest\": {\n                \"recipes\": [\"urn:li:digitalmediaRecipe:feedshare-image\"],\n                \"owner\": self.organization_urn,\n                \"serviceRelationships\": [{\n                    \"relationshipType\": \"OWNER\",\n                    \"identifier\": \"urn:li:userGeneratedContent\"\n                }]\n            }\n        }\n        \n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\",\n            \"Content-Type\": \"application/json\",\n            \"X-Restli-Protocol-Version\": \"2.0.0\"\n        }\n        \n        try:\n            logger.info(\"Registering image upload with LinkedIn...\")\n            response = requests.post(url, json=payload, headers=headers, timeout=30)\n            \n            if response.status_code == 200:\n                result = response.json()\n                asset_urn = result.get('value', {}).get('asset')\n                upload_url = result.get('value', {}).get('uploadMechanism', {}).get(\n                    'com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest', {}\n                ).get('uploadUrl')\n                \n                if asset_urn and upload_url:\n                    logger.info(f\"Upload registered: {asset_urn}\")\n                    return {\n                        'asset_urn': asset_urn,\n                        'upload_url': upload_url\n                    }\n                else:\n                    logger.error(f\"Invalid registration response: {result}\")\n                    return None\n            else:\n                logger.error(f\"Registration failed: {response.status_code} - {response.text}\")\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Failed to register upload: {e}\")\n            return None\n    \n    def _upload_image(self, upload_url: str, image_url: Optional[str] = None, local_image_path: Optional[str] = None) -> bool:\n        \"\"\"\n        Step 2: Upload image to LinkedIn\n        Downloads image from URL or reads from local file, then uploads to LinkedIn's storage\n        \n        Args:\n            upload_url: Pre-signed LinkedIn upload URL\n            image_url: Remote image URL (tried first)\n            local_image_path: Local file path (fallback)\n        \n        Note: Pre-signed upload URL handles auth, don't add Authorization header\n        \"\"\"\n        image_data = None\n        content_type = 'image/jpeg'\n        \n        # Try remote URL first\n        if image_url:\n            try:\n                logger.info(f\"Downloading image from remote URL: {image_url}\")\n                img_response = requests.get(image_url, timeout=30)\n                \n                if img_response.status_code == 200:\n                    # Validate it's actually an image\n                    response_content_type = img_response.headers.get('Content-Type', '')\n                    if 'image' in response_content_type:\n                        image_data = img_response.content\n                        content_type = response_content_type\n                        logger.info(f\"Downloaded from remote URL (Content-Type: {content_type})\")\n                    else:\n                        logger.warning(f\"Remote URL returned non-image Content-Type: {response_content_type}\")\n                else:\n                    logger.warning(f\"Failed to download from remote URL: {img_response.status_code}\")\n            except Exception as e:\n                logger.warning(f\"Exception downloading from remote URL: {e}\")\n        \n        # Fallback to local file if remote failed\n        if not image_data and local_image_path:\n            try:\n                logger.info(f\"Falling back to local image: {local_image_path}\")\n                with open(local_image_path, 'rb') as f:\n                    image_data = f.read()\n                \n                # Determine content type from file extension\n                if local_image_path.lower().endswith('.png'):\n                    content_type = 'image/png'\n                elif local_image_path.lower().endswith('.jpg') or local_image_path.lower().endswith('.jpeg'):\n                    content_type = 'image/jpeg'\n                \n                logger.info(f\"Loaded local image (Content-Type: {content_type})\")\n            except Exception as e:\n                logger.error(f\"Failed to read local image: {e}\")\n                return False\n        \n        # If we still don't have image data, fail\n        if not image_data:\n            logger.error(\"No valid image data available from remote URL or local file\")\n            return False\n        \n        # Upload to LinkedIn\n        try:\n            logger.info(f\"Uploading image to LinkedIn (size: {len(image_data)} bytes)...\")\n            \n            # Pre-signed URL handles auth - no Authorization header needed\n            headers = {\n                \"Content-Type\": content_type\n            }\n            \n            upload_response = requests.put(\n                upload_url,\n                data=image_data,\n                headers=headers,\n                timeout=60\n            )\n            \n            if upload_response.status_code in [200, 201]:\n                logger.info(\"Image uploaded successfully to LinkedIn\")\n                return True\n            else:\n                logger.error(f\"LinkedIn upload failed: {upload_response.status_code} - {upload_response.text}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Exception during LinkedIn upload: {e}\")\n            return False\n    \n    def _post_with_image(self, text: str, image_url: Optional[str] = None, local_image_path: Optional[str] = None) -> Dict:\n        \"\"\"\n        Post content with image to LinkedIn using proper asset upload workflow\n        \n        Steps:\n        1. Register upload to get asset URN\n        2. Upload image binary (tries remote URL first, falls back to local file)\n        3. Create post with asset URN\n        4. If image upload fails, degrades to text-only post\n        \n        Args:\n            text: Post text content\n            image_url: Remote image URL (optional)\n            local_image_path: Local image file path (optional fallback)\n        \"\"\"\n        \n        # Step 1: Register upload\n        upload_info = self._register_upload()\n        if not upload_info:\n            logger.warning(\"Image upload registration failed, posting text-only\")\n            return self._post_text_only(text)\n        \n        # Step 2: Upload image (with fallback)\n        upload_success = self._upload_image(upload_info['upload_url'], image_url, local_image_path)\n        if not upload_success:\n            logger.warning(\"Image upload failed, degrading to text-only post\")\n            return self._post_text_only(text)\n        \n        # Step 3: Create post with asset URN\n        url = f\"{self.base_url}/{self.api_version}/ugcPosts\"\n        \n        payload = {\n            \"author\": self.organization_urn,\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\n                        \"text\": text\n                    },\n                    \"shareMediaCategory\": \"IMAGE\",\n                    \"media\": [{\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": \"Article image\"\n                        },\n                        \"media\": upload_info['asset_urn'],\n                        \"title\": {\n                            \"text\": \"Image\"\n                        }\n                    }]\n                }\n            },\n            \"visibility\": {\n                \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n            }\n        }\n        \n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\",\n            \"Content-Type\": \"application/json\",\n            \"X-Restli-Protocol-Version\": \"2.0.0\"\n        }\n        \n        try:\n            logger.info(\"Creating LinkedIn post with uploaded image...\")\n            response = requests.post(url, json=payload, headers=headers, timeout=30)\n            \n            return self._parse_response(response)\n            \n        except Exception as e:\n            logger.error(f\"Failed to post to LinkedIn: {e}\")\n            return {\n                'status': 'error',\n                'message': str(e)\n            }\n    \n    def _parse_response(self, response) -> Dict:\n        \"\"\"Parse LinkedIn API response\"\"\"\n        \n        logger.info(f\"LinkedIn API response: {response.status_code}\")\n        \n        if response.status_code == 201:\n            result = response.json()\n            post_id = result.get('id', '')\n            \n            post_url = f\"https://www.linkedin.com/feed/update/{post_id}\"\n            \n            logger.info(f\"Posted to LinkedIn successfully: {post_id}\")\n            \n            return {\n                'status': 'success',\n                'post_id': post_id,\n                'post_url': post_url,\n                'posted_at': datetime.now().isoformat()\n            }\n        else:\n            error_msg = response.text\n            logger.error(f\"LinkedIn API error: {error_msg}\")\n            \n            if response.status_code == 401:\n                return {\n                    'status': 'error',\n                    'message': 'LinkedIn access token invalid or expired',\n                    'action_required': 'Refresh LINKEDIN_ACCESS_TOKEN'\n                }\n            elif response.status_code == 403:\n                return {\n                    'status': 'error',\n                    'message': 'Permission denied - check organization access and scopes',\n                    'action_required': 'Verify app has w_organization_social scope and you are org admin'\n                }\n            else:\n                return {\n                    'status': 'error',\n                    'message': f'LinkedIn API error ({response.status_code}): {error_msg}'\n                }\n    \n    def verify_token(self) -> bool:\n        \"\"\"\n        Verify that the access token is valid and has proper permissions\n        \"\"\"\n        if not self.access_token:\n            return False\n        \n        url = f\"{self.base_url}/v2/userinfo\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\"\n        }\n        \n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            \n            if response.status_code == 200:\n                logger.info(\"LinkedIn token is valid\")\n                return True\n            else:\n                logger.error(f\"LinkedIn token invalid: {response.status_code}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Token verification failed: {e}\")\n            return False\n\nlinkedin_poster = LinkedInPoster()\n","path":null,"size_bytes":15080,"size_tokens":null},"backend/services/telegram_webhook.py":{"content":"import logging\nimport os\nfrom typing import Dict\nfrom sqlalchemy.orm import Session\nfrom models.content import ContentQueue, ApprovalLog\nfrom datetime import datetime\nfrom services.facebook_poster import facebook_poster\nfrom services.notification_service import notification_service\nimport requests\n\nlogger = logging.getLogger(__name__)\n\nclass TelegramWebhookHandler:\n    def __init__(self):\n        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n        self.base_url = f\"https://api.telegram.org/bot{self.bot_token}\"\n    \n    def handle_callback_query(self, callback_query: Dict, db: Session) -> Dict:\n        \"\"\"\n        Handle Telegram inline button callbacks\n        \n        Args:\n            callback_query: Telegram callback query data\n            db: Database session\n            \n        Returns:\n            Dict with status and message\n        \"\"\"\n        callback_id = callback_query.get('id')\n        callback_data = callback_query.get('data')\n        message = callback_query.get('message')\n        \n        if not callback_data:\n            return {\"status\": \"error\", \"message\": \"No callback data\"}\n        \n        try:\n            if callback_data.startswith('approve_'):\n                content_id_str = callback_data.split('_')[1]\n                if not content_id_str.isdigit():\n                    logger.error(f\"Invalid content_id in approve callback: {content_id_str}\")\n                    self._answer_callback_query(callback_id, \"‚ùå Invalid content ID\")\n                    return {\"status\": \"error\", \"message\": \"Invalid content ID\"}\n                \n                content_id = int(content_id_str)\n                return self._approve_content(content_id, callback_id, message, db)\n                \n            elif callback_data.startswith('reject_'):\n                content_id_str = callback_data.split('_')[1]\n                if not content_id_str.isdigit():\n                    logger.error(f\"Invalid content_id in reject callback: {content_id_str}\")\n                    self._answer_callback_query(callback_id, \"‚ùå Invalid content ID\")\n                    return {\"status\": \"error\", \"message\": \"Invalid content ID\"}\n                \n                content_id = int(content_id_str)\n                return self._reject_content(content_id, callback_id, message, db)\n            \n            self._answer_callback_query(callback_id, \"‚ùå Unknown action\")\n            return {\"status\": \"error\", \"message\": \"Unknown callback data\"}\n            \n        except (ValueError, IndexError) as e:\n            logger.error(f\"Error parsing callback data '{callback_data}': {e}\")\n            self._answer_callback_query(callback_id, \"‚ùå Invalid request\")\n            return {\"status\": \"error\", \"message\": \"Invalid callback format\"}\n    \n    def _approve_content(self, content_id: int, callback_id: str, message: Dict, db: Session) -> Dict:\n        \"\"\"\n        Approve content for scheduled posting (NO IMMEDIATE POST)\n        Marks as 'approved' - scheduler will post at optimal times\n        \"\"\"\n        \n        try:\n            article = db.query(ContentQueue).filter(ContentQueue.id == content_id).first()\n            \n            if not article:\n                self._answer_callback_query(callback_id, \"‚ùå Article not found\")\n                return {\"status\": \"error\", \"message\": \"Article not found\"}\n            \n            if article.status != 'pending_approval':\n                self._answer_callback_query(callback_id, f\"‚ö†Ô∏è Already {article.status}\")\n                return {\"status\": \"error\", \"message\": f\"Article already {article.status}\"}\n            \n            article.status = 'approved'\n            article.reviewed_at = datetime.utcnow()\n            article.reviewed_by = 'telegram_bot'\n            \n            if not article.extra_metadata:\n                article.extra_metadata = {}\n            article.extra_metadata['approved_at'] = datetime.utcnow().isoformat()\n            article.extra_metadata['approved_by'] = 'telegram'\n            \n            log_entry = ApprovalLog(\n                content_id=content_id,\n                action=\"approved\",\n                moderator=\"telegram_bot\",\n                details={\n                    \"method\": \"telegram_inline_button\",\n                    \"note\": \"Approved for scheduled posting\"\n                }\n            )\n            db.add(log_entry)\n            db.commit()\n            db.refresh(article)\n            \n            logger.info(f\"Content {content_id} approved via Telegram - scheduled for posting\")\n            \n            title = article.translated_title or (article.extra_metadata.get('title', '') if article.extra_metadata else 'No title')\n            \n            posting_schedule = \"\"\"üìÖ <b>–†–æ–∑–∫–ª–∞–¥ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó:</b>\n‚Ä¢ Facebook: –©–æ–¥–Ω—è –æ 18:00\n‚Ä¢ LinkedIn: –ü–Ω/–°—Ä/–ü—Ç –æ 9:00\n\nüí° –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ–ø—É–±–ª—ñ–∫—É—î –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π —á–∞—Å –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó –≤–∑–∞—î–º–æ–¥—ñ—ó.\"\"\"\n            \n            new_caption = f\"\"\"‚úÖ <b>–ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ö–≤–∞–ª–µ–Ω–æ!</b>\n\nüì∞ <b>{title}</b>\n\n‚úÖ –°—Ç–∞—Ç—É—Å: –ì–æ—Ç–æ–≤–æ –¥–æ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó\nüÜî ID: {content_id}\n\n{posting_schedule}\"\"\"\n            \n            caption_updated = self._update_message_caption(message, new_caption)\n            if caption_updated:\n                self._answer_callback_query(callback_id, \"‚úÖ –°—Ö–≤–∞–ª–µ–Ω–æ! –ë—É–¥–µ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ –∑–∞ —Ä–æ–∑–∫–ª–∞–¥–æ–º\")\n            else:\n                self._answer_callback_query(callback_id, \"‚úÖ –°—Ö–≤–∞–ª–µ–Ω–æ –¥–ª—è –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó\")\n                logger.warning(f\"Content {content_id}: Approved but Telegram caption update failed\")\n            \n            return {\"status\": \"success\", \"message\": \"Content approved for scheduled posting\", \"content_id\": content_id}\n                \n        except Exception as e:\n            logger.error(f\"Error approving content {content_id}: {e}\")\n            db.rollback()\n            self._answer_callback_query(callback_id, f\"‚ùå Error: {str(e)[:100]}\")\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def _reject_content(self, content_id: int, callback_id: str, message: Dict, db: Session) -> Dict:\n        \"\"\"Reject content with proper transaction handling\"\"\"\n        \n        try:\n            article = db.query(ContentQueue).filter(ContentQueue.id == content_id).first()\n            \n            if not article:\n                self._answer_callback_query(callback_id, \"‚ùå Article not found\")\n                return {\"status\": \"error\", \"message\": \"Article not found\"}\n            \n            if article.status != 'pending_approval':\n                self._answer_callback_query(callback_id, f\"‚ö†Ô∏è Already {article.status}\")\n                return {\"status\": \"error\", \"message\": f\"Article already {article.status}\"}\n            \n            article.status = 'rejected'\n            article.reviewed_at = datetime.utcnow()\n            article.reviewed_by = 'telegram_bot'\n            article.rejection_reason = 'Rejected via Telegram'\n            \n            log_entry = ApprovalLog(\n                content_id=content_id,\n                action=\"rejected\",\n                moderator=\"telegram_bot\",\n                details={\"reason\": \"Rejected via Telegram inline button\"}\n            )\n            db.add(log_entry)\n            db.commit()\n            db.refresh(article)\n            \n            logger.info(f\"Content {content_id} rejected via Telegram\")\n            \n            title = article.translated_title or (article.extra_metadata.get('title', 'No title') if article.extra_metadata else 'No title')\n            new_caption = f\"\"\"‚ùå <b>–í—ñ–¥—Ö–∏–ª–µ–Ω–æ</b>\n\nüì∞ <b>{title}</b>\n\nüóëÔ∏è –ö–æ–Ω—Ç–µ–Ω—Ç –≤—ñ–¥—Ö–∏–ª–µ–Ω–æ —á–µ—Ä–µ–∑ Telegram\n‚è∞ {datetime.utcnow().strftime('%H:%M, %d %b %Y')}\"\"\"\n            \n            caption_updated = self._update_message_caption(message, new_caption)\n            if caption_updated:\n                self._answer_callback_query(callback_id, \"‚ùå Rejected\")\n            else:\n                self._answer_callback_query(callback_id, \"‚ùå Rejected (Notification update failed)\")\n                logger.warning(f\"Content {content_id}: Rejected successfully but Telegram caption update failed\")\n            \n            return {\"status\": \"success\", \"message\": \"Content rejected\", \"content_id\": content_id}\n            \n        except Exception as e:\n            logger.error(f\"Error rejecting content {content_id}: {e}\")\n            db.rollback()\n            self._answer_callback_query(callback_id, f\"‚ùå Error: {str(e)[:100]}\")\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def _update_message_caption(self, message: Dict, new_caption: str) -> bool:\n        \"\"\"\n        Update Telegram message caption or text\n        \n        Automatically uses editMessageCaption for photo messages (with caption)\n        or editMessageText for text-only messages\n        \n        Returns:\n            True if message updated successfully, False otherwise\n        \"\"\"\n        try:\n            chat_id = message['chat']['id']\n            message_id = message['message_id']\n            \n            has_photo = 'photo' in message\n            \n            if has_photo:\n                url = f\"{self.base_url}/editMessageCaption\"\n                payload = {\n                    \"chat_id\": chat_id,\n                    \"message_id\": message_id,\n                    \"caption\": new_caption,\n                    \"parse_mode\": \"HTML\"\n                }\n            else:\n                url = f\"{self.base_url}/editMessageText\"\n                payload = {\n                    \"chat_id\": chat_id,\n                    \"message_id\": message_id,\n                    \"text\": new_caption,\n                    \"parse_mode\": \"HTML\"\n                }\n            \n            response = requests.post(url, json=payload, timeout=10)\n            result = response.json()\n            \n            if result.get('ok'):\n                return True\n            else:\n                logger.error(f\"Failed to update message: {result.get('description', 'Unknown error')}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Exception updating message: {e}\")\n            return False\n    \n    def _answer_callback_query(self, callback_id: str, text: str):\n        \"\"\"Answer callback query to remove loading state\"\"\"\n        url = f\"{self.base_url}/answerCallbackQuery\"\n        payload = {\n            \"callback_query_id\": callback_id,\n            \"text\": text,\n            \"show_alert\": False\n        }\n        \n        try:\n            requests.post(url, json=payload, timeout=10)\n        except Exception as e:\n            logger.error(f\"Failed to answer callback query: {e}\")\n\ntelegram_webhook_handler = TelegramWebhookHandler()\n","path":null,"size_bytes":10845,"size_tokens":null},"backend/models/content.py":{"content":"from sqlalchemy import Column, Integer, String, Text, TIMESTAMP, CheckConstraint, ARRAY, JSON, Boolean\nfrom sqlalchemy.sql import func\nfrom .import Base\n\nclass ContentQueue(Base):\n    __tablename__ = \"content_queue\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    status = Column(String(20), nullable=False)\n    source = Column(String(255))\n    source_url = Column(String(500))\n    source_title = Column(Text)\n    original_text = Column(Text)\n    translated_title = Column(Text)\n    translated_text = Column(Text)\n    image_url = Column(Text)\n    image_prompt = Column(Text)\n    local_image_path = Column(Text)\n    scheduled_post_time = Column(TIMESTAMP)\n    platforms = Column(ARRAY(String))\n    created_at = Column(TIMESTAMP, server_default=func.now())\n    reviewed_at = Column(TIMESTAMP)\n    reviewed_by = Column(String(100))\n    rejection_reason = Column(Text)\n    edit_history = Column(JSON)\n    extra_metadata = Column(JSON)\n    analytics = Column(JSON)\n    posted_at = Column(TIMESTAMP)\n    \n    # Language support fields\n    language = Column(String(10), default='en')\n    needs_translation = Column(Boolean, default=True)\n    \n    __table_args__ = (\n        CheckConstraint(\n            \"status IN ('draft', 'pending_approval', 'approved', 'rejected', 'posted')\",\n            name='valid_status'\n        ),\n    )\n\nclass ApprovalLog(Base):\n    __tablename__ = \"approval_log\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    content_id = Column(Integer, nullable=False)\n    action = Column(String(50))\n    moderator = Column(String(100))\n    timestamp = Column(TIMESTAMP, server_default=func.now())\n    details = Column(JSON)\n","path":null,"size_bytes":1664,"size_tokens":null},"backend/migrations/__init__.py":{"content":"# Migrations package\n","path":null,"size_bytes":21,"size_tokens":null},"backend/services/scrapers/spirits_business.py":{"content":"\"\"\"\nThe Spirits Business scraper - English source, needs translation to Ukrainian\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport trafilatura\nimport logging\nimport re\nfrom typing import List, Optional\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass SpiritsBusinessScraper(ScraperBase):\n    \"\"\"Scraper for The Spirits Business (English source)\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"The Spirits Business\"\n    \n    def get_language(self) -> str:\n        return \"en\"\n    \n    def get_needs_translation(self) -> bool:\n        return True\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape latest articles from The Spirits Business\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name}...\")\n            url = \"https://www.thespiritsbusiness.com\"\n            \n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            article_links = soup.find_all('a', href=re.compile(r'/\\d{4}/\\d{2}/'))\n            \n            seen_urls = set()\n            \n            for link in article_links:\n                if len(articles) >= limit:\n                    break\n                \n                article_url = link.get('href')\n                if not article_url or article_url in seen_urls:\n                    continue\n                \n                if not article_url.startswith('http'):\n                    article_url = url + article_url\n                \n                seen_urls.add(article_url)\n                \n                # Get title from link\n                title_elem = link.find('strong') or link.find(['h1', 'h2', 'h3'])\n                if not title_elem:\n                    title = link.get_text(strip=True)\n                    if not title or len(title) < 10:\n                        continue\n                else:\n                    title = title_elem.get_text(strip=True)\n                \n                # Clean title\n                title = self._clean_title(title)\n                \n                logger.info(f\"  üìÑ Found: {title[:50]}...\")\n                \n                # Fetch article content with title for duplicate removal\n                content_data = self._fetch_article_content(article_url, title)\n                \n                if content_data and content_data['content']:\n                    article = ArticlePayload(\n                        source_name=self.source_name,\n                        language=self.language,\n                        needs_translation=self.needs_translation,\n                        url=article_url,\n                        title=title,\n                        content=content_data['content'],\n                        author=content_data.get('author'),\n                        published_at=content_data.get('published_date')\n                    )\n                    articles.append(article)\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _fetch_article_content(self, url: str, title: str = \"\") -> Optional[dict]:\n        \"\"\"Fetch and extract article content\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            # Use trafilatura for content extraction\n            content = trafilatura.extract(\n                response.content,\n                include_comments=False,\n                include_tables=False\n            )\n            \n            if not content:\n                return None\n            \n            # Extract author from content\n            author = self._extract_author(content)\n            \n            # Clean content with title for duplicate removal\n            content = self._clean_content(content, title)\n            \n            # Extract date from URL (format: /YYYY/MM/)\n            date_match = re.search(r'/(\\d{4})/(\\d{2})/', url)\n            published_date = None\n            if date_match:\n                year, month = date_match.groups()\n                published_date = f\"{year}-{month}-01\"\n            \n            return {\n                'content': content,\n                'author': author,\n                'published_date': published_date\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_title(self, title: str) -> str:\n        \"\"\"Remove source name from title\"\"\"\n        if not title:\n            return \"\"\n        \n        patterns = [\n            f' - {self.source_name}',\n            f' | {self.source_name}',\n            f' ‚Äì {self.source_name}',\n            f' ‚Äî {self.source_name}',\n        ]\n        \n        for pattern in patterns:\n            if title.endswith(pattern):\n                title = title[:-len(pattern)].strip()\n                break\n        \n        return title\n    \n    def _clean_content(self, content: str, title: str) -> str:\n        \"\"\"Clean article content - remove duplicate titles and metadata\"\"\"\n        if not content:\n            return \"\"\n        \n        # Remove ALL occurrences of title from content\n        # Title is added separately during posting\n        if title:\n            title_escaped = re.escape(title)\n            \n            # Remove duplicate title pattern (title appearing twice)\n            double_title_pattern = f'^{title_escaped}\\\\s*\\\\n\\\\s*{title_escaped}'\n            if re.match(double_title_pattern, content, re.MULTILINE | re.IGNORECASE):\n                content = re.sub(f'^{title_escaped}\\\\s*\\\\n', '', content, count=1, flags=re.MULTILINE | re.IGNORECASE)\n            \n            # Remove title from beginning (any remaining)\n            content = re.sub(f'^{title_escaped}\\\\s*', '', content, flags=re.IGNORECASE)\n            \n            # Also remove if title appears at start after newlines\n            content = re.sub(f'^\\\\s*{title_escaped}\\\\s*', '', content, flags=re.IGNORECASE)\n        \n        # Remove byline\n        byline_pattern = r'(?:^|\\n)By\\s+[A-Z][a-zA-Z\\'\\-\\.]*(?:\\s+[A-Z][a-zA-Z\\'\\-\\.]*)*(?=\\n|$|[A-Z][a-z]|[A-Z]{2,})'\n        content = re.sub(byline_pattern, '\\n', content, flags=re.MULTILINE)\n        \n        # Remove related content section\n        content = re.split(r'Related news|Related articles|Related content', content, flags=re.IGNORECASE)[0]\n        \n        # Get lines and filter\n        lines = [l.strip() for l in content.split('\\n') if l.strip()]\n        filtered = []\n        for line in lines:\n            # Skip exact title match\n            if title and line.lower() == title.lower():\n                continue\n            # Skip date patterns\n            if re.match(r'^\\d{1,2}\\s+\\w+,?\\s+\\d{4}$', line):\n                continue\n            # Skip very short metadata lines\n            if len(line) < 15 and not line.endswith('.'):\n                continue\n            filtered.append(line)\n        \n        # Join text\n        text = ' '.join(filtered)\n        text = re.sub(r'  +', ' ', text)\n        \n        # Add paragraph breaks at sentence boundaries\n        text = re.sub(r'([.!?¬ª\"]\\s+)([A-Z–ê-–Ø–Ü–á–Ñ])', r'\\1\\n\\n\\2', text)\n        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n        \n        return text.strip()\n    \n    def _extract_author(self, content: str) -> str:\n        \"\"\"Extract author name from content\"\"\"\n        byline_pattern = r'(?:^|\\n)By\\s+([A-Z][a-zA-Z\\'\\-\\.]*(?:\\s+[A-Z][a-zA-Z\\'\\-\\.]*)*?)(?=\\n|$|[A-Z][a-z]|[A-Z]{2,})'\n        match = re.search(byline_pattern, content, flags=re.MULTILINE)\n        if match:\n            return match.group(1).strip()\n        return \"\"\n","path":null,"size_bytes":8079,"size_tokens":null},"backend/services/scrapers/manager.py":{"content":"\"\"\"\nScraper Manager - coordinates multiple news sources\nHandles deduplication and content ingestion\n\"\"\"\n\nimport logging\nfrom typing import List, Dict, Set\nfrom .base import ArticlePayload\nfrom .spirits_business import SpiritsBusinessScraper\nfrom .delo_ua import DeloUaScraper\nfrom .drinks_international import DrinksInternationalScraper\nfrom .just_drinks import JustDrinksScraper\nfrom .restorator_ua import RestoratorUaScraper\n\nlogger = logging.getLogger(__name__)\n\nclass ScraperManager:\n    \"\"\"Manages multiple news scrapers and coordinates content ingestion\"\"\"\n    \n    def __init__(self):\n        self.scrapers = {\n            'spirits_business': SpiritsBusinessScraper(),\n            'delo_ua': DeloUaScraper(),\n            'drinks_international': DrinksInternationalScraper(),\n            'just_drinks': JustDrinksScraper(),\n            'restorator_ua': RestoratorUaScraper()\n        }\n        \n        logger.info(f\"‚úÖ ScraperManager initialized with {len(self.scrapers)} sources\")\n    \n    def scrape_all_sources(self, limit_per_source: int = 5) -> Dict[str, List[ArticlePayload]]:\n        \"\"\"\n        Scrape all enabled sources\n        \n        Args:\n            limit_per_source: Maximum articles per source\n            \n        Returns:\n            Dict mapping source name to list of articles\n        \"\"\"\n        results = {}\n        total_articles = 0\n        \n        logger.info(f\"üîÑ Starting multi-source scraping...\")\n        \n        for source_name, scraper in self.scrapers.items():\n            if not scraper.is_enabled():\n                logger.info(f\"‚è≠Ô∏è  Skipping disabled source: {source_name}\")\n                continue\n            \n            try:\n                articles = scraper.scrape_articles(limit=limit_per_source)\n                results[source_name] = articles\n                total_articles += len(articles)\n                \n                logger.info(f\"  ‚úÖ {source_name}: {len(articles)} articles\")\n                \n            except Exception as e:\n                logger.error(f\"  ‚ùå {source_name} failed: {e}\")\n                results[source_name] = []\n        \n        logger.info(f\"‚úÖ Multi-source scraping complete: {total_articles} total articles from {len(results)} sources\")\n        \n        return results\n    \n    def scrape_source(self, source_name: str, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"\n        Scrape a specific source\n        \n        Args:\n            source_name: Name of the source to scrape\n            limit: Maximum articles to scrape\n            \n        Returns:\n            List of articles\n        \"\"\"\n        scraper = self.scrapers.get(source_name)\n        \n        if not scraper:\n            logger.error(f\"Unknown source: {source_name}\")\n            return []\n        \n        if not scraper.is_enabled():\n            logger.warning(f\"Source disabled: {source_name}\")\n            return []\n        \n        try:\n            articles = scraper.scrape_articles(limit=limit)\n            logger.info(f\"‚úÖ {source_name}: scraped {len(articles)} articles\")\n            return articles\n        except Exception as e:\n            logger.error(f\"‚ùå {source_name} scraping failed: {e}\")\n            return []\n    \n    def check_duplicate(self, article: ArticlePayload, existing_urls: Set[str], existing_hashes: Set[str]) -> bool:\n        \"\"\"\n        Check if article is a duplicate\n        \n        Args:\n            article: Article to check\n            existing_urls: Set of existing source URLs\n            existing_hashes: Set of existing content hashes\n            \n        Returns:\n            True if duplicate, False if new\n        \"\"\"\n        # Check URL\n        if article.url in existing_urls:\n            return True\n        \n        # Check content hash\n        content_hash = article.get_content_hash()\n        if content_hash in existing_hashes:\n            return True\n        \n        return False\n    \n    def get_enabled_sources(self) -> List[str]:\n        \"\"\"Get list of enabled source names\"\"\"\n        return [name for name, scraper in self.scrapers.items() if scraper.is_enabled()]\n    \n    def get_all_sources(self) -> List[str]:\n        \"\"\"Get list of all source names\"\"\"\n        return list(self.scrapers.keys())\n\n# Singleton instance\nscraper_manager = ScraperManager()\n","path":null,"size_bytes":4288,"size_tokens":null},"SCRAPER_DIAGNOSTIC_REPORT.md":{"content":"# Facebook Scraping Diagnostic Report\n**Date**: November 24, 2025  \n**Issue**: Zero articles scraped from Facebook sources\n\n---\n\n## üîç Root Causes Identified\n\n### 1. Restorator.ua ‚Üí HoReCa-–£–∫—Ä–∞—ó–Ω–∞\n**Status**: ‚ùå WEBSITE DOWN\n\n**Original scraper:**\n- URL: `https://restorator.ua/post/`\n- Error: **404 Not Found** - Site is completely non-functional\n- Wix error: \"Domain not connected to website\"\n\n**Attempted replacement:**\n- New source: HoReCa-–£–∫—Ä–∞—ó–Ω–∞  \n- URL: `https://horeca-ukraine.com/category/news/`\n- Error: **404 Not Found** - Category URL doesn't exist\n\n**Finding**: Both the original site and replacement have URL structure issues. The main HoReCa site works (200 OK), but proper article listing requires either:\n- Correct category URL discovery\n- JavaScript rendering (site may be React/Vue-based)\n\n---\n\n### 2. The Drinks Report ‚Üí The Drinks Business\n**Status**: ‚ö†Ô∏è  REQUIRES JAVASCRIPT RENDERING\n\n**Original scraper:**\n- URL: `https://www.thedrinksreport.com/news/`\n- Issue: Domain redirects to parent company (paragraph.co.uk)\n- Site appears inactive for new content (last articles: March 2023)\n\n**Attempted replacement:**\n- New source: The Drinks Business\n- URL: `https://www.thedrinksbusiness.com/category/news/`\n- Status: **200 OK** but 0 articles found with standard CSS selectors\n- Finding: Site uses JavaScript rendering - HTML contains minimal elements until JS executes\n\n**CSS Selector Analysis:**\n- `article`: 0 elements\n- `.post`: 0 elements  \n- `h2 a`: Only 2 elements (likely navigation, not articles)\n- Conclusion: Content loaded dynamically via JavaScript\n\n---\n\n### 3. Just Drinks\n**Status**: ‚úÖ **WORKING PERFECTLY**\n\n- Successfully scraping 2 articles per test\n- Reliable English-language source\n- No changes needed\n\n**Example articles scraped:**\n1. \"Brockmans Gin MD steps down, founder back at helm\"\n2. \"WarRoom Cellars buys Simi brand from The Wine Group\"\n\n---\n\n## üìä Current Scraping Status\n\n| Source | Status | Articles/Day | Language |\n|--------|--------|--------------|----------|\n| **Just Drinks** | ‚úÖ Working | 3 | English |\n| **Restorator.ua/HoReCa** | ‚ùå Failed | 0 | Ukrainian |\n| **The Drinks Report/Business** | ‚ùå Failed | 0 | English |\n| **The Spirits Business** | ‚úÖ Working | 3 | English |\n| **Delo.ua** | ‚úÖ Working | 3 | Ukrainian |\n| **MinFin.ua** | ‚úÖ Working | 3 | Ukrainian |\n\n**Total Working**: 4/6 sources  \n**Daily Articles**: ~12 articles (down from planned 18)\n\n---\n\n## ‚úÖ Recommended Solutions\n\n### Option 1: Simplify & Focus (Recommended)\n**Remove broken scrapers, keep 4 working sources**\n\n**Facebook sources (Daily 2am):**\n- Just Drinks ‚úÖ (English, 3 articles)\n\n**LinkedIn sources (Mon/Wed/Fri 1am):**\n- The Spirits Business ‚úÖ (English, 3 articles)\n- Delo.ua ‚úÖ (Ukrainian, 3 articles)  \n- MinFin.ua ‚úÖ (Ukrainian, 3 articles)\n\n**Benefits:**\n- All sources 100% reliable\n- 3 articles/day for Facebook (sufficient for daily posting)\n- 9 articles/week for LinkedIn (3 posts per week)\n- Balanced English/Ukrainian content\n- No failed scraping runs\n\n---\n\n### Option 2: Add JavaScript Rendering\n**Use Selenium or Playwright to scrape JS-heavy sites**\n\n**Requirements:**\n- Install `selenium` or `playwright`\n- Add browser automation to scrapers\n- Increases complexity and resource usage\n\n**Trade-offs:**\n- More articles but slower scraping\n- Higher memory usage\n- More maintenance required\n\n---\n\n### Option 3: Find Alternative Sources\n**Replace with static HTML sites**\n\n**Ukrainian HoReCa alternatives:**\n- Komersant UA (komersant.ua) - Business news\n- UNN.ua - General news with restaurant coverage\n- QB Tools blog - HoReCa tech & events\n\n**English drinks alternatives:**\n- Spirits Business (already using ‚úÖ)\n- Just Drinks (already using ‚úÖ)\n- Consider specialized RSS feeds\n\n---\n\n## üéØ My Recommendation\n\n**Implement Option 1: Simplify & Focus**\n\n**Reasons:**\n1. **Reliability**: 4/4 sources working vs 4/6 with failures\n2. **Quality over quantity**: 12 quality articles > 18 with errors\n3. **Lower maintenance**: No debugging broken scrapers\n4. **Sufficient content**: 3 FB posts/day + 9 LI posts/week meets needs\n5. **Clean logs**: No error spam from failed scraping\n\n**Implementation:**\n- Update scheduler to use only working scrapers\n- Remove/disable broken scraper files\n- Update documentation\n- Monitor for 1 week to confirm stability\n\n---\n\n## üìù Implementation Steps\n\nIf you approve Option 1, I will:\n\n1. ‚úÖ Update `backend/services/scheduler.py`:\n   - Facebook sources: `['just_drinks']`\n   - LinkedIn sources: `['spirits_business', 'delo_ua', 'minfin_ua']`\n\n2. ‚úÖ Update source names in comments/docs\n\n3. ‚úÖ Restart backend to apply changes\n\n4. ‚úÖ Test scheduled scraping  \n\n5. ‚úÖ Update replit.md documentation\n\n---\n\n## Questions?\n\n- Want to try Option 2 (JavaScript rendering)?\n- Want to try different sources?\n- Happy with Option 1 (4 working sources)?\n\nLet me know and I'll proceed!\n","path":null,"size_bytes":4907,"size_tokens":null},"backend/services/scrapers/minfin_ua.py":{"content":"\"\"\"\nMinFin.ua scraper - Ukrainian source, NO translation needed\nScrapes alcohol market news from minfin.com.ua\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Optional\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass MinFinUaScraper(ScraperBase):\n    \"\"\"Scraper for MinFin.ua alcohol market section (Ukrainian source)\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"MinFin.ua\"\n    \n    def get_language(self) -> str:\n        return \"uk\"\n    \n    def get_needs_translation(self) -> bool:\n        return False\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape articles from MinFin.ua alcohol market section\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name} (Ukrainian)...\")\n            section_url = \"https://minfin.com.ua/ua/company/alkogol/\"\n            \n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(section_url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article cards\n            article_elements = (\n                soup.select('.mfn-table-item') or\n                soup.select('.news-item') or\n                soup.select('.article-item') or\n                soup.select('.list-item') or\n                soup.select('tr[class*=\"item\"]') or\n                soup.find_all('div', class_='item')\n            )\n            \n            logger.info(f\"  Found {len(article_elements)} potential articles\")\n            \n            for element in article_elements[:limit * 2]:\n                if len(articles) >= limit:\n                    break\n                \n                try:\n                    article_data = self._parse_article_card(element)\n                    \n                    if not article_data:\n                        continue\n                    \n                    # Fetch full article content\n                    content = self._fetch_article_content(article_data['url'])\n                    \n                    if content and len(content) > 100:\n                        article = ArticlePayload(\n                            source_name=self.source_name,\n                            language=self.language,\n                            needs_translation=self.needs_translation,\n                            url=article_data['url'],\n                            title=article_data['title'],\n                            content=content,\n                            published_at=article_data.get('published_date'),\n                            image_url=article_data.get('image_url')\n                        )\n                        articles.append(article)\n                        logger.info(f\"  ‚úÖ {article_data['title'][:50]}...\")\n                        \n                except Exception as e:\n                    logger.error(f\"  Error parsing article element: {e}\")\n                    continue\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} Ukrainian articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _parse_article_card(self, element) -> Optional[dict]:\n        \"\"\"Parse article card from listing page\"\"\"\n        try:\n            # Find title - MinFin uses various structures\n            title_elem = (\n                element.select_one('h2') or\n                element.select_one('h3') or\n                element.select_one('.title') or\n                element.select_one('.mfn-table-header') or\n                element.select_one('[class*=\"title\"]') or\n                element.select_one('a')\n            )\n            \n            if not title_elem:\n                return None\n            \n            title = title_elem.get_text(strip=True)\n            \n            if len(title) < 10:\n                return None\n            \n            # Find link\n            link_elem = title_elem.find_parent('a') or element.select_one('a')\n            \n            if not link_elem:\n                return None\n            \n            url = link_elem.get('href')\n            if not url:\n                return None\n            \n            # Make URL absolute\n            if not url.startswith('http'):\n                base_url = \"https://minfin.com.ua\"\n                url = base_url + url if url.startswith('/') else f\"{base_url}/{url}\"\n            \n            # Find image (optional)\n            img_elem = element.select_one('img')\n            image_url = None\n            if img_elem:\n                image_url = img_elem.get('src') or img_elem.get('data-src')\n                if image_url and not image_url.startswith('http'):\n                    image_url = f\"https:{image_url}\" if image_url.startswith('//') else f\"https://minfin.com.ua{image_url}\"\n            \n            # Find date (optional)\n            date_elem = element.select_one('.date') or element.select_one('time') or element.select_one('.mfn-table-date')\n            published_date = None\n            if date_elem:\n                published_date = date_elem.get_text(strip=True)\n            \n            return {\n                'title': title,\n                'url': url,\n                'image_url': image_url,\n                'published_date': published_date\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article card: {e}\")\n            return None\n    \n    def _fetch_article_content(self, url: str) -> Optional[str]:\n        \"\"\"Fetch full article content from article page\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article content\n            content_elem = (\n                soup.select_one('.article-content') or\n                soup.select_one('.mfn-article-content') or\n                soup.select_one('.content') or\n                soup.select_one('article') or\n                soup.select_one('main') or\n                soup.select_one('.main-content') or\n                soup.select_one('[class*=\"content\"]')\n            )\n            \n            if not content_elem:\n                logger.warning(f\"  Could not find content container for: {url}\")\n                return None\n            \n            # Remove unwanted elements\n            for unwanted in content_elem.select('script, style, aside, .ads, .advertisement, nav, footer, .related, .comments, .share, .social'):\n                unwanted.decompose()\n            \n            # Extract text\n            content = content_elem.get_text(separator='\\n', strip=True)\n            \n            # Clean text\n            content = self._clean_text(content)\n            \n            return content\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Clean scraped Ukrainian text\"\"\"\n        if not text:\n            return \"\"\n        \n        # Remove extra whitespace\n        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n        text = '\\n\\n'.join(lines)\n        \n        # Remove non-breaking spaces\n        text = text.replace('\\xa0', ' ')\n        text = text.replace('\\u200b', '')\n        \n        # Remove very short lines (likely navigation/UI elements)\n        lines = text.split('\\n\\n')\n        lines = [line for line in lines if len(line) > 20]\n        text = '\\n\\n'.join(lines)\n        \n        # Remove multiple spaces\n        import re\n        text = re.sub(r' +', ' ', text)\n        \n        return text.strip()\n","path":null,"size_bytes":8064,"size_tokens":null},"frontend/src/pages/HomePage.jsx":{"content":"import { useState, useEffect } from 'react'\nimport { BarChart, MessageSquare, FileText, Users, TrendingUp } from 'lucide-react'\nimport { Link } from 'react-router-dom'\nimport api from '../lib/api'\n\nfunction HomePage() {\n  const [stats, setStats] = useState({\n    pending: 0,\n    approved: 0,\n    posted: 0,\n    rejected: 0\n  })\n  const [loading, setLoading] = useState(true)\n\n  useEffect(() => {\n    fetchStats()\n    const interval = setInterval(fetchStats, 30000)\n    return () => clearInterval(interval)\n  }, [])\n\n  const fetchStats = async () => {\n    try {\n      const response = await api.get('/content/stats')\n      setStats(response.data)\n      setLoading(false)\n    } catch (error) {\n      console.error('Failed to fetch stats:', error)\n      setLoading(false)\n    }\n  }\n\n  return (\n    <div>\n      <div className=\"mb-8\">\n        <h1 className=\"text-3xl font-bold text-gray-900 mb-2\">Welcome to Gradus Media AI Agent</h1>\n        <p className=\"text-gray-600\">Automated content creation and approval system for social media</p>\n      </div>\n\n      <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6 mb-8\">\n        <div className=\"bg-white p-6 rounded-lg shadow\">\n          <div className=\"flex items-center justify-between mb-4\">\n            <h3 className=\"text-lg font-semibold\">Pending Approval</h3>\n            <FileText className=\"text-purple-600\" size={24} />\n          </div>\n          <p className=\"text-3xl font-bold text-purple-600\">\n            {loading ? '...' : stats.pending}\n          </p>\n          <p className=\"text-sm text-gray-500 mt-2\">Content awaiting review</p>\n        </div>\n\n        <div className=\"bg-white p-6 rounded-lg shadow\">\n          <div className=\"flex items-center justify-between mb-4\">\n            <h3 className=\"text-lg font-semibold\">Approved</h3>\n            <TrendingUp className=\"text-green-600\" size={24} />\n          </div>\n          <p className=\"text-3xl font-bold text-green-600\">\n            {loading ? '...' : stats.approved}\n          </p>\n          <p className=\"text-sm text-gray-500 mt-2\">Ready to post</p>\n        </div>\n\n        <div className=\"bg-white p-6 rounded-lg shadow\">\n          <div className=\"flex items-center justify-between mb-4\">\n            <h3 className=\"text-lg font-semibold\">Posted</h3>\n            <Users className=\"text-blue-600\" size={24} />\n          </div>\n          <p className=\"text-3xl font-bold text-blue-600\">\n            {loading ? '...' : stats.posted}\n          </p>\n          <p className=\"text-sm text-gray-500 mt-2\">Published content</p>\n        </div>\n      </div>\n\n      <div className=\"bg-white rounded-lg shadow p-6\">\n        <h2 className=\"text-xl font-semibold mb-4\">Quick Actions</h2>\n        <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n          <Link \n            to=\"/chat\"\n            className=\"flex items-center space-x-3 p-4 border border-gray-200 rounded-lg hover:bg-purple-50 hover:border-purple-300 transition\"\n          >\n            <MessageSquare className=\"text-purple-600\" size={24} />\n            <div>\n              <h3 className=\"font-semibold\">Test Claude Chat</h3>\n              <p className=\"text-sm text-gray-500\">Chat with Claude AI and test translation</p>\n            </div>\n          </Link>\n\n          <Link \n            to=\"/content\"\n            className=\"flex items-center space-x-3 p-4 border border-gray-200 rounded-lg hover:bg-purple-50 hover:border-purple-300 transition\"\n          >\n            <FileText className=\"text-purple-600\" size={24} />\n            <div>\n              <h3 className=\"font-semibold\">Review Content</h3>\n              <p className=\"text-sm text-gray-500\">Approve or reject pending content</p>\n            </div>\n          </Link>\n        </div>\n      </div>\n\n      <div className=\"mt-8 bg-blue-50 border border-blue-200 rounded-lg p-6\">\n        <h3 className=\"text-lg font-semibold text-blue-900 mb-2\">System Status</h3>\n        <ul className=\"space-y-2 text-sm text-blue-800\">\n          <li>‚úÖ FastAPI Backend: Running</li>\n          <li>‚úÖ Claude API: Configured</li>\n          <li>‚úÖ PostgreSQL Database: Connected</li>\n          <li>‚è≥ DALL-E Image Generation: Awaiting API key</li>\n          <li>‚è≥ Social Media Posting: Awaiting credentials</li>\n        </ul>\n      </div>\n    </div>\n  )\n}\n\nexport default HomePage\n","path":null,"size_bytes":4322,"size_tokens":null},"frontend/src/main.jsx":{"content":"import React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport App from './App'\nimport './index.css'\n\nReactDOM.createRoot(document.getElementById('root')).render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>\n)\n","path":null,"size_bytes":230,"size_tokens":null},"backend/models/__init__.py":{"content":"from sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nimport os\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\n\nBase = declarative_base()\n\nengine = None\nSessionLocal = None\n\ndef init_db():\n    global engine, SessionLocal\n    if engine is None:\n        if not DATABASE_URL:\n            raise ValueError(\"DATABASE_URL environment variable is not set\")\n        engine = create_engine(\n            DATABASE_URL,\n            pool_pre_ping=True,\n            pool_recycle=300,\n            pool_size=10,\n            max_overflow=20\n        )\n        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n        Base.metadata.create_all(bind=engine)\n\ndef get_db():\n    if SessionLocal is None:\n        init_db()\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n","path":null,"size_bytes":882,"size_tokens":null},"backend/docs/UKRAINIAN_CONTENT_GUIDE.md":{"content":"# Ukrainian Content Support Guide\n\n## Overview\n\nThe Gradus Media AI Agent now supports **bilingual content processing**:\n- **English sources**: Automatically translated to Ukrainian\n- **Ukrainian sources**: Skip translation, processed directly\n\n## How It Works\n\n### Database Schema\n\nNew fields in `ContentQueue` table:\n- `language` (VARCHAR(10), default='en'): Source language code ('en', 'uk', etc.)\n- `needs_translation` (BOOLEAN, default=TRUE): Whether article needs translation\n- `posted_at` (TIMESTAMP): When content was posted to social media\n\n### Content Workflows\n\n#### English Content (Automatic)\n1. ‚úÖ Scraper creates article with defaults (`language='en'`, `needs_translation=True`)\n2. ‚úÖ Translation task translates to Ukrainian\n3. ‚úÖ Image generation creates social media image\n4. ‚úÖ Telegram notification sent for approval\n5. ‚úÖ Scheduled posting to Facebook/LinkedIn\n\n#### Ukrainian Content (Manual)\n1. ‚úÖ API creates article with (`language='uk'`, `needs_translation=False`)\n2. ‚è≠Ô∏è  Translation task **SKIPS** (no translation needed)\n3. ‚úÖ Image generation creates social media image\n4. ‚úÖ Ukrainian text ‚Üí marked as \"translated\" (for consistency)\n5. ‚úÖ Status changes to 'pending_approval'\n6. ‚úÖ Telegram notification sent for approval\n7. ‚úÖ Scheduled posting to Facebook/LinkedIn\n\n## Adding Ukrainian Content\n\n### Method 1: API Endpoint (Recommended)\n\nUse the `/api/content/create` endpoint:\n\n```bash\ncurl -X POST http://localhost:8000/api/content/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"title\": \"–£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—Ç—ñ\",\n    \"content\": \"–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó —Å—Ç–∞—Ç—Ç—ñ...\",\n    \"source\": \"Ukrainian News Source\",\n    \"source_url\": \"https://example.com/article\",\n    \"language\": \"uk\",\n    \"needs_translation\": false,\n    \"platforms\": [\"facebook\", \"linkedin\"]\n  }'\n```\n\n**Response:**\n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Ukrainian content created successfully\",\n  \"content_id\": 123,\n  \"language\": \"uk\",\n  \"needs_translation\": false,\n  \"next_steps\": \"Content will be processed for images and sent for approval\"\n}\n```\n\n### Method 2: Add Ukrainian News Scraper (Future)\n\nTo automatically scrape Ukrainian sources:\n\n1. **Create new scraper** in `backend/services/ukrainian_scraper.py`\n2. **Set language flags** when creating ContentQueue:\n   ```python\n   new_article = ContentQueue(\n       status='draft',\n       source='Ukrainian Source Name',\n       source_url=article_url,\n       original_text=article_content,\n       language='uk',              # ‚Üê IMPORTANT\n       needs_translation=False,     # ‚Üê IMPORTANT\n       platforms=['facebook', 'linkedin'],\n       extra_metadata={'title': article_title}\n   )\n   ```\n3. **Add to scheduler** for automated scraping\n\n## API Reference\n\n### POST /api/content/create\n\nCreate Ukrainian content manually.\n\n**Request Body:**\n```typescript\n{\n  title: string;              // Ukrainian title\n  content: string;            // Ukrainian content\n  source?: string;            // Source name (default: \"Manual\")\n  source_url?: string;        // Source URL (optional)\n  language?: string;          // Language code (default: \"uk\")\n  needs_translation?: boolean; // Translation needed? (default: false)\n  platforms?: string[];       // Platforms (default: [\"facebook\", \"linkedin\"])\n}\n```\n\n**Example:**\n```javascript\nconst response = await fetch('http://localhost:8000/api/content/create', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    title: \"–ù–æ–≤—ñ —Ç—Ä–µ–Ω–¥–∏ –≤ –∞–ª–∫–æ–≥–æ–ª—å–Ω—ñ–π —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó\",\n    content: \"–î–µ—Ç–∞–ª—å–Ω–∏–π –æ–ø–∏—Å —Ç—Ä–µ–Ω–¥—ñ–≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é...\",\n    source: \"Ukrainian Industry News\",\n    language: \"uk\",\n    needs_translation: false\n  })\n});\n```\n\n## Scheduler Behavior\n\n### Translation Task (Every hour at :15)\n**Filter:** `status='draft' AND needs_translation=True AND translated_text IS NULL`\n\n- ‚úÖ English articles: **Processed**\n- ‚è≠Ô∏è Ukrainian articles: **Skipped** (needs_translation=False)\n\n### Image Generation Task (Every hour at :30)\n**Filter:** \n```sql\nWHERE (status='pending_approval')  -- Translated English articles\n   OR (status='draft' AND needs_translation=False)  -- Ukrainian articles\nAND image_url IS NULL\n```\n\n- ‚úÖ English articles: **Processed** after translation\n- ‚úÖ Ukrainian articles: **Processed** directly, then:\n  - `status` ‚Üí 'pending_approval'\n  - `translated_title` ‚Üê `extra_metadata.title`\n  - `translated_text` ‚Üê `original_text`\n\n## Testing\n\n### Test Ukrainian Content Flow\n\n1. **Create Ukrainian article:**\n   ```bash\n   curl -X POST http://localhost:8000/api/content/create \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"title\": \"–¢–µ—Å—Ç–æ–≤–∞ —Å—Ç–∞—Ç—Ç—è\",\n       \"content\": \"–¶–µ —Ç–µ—Å—Ç–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é\",\n       \"language\": \"uk\",\n       \"needs_translation\": false\n     }'\n   ```\n\n2. **Wait for image generation** (runs every hour at :30)\n   - Or manually trigger: `POST /api/images/generate/{article_id}`\n\n3. **Check Telegram** for approval notification\n   - Should show Ukrainian text without translation\n\n4. **Approve** via Telegram button\n   - Content marked for scheduled posting\n\n5. **Verify posting** at scheduled times:\n   - Facebook: Daily at 18:00\n   - LinkedIn: Mon/Wed/Fri at 09:00\n\n## Migration\n\nThe database migration ran successfully:\n```\n‚úÖ Migration completed successfully!\n   - Added 'language' column (default: 'en')\n   - Added 'needs_translation' column (default: TRUE)\n   - Added 'posted_at' column\n```\n\n**All existing articles** have:\n- `language='en'`\n- `needs_translation=True`\n- Will continue to be translated as before\n\n## Troubleshooting\n\n### Ukrainian content being translated\n**Problem:** Ukrainian article went through translation\n**Solution:** Verify API request set `needs_translation=false`\n\n### Ukrainian content stuck in 'draft'\n**Problem:** Image generation not picking it up\n**Solution:** Check that `language='uk'` and `needs_translation=False`\n\n### No Telegram notification\n**Problem:** Notification not sent after image generation\n**Solution:** Check logs for image generation errors\n\n## Future Enhancements\n\n1. **Automated Ukrainian scraper**\n   - Add Ukrainian news source\n   - Auto-detect language and set flags\n\n2. **Language detection**\n   - Auto-detect article language\n   - Set `needs_translation` automatically\n\n3. **Multi-language support**\n   - Support more languages (Russian, Polish, etc.)\n   - Language-specific posting schedules\n\n## Example Use Cases\n\n### Use Case 1: Manual Ukrainian News\n```bash\n# Add Ukrainian article from external source\nPOST /api/content/create\n{\n  \"title\": \"–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –≥–æ—Ä—ñ–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–ª–∞ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω—É –Ω–∞–≥–æ—Ä–æ–¥—É\",\n  \"content\": \"–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –≥–æ—Ä—ñ–ª–∫–∞ \\\"–•–ª—ñ–±–Ω–∏–π –î–∞—Ä\\\" –æ—Ç—Ä–∏–º–∞–ª–∞ –∑–æ–ª–æ—Ç—É –º–µ–¥–∞–ª—å...\",\n  \"source\": \"Ukrainian Industry Awards\",\n  \"source_url\": \"https://example.com/award\",\n  \"language\": \"uk\",\n  \"needs_translation\": false\n}\n```\n\n### Use Case 2: Mixed Language Workflow\n```bash\n# English article (automatic translation)\nScraper creates ‚Üí Translation ‚Üí Image ‚Üí Approval ‚Üí Post\n\n# Ukrainian article (skip translation)\nAPI creates ‚Üí Image ‚Üí Approval ‚Üí Post\n```\n\n## Database Queries\n\n### Check Ukrainian articles\n```sql\nSELECT id, status, language, needs_translation, \n       extra_metadata->>'title' as title\nFROM content_queue\nWHERE language = 'uk';\n```\n\n### Check translation queue\n```sql\nSELECT id, language, needs_translation, status\nFROM content_queue\nWHERE needs_translation = TRUE AND status = 'draft';\n```\n\n### Check ready for approval\n```sql\nSELECT id, language, status, image_url IS NOT NULL as has_image\nFROM content_queue\nWHERE status = 'pending_approval';\n```\n\n## Summary\n\n‚úÖ **Implemented:**\n- Database schema for language support\n- Translation task filters Ukrainian content\n- Image generation handles Ukrainian articles\n- API endpoint for manual Ukrainian content\n- Ukrainian articles flow to approval without translation\n\n‚è≥ **Future Work:**\n- Automated Ukrainian news scraper\n- Language auto-detection\n- More language support\n\nüìù **Documentation:**\n- API endpoint usage\n- Workflow diagrams\n- Testing procedures\n- Troubleshooting guide\n","path":null,"size_bytes":8319,"size_tokens":null},"backend/services/image_generator.py":{"content":"import os\nimport requests\nimport hashlib\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom typing import Dict, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ImageGenerator:\n    def __init__(self):\n        openai_key = os.getenv(\"OPENAI_API_KEY\")\n        anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        \n        if not openai_key:\n            logger.warning(\"OPENAI_API_KEY not set - image generation will not work\")\n            self.openai_client = None\n        else:\n            self.openai_client = OpenAI(api_key=openai_key)\n        \n        if not anthropic_key:\n            logger.warning(\"ANTHROPIC_API_KEY not set - prompt generation will not work\")\n            self.claude_client = None\n        else:\n            self.claude_client = Anthropic(api_key=anthropic_key)\n        \n        # Set up permanent image storage directory\n        self.image_storage_dir = Path(\"attached_assets/generated_images\")\n        self.image_storage_dir.mkdir(parents=True, exist_ok=True)\n    \n    def generate_image_prompt(self, article_data: Dict) -> str:\n        \"\"\"\n        Use Claude to generate a professional DALL-E prompt based on article content\n        \"\"\"\n        if not self.claude_client:\n            logger.error(\"Claude client not initialized\")\n            return \"\"\n        \n        title = article_data.get('title', '')\n        content = article_data.get('content', '')[:1000]\n        \n        prompt = f\"\"\"Based on this alcohol industry article, create a professional DALL-E image prompt for a social media post.\n\nArticle Title: {title}\nArticle Content: {content}\n\nRequirements for the image prompt:\n- Professional, premium alcohol industry aesthetic\n- Suitable for Facebook/LinkedIn business posts\n- Modern, clean, minimalist design\n- Relevant to the article topic\n- Brand-safe (no specific brand logos unless mentioned in article)\n- Color scheme: premium (blues, golds, silvers, or warm earth tones)\n- Style: photorealistic or professional infographic\n\nCRITICAL: NO TEXT OR WORDS should appear in the image\n- Do not include any letters, numbers, labels, or captions\n- Use purely visual elements (charts, bottles, ingredients, settings)\n- Let the imagery speak for itself without text overlays\n\nCreate a detailed DALL-E prompt (2-3 sentences) that will generate an appropriate text-free image.\nReturn ONLY the prompt text, nothing else.\"\"\"\n\n        try:\n            message = self.claude_client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=300,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }]\n            )\n            \n            dalle_prompt = message.content[0].text.strip()\n            \n            # Add explicit \"no text\" instruction to DALL-E prompt\n            dalle_prompt += \" No text, labels, or words in the image.\"\n            \n            logger.info(f\"Generated DALL-E prompt: {dalle_prompt[:100]}...\")\n            return dalle_prompt\n            \n        except Exception as e:\n            logger.error(f\"Failed to generate image prompt: {e}\")\n            return \"\"\n    \n    def download_and_save_image(self, image_url: str) -> Optional[str]:\n        \"\"\"\n        Download image from DALL-E temporary URL and save permanently to filesystem.\n        Returns the ABSOLUTE local file path.\n        \"\"\"\n        try:\n            # Download the image\n            response = requests.get(image_url, timeout=30)\n            response.raise_for_status()\n            \n            # Generate unique filename using hash of URL\n            url_hash = hashlib.md5(image_url.encode()).hexdigest()[:12]\n            filename = f\"dalle_{url_hash}.png\"\n            filepath = self.image_storage_dir / filename\n            \n            # Save to filesystem\n            with open(filepath, 'wb') as f:\n                f.write(response.content)\n            \n            # Return ABSOLUTE path for reliable access\n            absolute_path = str(filepath.absolute())\n            logger.info(f\"Image saved permanently to: {absolute_path}\")\n            return absolute_path\n            \n        except Exception as e:\n            logger.error(f\"Failed to download and save image: {e}\")\n            return None\n    \n    def generate_image(self, prompt: str) -> Optional[Dict[str, str]]:\n        \"\"\"\n        Generate an image using DALL-E based on the provided prompt.\n        Returns dict with both temporary URL and permanent local path.\n        \"\"\"\n        if not self.openai_client:\n            logger.error(\"OpenAI client not initialized - missing API key\")\n            return None\n        \n        if not prompt:\n            logger.error(\"Empty prompt provided\")\n            return None\n        \n        try:\n            response = self.openai_client.images.generate(\n                model=\"dall-e-3\",\n                prompt=prompt,\n                size=\"1024x1024\",\n                quality=\"standard\",\n                n=1,\n            )\n            \n            if response.data and len(response.data) > 0 and response.data[0].url:\n                image_url = response.data[0].url\n                logger.info(f\"Image generated successfully: {image_url}\")\n                \n                # Download and save immediately to prevent expiration issues\n                local_path = self.download_and_save_image(image_url)\n                \n                return {\n                    'url': image_url,  # Temporary DALL-E URL (expires in 1-2 hours)\n                    'local_path': local_path  # Permanent local file path\n                }\n            else:\n                logger.error(\"No image URL in response\")\n                return None\n            \n        except Exception as e:\n            logger.error(f\"DALL-E image generation error: {str(e)}\")\n            return None\n    \n    def generate_article_image(self, article_data: Dict) -> Dict[str, str]:\n        \"\"\"\n        Complete pipeline: generate prompt + generate image\n        \n        Returns:\n            Dict with 'prompt', 'image_url', and 'local_path'\n        \"\"\"\n        dalle_prompt = self.generate_image_prompt(article_data)\n        \n        if not dalle_prompt:\n            return {\"prompt\": \"\", \"image_url\": \"\", \"local_path\": \"\"}\n        \n        image_result = self.generate_image(dalle_prompt)\n        \n        if not image_result:\n            return {\"prompt\": dalle_prompt, \"image_url\": \"\", \"local_path\": \"\"}\n        \n        return {\n            \"prompt\": dalle_prompt,\n            \"image_url\": image_result.get('url', ''),\n            \"local_path\": image_result.get('local_path', '')\n        }\n\nimage_generator = ImageGenerator()\n","path":null,"size_bytes":6697,"size_tokens":null},"backend/services/scrapers/restorator_ua.py":{"content":"\"\"\"\nRestorator.ua scraper - Ukrainian source, NO translation needed\nScrapes HoReCa (Hotel/Restaurant/Catering) industry news from restorator.ua\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Optional\nfrom .base import ScraperBase, ArticlePayload\n\nlogger = logging.getLogger(__name__)\n\nclass RestoratorUaScraper(ScraperBase):\n    \"\"\"Scraper for HoReCa-–£–∫—Ä–∞—ó–Ω–∞ (horeca-ukraine.com) - Ukrainian HoReCa news\"\"\"\n    \n    def get_source_name(self) -> str:\n        return \"HoReCa-–£–∫—Ä–∞—ó–Ω–∞\"\n    \n    def get_language(self) -> str:\n        return \"uk\"\n    \n    def get_needs_translation(self) -> bool:\n        return False\n    \n    def scrape_articles(self, limit: int = 5) -> List[ArticlePayload]:\n        \"\"\"Scrape articles from HoReCa-–£–∫—Ä–∞—ó–Ω–∞ HoReCa news section\"\"\"\n        articles = []\n        \n        try:\n            logger.info(f\"üîç Scraping {self.source_name} (Ukrainian)...\")\n            posts_url = \"https://horeca-ukraine.com/category/horeca-news/\"\n            \n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(posts_url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article cards\n            article_elements = (\n                soup.select('.post-item') or\n                soup.select('article') or\n                soup.select('.news-item') or\n                soup.select('.card') or\n                soup.select('.item') or\n                soup.select('div[class*=\"post\"]')\n            )\n            \n            logger.info(f\"  Found {len(article_elements)} potential articles\")\n            \n            for element in article_elements[:limit * 2]:\n                if len(articles) >= limit:\n                    break\n                \n                try:\n                    article_data = self._parse_article_card(element)\n                    \n                    if not article_data:\n                        continue\n                    \n                    # Fetch full article content (pass title for duplicate removal)\n                    content = self._fetch_article_content(article_data['url'], article_data['title'])\n                    \n                    if content and len(content) > 100:\n                        article = ArticlePayload(\n                            source_name=self.source_name,\n                            language=self.language,\n                            needs_translation=self.needs_translation,\n                            url=article_data['url'],\n                            title=article_data['title'],\n                            content=content,\n                            published_at=article_data.get('published_date'),\n                            image_url=article_data.get('image_url')\n                        )\n                        articles.append(article)\n                        logger.info(f\"  ‚úÖ {article_data['title'][:50]}...\")\n                        \n                except Exception as e:\n                    logger.error(f\"  Error parsing article element: {e}\")\n                    continue\n            \n            logger.info(f\"‚úÖ Scraped {len(articles)} Ukrainian articles from {self.source_name}\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"‚ùå {self.source_name} scraping failed: {e}\")\n            return []\n    \n    def _parse_article_card(self, element) -> Optional[dict]:\n        \"\"\"Parse article card from listing page\"\"\"\n        try:\n            # Find title\n            title_elem = (\n                element.select_one('h2') or\n                element.select_one('h3') or\n                element.select_one('.title') or\n                element.select_one('.post-title') or\n                element.select_one('.card-title') or\n                element.select_one('[class*=\"title\"]')\n            )\n            \n            if not title_elem:\n                return None\n            \n            title = title_elem.get_text(strip=True)\n            \n            if len(title) < 10:\n                return None\n            \n            # Find link\n            link_elem = title_elem.find_parent('a') or element.select_one('a')\n            \n            if not link_elem:\n                return None\n            \n            url = link_elem.get('href')\n            if not url:\n                return None\n            \n            # Make URL absolute\n            if not url.startswith('http'):\n                base_url = \"https://restorator.ua\"\n                url = base_url + url if url.startswith('/') else f\"{base_url}/{url}\"\n            \n            # Find image (optional)\n            img_elem = element.select_one('img')\n            image_url = None\n            if img_elem:\n                image_url = img_elem.get('src') or img_elem.get('data-src')\n                if image_url and not image_url.startswith('http'):\n                    image_url = f\"https:{image_url}\" if image_url.startswith('//') else f\"https://restorator.ua{image_url}\"\n            \n            # Find date (optional)\n            date_elem = element.select_one('.date') or element.select_one('time') or element.select_one('.published')\n            published_date = None\n            if date_elem:\n                published_date = date_elem.get_text(strip=True)\n            \n            return {\n                'title': title,\n                'url': url,\n                'image_url': image_url,\n                'published_date': published_date\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article card: {e}\")\n            return None\n    \n    def _fetch_article_content(self, url: str, title: str = \"\") -> Optional[str]:\n        \"\"\"Fetch full article content from article page\"\"\"\n        try:\n            headers = {'User-Agent': self.user_agent}\n            response = requests.get(url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Try multiple selectors for article content\n            content_elem = (\n                soup.select_one('.post-content') or\n                soup.select_one('article') or\n                soup.select_one('.content') or\n                soup.select_one('.entry-content') or\n                soup.select_one('.article-body') or\n                soup.select_one('[class*=\"content\"]')\n            )\n            \n            if not content_elem:\n                logger.warning(f\"  Could not find content container for: {url}\")\n                return None\n            \n            # Remove unwanted elements (ads, scripts, social, etc.)\n            for unwanted in content_elem.select('script, style, aside, .ads, .advertisement, nav, footer, .related, .comments, .share'):\n                unwanted.decompose()\n            \n            # Remove metadata elements BEFORE extracting text\n            metadata_selectors = [\n                '.article-meta', '.meta', '.metadata', '.post-meta', '.entry-meta',\n                '.category', '.post-category', '.article-category',\n                '.breadcrumb', '.breadcrumbs',\n                '.tags', '.post-tags', '.article-tags',\n                '.related', '.related-posts', '.related-articles',\n                '.footer-text', '.disclaimer', '.copyright',\n                '.author', '.post-author', '.date', '.post-date'\n            ]\n            for selector in metadata_selectors:\n                for element in content_elem.select(selector):\n                    element.decompose()\n            \n            # Extract text\n            content = content_elem.get_text(separator='\\n', strip=True)\n            \n            # Clean content (remove metadata patterns, duplicate title, fix formatting)\n            content = self._clean_content(content, title)\n            \n            return content\n            \n        except Exception as e:\n            logger.error(f\"Error fetching content from {url}: {e}\")\n            return None\n    \n    def _clean_content(self, content: str, title: str) -> str:\n        \"\"\"Clean HoReCa article content with improved line break handling\"\"\"\n        import re\n        \n        if not content:\n            return \"\"\n        \n        # Remove duplicate title if present at start\n        if title and len(title) > 15:\n            if content.startswith(title):\n                content = content[len(title):].strip()\n            else:\n                title_pos = content[:500].find(title)\n                if title_pos >= 0:\n                    content = content[:title_pos] + content[title_pos + len(title):]\n                    content = content.strip()\n        \n        # Remove HoReCa-specific metadata and footer patterns\n        patterns_to_remove = [\n            r'Pro-HoReCa\\s*/?\\s*–°—Ç–∞—Ç—Ç—ñ.*?\\n',\n            r'Pro-HoReCa.*?\\n',\n            r'^–°—Ç–∞—Ç—Ç—ñ\\s*\\n',\n            r'–¶—ñ–∫–∞–≤–µ –∑–∞ —Ü–µ–π —Ç–∏–∂–¥–µ–Ω—å:.*',\n            r'–£—Å—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ñ —Ñ–æ—Ç–æ.*',\n            r'–£—Å—ñ –ø—Ä–∞–≤–∞ –∑–∞—Ö–∏—â–µ–Ω—ñ.*',\n            r'–ù–æ–≤–∏–Ω–∏ –†–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤.*?\\n',\n            r'–ê–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ –æ–≥–ª—è–¥–∏.*?\\n',\n            r'–¢–µ–Ω–¥–µ–Ω—Ü—ñ—ó —Ä–∏–Ω–∫—É.*?\\n',\n            r'–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa.*?\\n',\n            r'–ü–æ–¥–∏–≤–∏—Ç–∏—Å—è –≤—Å—ñ.*?\\n',\n            r'–ü–æ–¥—ñ–ª–∏—Ç–∏—Å—è.*?\\n',\n            r'Share.*?\\n',\n        ]\n        \n        for pattern in patterns_to_remove:\n            content = re.sub(pattern, '', content, flags=re.MULTILINE | re.IGNORECASE)\n        \n        # Words/phrases to skip completely\n        skip_phrases = [\n            'Pro-HoReCa', '–°—Ç–∞—Ç—Ç—ñ', '–ö–∞—Ç–µ–≥–æ—Ä—ñ—è', '–¶—ñ–∫–∞–≤–µ –∑–∞ —Ü–µ–π —Ç–∏–∂–¥–µ–Ω—å',\n            '–£—Å—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ñ —Ñ–æ—Ç–æ', '–ê–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ –æ–≥–ª—è–¥–∏', '–¢–µ–Ω–¥–µ–Ω—Ü—ñ—ó —Ä–∏–Ω–∫—É',\n            '–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa', '–ü–æ–¥–∏–≤–∏—Ç–∏—Å—è –≤—Å—ñ', '–ü–æ–¥—ñ–ª–∏—Ç–∏—Å—è', 'Share',\n            'Facebook', 'Twitter', 'Telegram', 'Viber', '–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ', '–†–µ–∫–ª–∞–º–∞',\n            '–ù–æ–≤–∏–Ω–∏ –†–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤'\n        ]\n        \n        # First pass: filter out metadata lines, join everything else\n        lines = content.split('\\n')\n        valid_lines = []\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            \n            # Skip standalone metadata words\n            if line in skip_phrases:\n                continue\n            \n            # Skip short lines containing metadata\n            skip_line = False\n            for phrase in skip_phrases:\n                if phrase in line and len(line) < 60:\n                    skip_line = True\n                    break\n            if skip_line:\n                continue\n            \n            # Skip very short non-sentence lines (likely UI elements)\n            if len(line) < 8 and not line.endswith(('.', '!', '?', '¬ª', '\"')):\n                continue\n            \n            valid_lines.append(line)\n        \n        # Join ALL valid lines into one continuous text block\n        content = ' '.join(valid_lines)\n        \n        # Clean up spacing\n        content = content.replace('\\xa0', ' ')\n        content = content.replace('\\u200b', '')\n        content = re.sub(r'  +', ' ', content)\n        \n        # Now split at proper sentence boundaries only\n        # Pattern: sentence-ending punctuation + space + capital letter (Latin or Cyrillic)\n        # This creates paragraph breaks at natural sentence boundaries\n        content = re.sub(\n            r'([.!?¬ª\"]\\s+)([A-Z–ê-–Ø–Ü–á–Ñ“ê¬´\"])',\n            r'\\1\\n\\n\\2',\n            content\n        )\n        \n        # Group into larger paragraphs (every 2-3 sentences)\n        # Split by current double newlines\n        paragraphs = content.split('\\n\\n')\n        grouped_paragraphs = []\n        current_group = []\n        \n        for para in paragraphs:\n            para = para.strip()\n            if not para:\n                continue\n            \n            current_group.append(para)\n            \n            # Create paragraph break after 2-3 sentences or if paragraph is long enough\n            total_len = sum(len(p) for p in current_group)\n            if len(current_group) >= 2 or total_len > 300:\n                grouped_paragraphs.append(' '.join(current_group))\n                current_group = []\n        \n        # Add remaining\n        if current_group:\n            grouped_paragraphs.append(' '.join(current_group))\n        \n        # Join with double newlines\n        content = '\\n\\n'.join(grouped_paragraphs)\n        \n        # Final cleanup\n        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n        content = re.sub(r'  +', ' ', content)\n        \n        # Remove footer block with related articles (appears at end after main content)\n        # Pattern: multiple article titles followed by category names\n        footer_patterns = [\n            r'–ù–æ–≤–∏–π –≥–∞—Å—Ç—Ä–æ–ø—Ä–æ—î–∫—Ç.*?–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–ì–∞—Å—Ç—Ä–æ–Ω–æ–º—ñ—á–Ω–∏–π Netflix.*?–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–ë–æ–π–∫—ñ–≤—Å—å–∫–∞ –∫—É—Ö–Ω—è.*?–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –∫—É—Ö–Ω—è.*?–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–í—ñ–¥ –ß–µ—Ä–Ω—ñ–≥–æ–≤–∞.*?–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–£ –ù—ñ–º–µ—á—á–∏–Ω—ñ.*?–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–ù–æ–≤–∏–Ω–∏ –ö–∞—Ñ–µ\\s*–ù–æ–≤–∏–Ω–∏ –ë–∞—Ä—ñ–≤\\s*–ù–æ–≤–∏–Ω–∏ –ì–æ—Ç–µ–ª—ñ–≤\\s*–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n            r'–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏ HoReCa –£–∫—Ä–∞—ó–Ω–∞\\s*$',\n        ]\n        \n        for pattern in footer_patterns:\n            content = re.sub(pattern, '', content, flags=re.DOTALL)\n        \n        return content.strip()\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Legacy method - redirects to _clean_content\"\"\"\n        return self._clean_content(text, \"\")\n","path":null,"size_bytes":14234,"size_tokens":null},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"anthropic>=0.72.1\",\n    \"apscheduler>=3.11.1\",\n    \"beautifulsoup4>=4.14.2\",\n    \"fastapi>=0.121.1\",\n    \"lxml>=6.0.2\",\n    \"openai>=2.7.2\",\n    \"playwright>=1.56.0\",\n    \"psycopg2-binary>=2.9.11\",\n    \"pydantic>=2.12.4\",\n    \"python-dotenv>=1.2.1\",\n    \"python-multipart>=0.0.20\",\n    \"python-telegram-bot>=22.5\",\n    \"requests>=2.32.5\",\n    \"sqlalchemy>=2.0.44\",\n    \"trafilatura>=2.0.0\",\n    \"uvicorn>=0.38.0\",\n]\n","path":null,"size_bytes":563,"size_tokens":null},"README.md":{"content":"# Gradus Media AI Agent\n\nMulti-stage AI agent system for automated content creation and approval workflow.\n\n## Quick Start\n\n### Frontend (Already Running)\nThe frontend is automatically running on port 5000. Access it through the Replit webview.\n\n### Backend\nStart the backend API server:\n```bash\ncd backend && python start.py\n```\n\nOr manually:\n```bash\ncd backend && python -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n```\n\nThe API will be available at `http://localhost:8000` with interactive docs at `/docs`.\n\n## Testing\n\n### Test Claude Chat\n1. Go to the **Chat** page in the dashboard\n2. Enter a message and click \"Send Message\"\n3. Try the translation feature with English text\n\n### Test Content Approval\n1. Content will appear on the **Content Approval** page when scraped\n2. Review, approve, or reject content\n3. Edit translations or regenerate images before approval\n\n## Environment Setup\n\nRequired environment variables (set in Replit Secrets):\n- `ANTHROPIC_API_KEY` - For Claude AI (chat & translation)\n- `DATABASE_URL` - PostgreSQL connection (auto-configured)\n\nOptional:\n- `OPENAI_API_KEY` - For DALL-E image generation\n- `PINECONE_API_KEY` - For RAG functionality\n\n## API Endpoints\n\n- `GET /` - API info\n- `GET /health` - Health check\n- `POST /chat` - Chat with Claude\n- `POST /translate` - Translate English ‚Üí Ukrainian\n- `GET /api/content/pending` - Get pending content\n- `POST /api/content/{id}/approve` - Approve content\n- `POST /api/content/{id}/reject` - Reject content\n- `PUT /api/content/{id}/edit` - Edit content\n- `GET /api/content/stats` - Get statistics\n\n## Project Structure\n\n```\n‚îú‚îÄ‚îÄ backend/              # FastAPI application\n‚îÇ   ‚îú‚îÄ‚îÄ main.py          # API endpoints\n‚îÇ   ‚îú‚îÄ‚îÄ models/          # Database models\n‚îÇ   ‚îî‚îÄ‚îÄ services/        # Business logic\n‚îú‚îÄ‚îÄ frontend/            # React dashboard\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ pages/       # Dashboard pages\n‚îÇ       ‚îî‚îÄ‚îÄ App.jsx      # Main app\n‚îî‚îÄ‚îÄ replit.md           # Detailed documentation\n```\n\nSee `replit.md` for complete documentation.\n","path":null,"size_bytes":2090,"size_tokens":null},"backend/services/news_scraper.py":{"content":"import requests\nfrom bs4 import BeautifulSoup\nimport trafilatura\nfrom typing import List, Dict\nfrom datetime import datetime\nimport logging\nimport re\n\nlogger = logging.getLogger(__name__)\n\nclass NewsScraper:\n    def __init__(self):\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n    \n    def clean_article_content(self, content: str, title: str) -> str:\n        \"\"\"\n        Clean article content by removing:\n        - Duplicate title at the beginning\n        - Author byline (By Author Name)\n        - Related news section at the end\n        - Extra metadata\n        \n        Args:\n            content: Raw article text\n            title: Article title\n            \n        Returns:\n            Clean article content\n        \"\"\"\n        if not content:\n            return \"\"\n        \n        title_escaped = re.escape(title)\n        content = re.sub(f'^{title_escaped}\\\\s*', '', content, flags=re.IGNORECASE)\n        \n        byline_pattern = r'(?:^|\\n)By\\s+[A-Z][a-zA-Z\\'\\-\\.]*(?:\\s+[A-Z][a-zA-Z\\'\\-\\.]*)*(?=\\n|$|[A-Z][a-z]|[A-Z]{2,})'\n        content = re.sub(byline_pattern, '\\n', content, flags=re.MULTILINE)\n        \n        content = re.split(r'Related news|Related articles|Related content', content, flags=re.IGNORECASE)[0]\n        \n        content = re.sub(r'\\n\\s*\\n+', '\\n\\n', content)\n        content = content.strip()\n        \n        return content\n    \n    def extract_author(self, content: str) -> str:\n        \"\"\"\n        Extract author name from content.\n        Handles multi-word names, hyphens, apostrophes, and initials.\n        Returns author name or empty string\n        \"\"\"\n        byline_pattern = r'(?:^|\\n)By\\s+([A-Z][a-zA-Z\\'\\-\\.]*(?:\\s+[A-Z][a-zA-Z\\'\\-\\.]*)*?)(?=\\n|$|[A-Z][a-z]|[A-Z]{2,})'\n        match = re.search(byline_pattern, content, flags=re.MULTILINE)\n        if match:\n            return match.group(1).strip()\n        return \"\"\n    \n    def clean_article_title(self, title: str, source_name: str = \"The Spirits Business\") -> str:\n        \"\"\"\n        Clean article title by removing source suffix\n        \n        Args:\n            title: Raw article title\n            source_name: Source publication name to remove\n            \n        Returns:\n            Clean title without source suffix\n        \"\"\"\n        if not title:\n            return \"\"\n        \n        patterns = [\n            f' - {source_name}',\n            f' | {source_name}',\n            f' ‚Äì {source_name}',\n            f' ‚Äî {source_name}',\n        ]\n        \n        cleaned_title = title\n        for pattern in patterns:\n            if cleaned_title.endswith(pattern):\n                cleaned_title = cleaned_title[:-len(pattern)].strip()\n                break\n        \n        return cleaned_title\n    \n    def scrape_spirits_business(self, limit: int = 5) -> List[Dict]:\n        \"\"\"\n        Scrape latest articles from The Spirits Business homepage.\n        Returns a list of articles with title, URL, excerpt, date, and author.\n        \"\"\"\n        articles = []\n        \n        try:\n            logger.info(\"Scraping The Spirits Business...\")\n            url = \"https://www.thespiritsbusiness.com\"\n            \n            response = requests.get(url, headers=self.headers, timeout=15)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            article_links = soup.find_all('a', href=re.compile(r'/\\d{4}/\\d{2}/'))\n            \n            seen_urls = set()\n            \n            for link in article_links:\n                if len(articles) >= limit:\n                    break\n                \n                article_url = link.get('href')\n                if not article_url or article_url in seen_urls:\n                    continue\n                \n                if not article_url.startswith('http'):\n                    article_url = url + article_url\n                \n                seen_urls.add(article_url)\n                \n                title_elem = link.find('strong') or link.find(['h1', 'h2', 'h3'])\n                if title_elem:\n                    title = title_elem.get_text(strip=True)\n                else:\n                    title = link.get_text(strip=True)\n                \n                if not title or len(title) < 10:\n                    continue\n                \n                parent = link.find_parent(['article', 'div', 'li'])\n                excerpt = \"\"\n                date_str = \"\"\n                author = \"\"\n                \n                if parent:\n                    date_elem = parent.find('time') or parent.find(text=re.compile(r'\\d{1,2}\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}'))\n                    if date_elem:\n                        date_str = date_elem if isinstance(date_elem, str) else date_elem.get_text(strip=True)\n                    \n                    author_elem = parent.find(text=re.compile(r'^By\\s+'))\n                    if author_elem:\n                        author = author_elem.strip()\n                    \n                    for p in parent.find_all(['p', 'div'], limit=3):\n                        text = p.get_text(strip=True)\n                        if text and len(text) > 50 and 'By ' not in text[:10]:\n                            excerpt = text[:300]\n                            break\n                \n                clean_data = self.extract_article_content(article_url)\n                raw_content = clean_data.get('content', '')\n                raw_title = clean_data.get('title') or title\n                \n                article_title = self.clean_article_title(raw_title, \"The Spirits Business\")\n                \n                extracted_author = self.extract_author(raw_content)\n                if extracted_author:\n                    author = extracted_author\n                elif author:\n                    author = re.sub(r'^By\\s+', '', author).strip()\n                \n                cleaned_content = self.clean_article_content(raw_content, article_title)\n                \n                article_data = {\n                    'source': 'The Spirits Business',\n                    'url': article_url,\n                    'title': article_title,\n                    'summary': cleaned_content[:1000],\n                    'content': cleaned_content,\n                    'image_url': '',\n                    'published_date': date_str,\n                    'author': author,\n                    'scraped_at': datetime.utcnow().isoformat()\n                }\n                \n                articles.append(article_data)\n                logger.info(f\"Scraped article: {title[:50]}... ({len(cleaned_content)} chars cleaned)\")\n            \n            logger.info(f\"Successfully scraped {len(articles)} articles from The Spirits Business\")\n            return articles\n            \n        except Exception as e:\n            logger.error(f\"Error scraping The Spirits Business: {str(e)}\")\n            raise\n    \n    def extract_article_content(self, url: str) -> Dict[str, str]:\n        \"\"\"\n        Extract clean article content from URL using Trafilatura.\n        Removes metadata like date, author, category.\n        \n        Returns dict with 'title' and 'content'\n        \"\"\"\n        try:\n            logger.info(f\"Extracting clean content from: {url}\")\n            \n            downloaded = trafilatura.fetch_url(url)\n            \n            if not downloaded:\n                logger.warning(f\"Failed to download {url}\")\n                return {\"title\": \"\", \"content\": \"\"}\n            \n            result = trafilatura.extract(\n                downloaded,\n                include_comments=False,\n                include_tables=False,\n                output_format='json',\n                with_metadata=True\n            )\n            \n            if result:\n                import json\n                data = json.loads(result)\n                \n                title = data.get('title', '')\n                content = data.get('text', '')\n                \n                logger.info(f\"Extracted {len(content)} chars from: {title[:50]}...\")\n                \n                return {\n                    \"title\": title,\n                    \"content\": content\n                }\n            \n            logger.warning(f\"No content extracted from {url}\")\n            return {\"title\": \"\", \"content\": \"\"}\n            \n        except Exception as e:\n            logger.error(f\"Failed to extract content from {url}: {e}\")\n            return {\"title\": \"\", \"content\": \"\"}\n    \n    def get_article_content(self, url: str) -> Dict:\n        \"\"\"\n        Fetch and extract full content from a specific article URL using trafilatura.\n        (Deprecated - use extract_article_content instead)\n        \"\"\"\n        try:\n            logger.info(f\"Fetching article content from: {url}\")\n            \n            response = requests.get(url, headers=self.headers, timeout=15)\n            response.raise_for_status()\n            \n            downloaded = response.content\n            \n            text_content = trafilatura.extract(\n                downloaded,\n                include_comments=False,\n                include_tables=False,\n                no_fallback=False\n            )\n            \n            metadata = trafilatura.extract_metadata(downloaded)\n            \n            article_data = {\n                'url': url,\n                'title': metadata.title if metadata and metadata.title else 'No title',\n                'author': metadata.author if metadata and metadata.author else '',\n                'date': metadata.date if metadata and metadata.date else '',\n                'content': text_content if text_content else '',\n                'description': metadata.description if metadata and metadata.description else '',\n                'scraped_at': datetime.utcnow().isoformat()\n            }\n            \n            logger.info(f\"Successfully extracted content from: {url}\")\n            return article_data\n            \n        except Exception as e:\n            logger.error(f\"Error fetching article content from {url}: {str(e)}\")\n            raise\n\nnews_scraper = NewsScraper()\n","path":null,"size_bytes":10276,"size_tokens":null},"export_database.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nDatabase Export Script\nExports the Replit PostgreSQL database to a backup.sql file\n\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom urllib.parse import urlparse\nfrom datetime import datetime\n\ndef export_database():\n    \"\"\"Export PostgreSQL database to backup.sql file\"\"\"\n    \n    print(\"=\" * 50)\n    print(\"PostgreSQL Database Export\")\n    print(\"=\" * 50)\n    \n    database_url = os.getenv(\"DATABASE_URL\")\n    \n    if not database_url:\n        print(\"ERROR: DATABASE_URL environment variable not found\")\n        sys.exit(1)\n    \n    try:\n        parsed = urlparse(database_url)\n        \n        db_host = parsed.hostname\n        db_port = parsed.port or 5432\n        db_name = parsed.path.lstrip('/')\n        db_user = parsed.username\n        db_password = parsed.password\n        \n        print(f\"Host: {db_host}\")\n        print(f\"Port: {db_port}\")\n        print(f\"Database: {db_name}\")\n        print(f\"User: {db_user}\")\n        print(\"-\" * 50)\n        \n    except Exception as e:\n        print(f\"ERROR: Failed to parse DATABASE_URL: {e}\")\n        sys.exit(1)\n    \n    output_file = \"backup.sql\"\n    \n    env = os.environ.copy()\n    env[\"PGPASSWORD\"] = db_password\n    \n    pg_dump_cmd = [\n        \"pg_dump\",\n        \"-h\", db_host,\n        \"-p\", str(db_port),\n        \"-U\", db_user,\n        \"-d\", db_name,\n        \"-F\", \"p\",\n        \"-f\", output_file,\n        \"--no-owner\",\n        \"--no-acl\",\n        \"--clean\",\n        \"--if-exists\",\n        \"--create\"\n    ]\n    \n    print(f\"Exporting database to {output_file}...\")\n    print(\"This may take a moment...\")\n    print()\n    \n    try:\n        result = subprocess.run(\n            pg_dump_cmd,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode != 0:\n            print(f\"ERROR: pg_dump failed with exit code {result.returncode}\")\n            if result.stderr:\n                print(f\"Error details: {result.stderr}\")\n            sys.exit(1)\n        \n        if os.path.exists(output_file):\n            file_size = os.path.getsize(output_file)\n            file_size_kb = file_size / 1024\n            file_size_mb = file_size_kb / 1024\n            \n            print(\"=\" * 50)\n            print(\"EXPORT COMPLETE\")\n            print(\"=\" * 50)\n            print(f\"Output file: {output_file}\")\n            \n            if file_size_mb >= 1:\n                print(f\"File size: {file_size_mb:.2f} MB\")\n            else:\n                print(f\"File size: {file_size_kb:.2f} KB\")\n            \n            print(f\"Exported at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n            print()\n            print(\"The backup includes:\")\n            print(\"  - All table schemas\")\n            print(\"  - All table data\")\n            print(\"  - All indexes and constraints\")\n            print(\"  - DROP/CREATE statements for clean restore\")\n            print()\n            \n        else:\n            print(\"ERROR: Output file was not created\")\n            sys.exit(1)\n            \n    except FileNotFoundError:\n        print(\"ERROR: pg_dump command not found\")\n        print(\"Make sure PostgreSQL client tools are installed\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"ERROR: Unexpected error during export: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    export_database()\n","path":null,"size_bytes":3343,"size_tokens":null},"start.sh":{"content":"#!/bin/bash\n\necho \"========================================\"\necho \"  Gradus Media AI Agent - Starting...\"\necho \"========================================\"\necho \"\"\n\ncleanup() {\n    echo \"\"\n    echo \"Shutting down services...\"\n    kill $BACKEND_PID $FRONTEND_PID 2>/dev/null\n    exit 0\n}\n\ntrap cleanup SIGINT SIGTERM\n\necho \"[$(date '+%Y-%m-%d %H:%M:%S')] Starting FastAPI backend on port 8000...\"\ncd backend\nuvicorn main:app --host 0.0.0.0 --port 8000 --log-level info &\nBACKEND_PID=$!\ncd ..\n\nsleep 2\n\necho \"[$(date '+%Y-%m-%d %H:%M:%S')] Starting React frontend on port 5000...\"\ncd frontend\nnpm run preview -- --host 0.0.0.0 --port 5000 &\nFRONTEND_PID=$!\ncd ..\n\nsleep 2\n\necho \"\"\necho \"========================================\"\necho \"  Services Started Successfully\"\necho \"========================================\"\necho \"  Backend:  http://0.0.0.0:8000\"\necho \"  Frontend: http://0.0.0.0:5000\"\necho \"  Health:   http://0.0.0.0:8000/health\"\necho \"========================================\"\necho \"\"\necho \"Press Ctrl+C to stop all services\"\necho \"\"\n\nwait $BACKEND_PID $FRONTEND_PID\n","path":null,"size_bytes":1074,"size_tokens":null}},"version":2}
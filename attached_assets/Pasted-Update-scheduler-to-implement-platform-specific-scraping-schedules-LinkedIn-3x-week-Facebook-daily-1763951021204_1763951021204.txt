Update scheduler to implement platform-specific scraping schedules (LinkedIn 3x/week, Facebook daily):

üìã TASK: Configure scheduler for platform-optimized scraping and posting

UPDATE backend/services/scheduler.py:

REPLACE the existing scraping tasks with platform-specific logic:
```python
def scrape_linkedin_sources_task(self):
    """
    Scrape LinkedIn sources: The Spirits Business, Delo.ua, MinFin.ua
    Runs: Mon/Wed/Fri at 1:00 AM
    
    These are professional, business-oriented sources for B2B audience
    """
    logger.info("ü§ñ [SCHEDULER] Scraping LinkedIn sources...")
    
    try:
        from services.scrapers.scraper_manager import ScraperManager
        from models import ContentQueue
        
        db = self._get_db_session()
        manager = ScraperManager()
        
        # LinkedIn sources
        linkedin_sources = ['the_spirits_business', 'delo_ua', 'minfin_ua']
        
        all_articles = []
        for source_name in linkedin_sources:
            try:
                articles = manager.scrape_source(source_name, limit=3)
                all_articles.extend(articles)
                logger.info(f"  ‚úÖ {source_name}: {len(articles)} articles")
            except Exception as e:
                logger.error(f"  ‚ùå {source_name} failed: {e}")
        
        # Save to database
        new_count = 0
        for article in all_articles:
            # Check duplicates
            existing = db.query(ContentQueue).filter(
                ContentQueue.source_url == article.url
            ).first()
            
            if not existing:
                new_article = ContentQueue(
                    original_text=article.content,
                    source_url=article.url,
                    status='draft',
                    target_platform='linkedin',
                    language=article.language,
                    needs_translation=article.needs_translation,
                    source_category='business',
                    content_tone='professional',
                    extra_metadata={
                        'title': article.title,
                        'source': article.source,
                        'author': article.author,
                        'image_url': article.image_url,
                        'published_date': article.published_date,
                        'scraped_at': datetime.now().isoformat()
                    }
                )
                db.add(new_article)
                new_count += 1
        
        db.commit()
        db.close()
        
        logger.info(f"‚úÖ [SCHEDULER] LinkedIn: Scraped {new_count} new articles")
        
    except Exception as e:
        logger.error(f"‚ùå [SCHEDULER] LinkedIn scraping failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

def scrape_facebook_sources_task(self):
    """
    Scrape Facebook sources: Just Drinks, Restorator.ua, The Drinks Report
    Runs: Every day at 2:00 AM
    
    These are lighter, more engaging sources for general audience
    """
    logger.info("ü§ñ [SCHEDULER] Scraping Facebook sources...")
    
    try:
        from services.scrapers.scraper_manager import ScraperManager
        from models import ContentQueue
        
        db = self._get_db_session()
        manager = ScraperManager()
        
        # Facebook sources
        facebook_sources = ['just_drinks', 'restorator_ua', 'the_drinks_report']
        
        all_articles = []
        for source_name in facebook_sources:
            try:
                articles = manager.scrape_source(source_name, limit=3)
                all_articles.extend(articles)
                logger.info(f"  ‚úÖ {source_name}: {len(articles)} articles")
            except Exception as e:
                logger.error(f"  ‚ùå {source_name} failed: {e}")
        
        # Save to database
        new_count = 0
        for article in all_articles:
            # Check duplicates
            existing = db.query(ContentQueue).filter(
                ContentQueue.source_url == article.url
            ).first()
            
            if not existing:
                new_article = ContentQueue(
                    original_text=article.content,
                    source_url=article.url,
                    status='draft',
                    target_platform='facebook',
                    language=article.language,
                    needs_translation=article.needs_translation,
                    source_category='horeca' if 'restorator' in source_name.lower() else 'industry',
                    content_tone='casual',
                    extra_metadata={
                        'title': article.title,
                        'source': article.source,
                        'author': article.author,
                        'image_url': article.image_url,
                        'published_date': article.published_date,
                        'scraped_at': datetime.now().isoformat()
                    }
                )
                db.add(new_article)
                new_count += 1
        
        db.commit()
        db.close()
        
        logger.info(f"‚úÖ [SCHEDULER] Facebook: Scraped {new_count} new articles")
        
    except Exception as e:
        logger.error(f"‚ùå [SCHEDULER] Facebook scraping failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
```

UPDATE the start() method to use new scraping schedule:
```python
def start(self):
    """Start scheduler with platform-specific scraping and posting"""
    
    logger.info("üöÄ Starting Gradus Media AI Agent Scheduler...")
    
    # ========================================
    # SCRAPING (Platform-specific)
    # ========================================
    
    # LinkedIn sources: Mon/Wed/Fri at 1:00 AM
    self.scheduler.add_job(
        self.scrape_linkedin_sources_task,
        CronTrigger(day_of_week='mon,wed,fri', hour=1, minute=0),
        id='scrape_linkedin',
        name='Scrape LinkedIn sources (TSB, Delo, MinFin)',
        replace_existing=True
    )
    
    # Facebook sources: Daily at 2:00 AM
    self.scheduler.add_job(
        self.scrape_facebook_sources_task,
        CronTrigger(hour=2, minute=0),
        id='scrape_facebook',
        name='Scrape Facebook sources (Just Drinks, Restorator, Drinks Report)',
        replace_existing=True
    )
    
    # ========================================
    # PROCESSING
    # ========================================
    
    # Translation: Every 4 hours at :15 (only English articles)
    self.scheduler.add_job(
        self.translate_pending_task,
        CronTrigger(hour='*/4', minute=15),
        id='translate_articles',
        name='Translate pending articles',
        replace_existing=True
    )
    
    # Images: Every 4 hours at :30 (all articles)
    self.scheduler.add_job(
        self.generate_images_task,
        CronTrigger(hour='*/4', minute=30),
        id='generate_images',
        name='Generate images & send Telegram notifications',
        replace_existing=True
    )
    
    # ========================================
    # POSTING (Scheduled times)
    # ========================================
    
    # LinkedIn: Mon/Wed/Fri at 9:00 AM
    self.scheduler.add_job(
        self.post_to_linkedin_task,
        CronTrigger(day_of_week='mon,wed,fri', hour=9, minute=0),
        id='post_linkedin',
        name='Post to LinkedIn',
        replace_existing=True
    )
    
    # Facebook: Daily at 6:00 PM
    self.scheduler.add_job(
        self.post_to_facebook_task,
        CronTrigger(hour=18, minute=0),
        id='post_facebook',
        name='Post to Facebook',
        replace_existing=True
    )
    
    # ========================================
    # MAINTENANCE
    # ========================================
    
    # Cleanup old content: Daily at 3:00 AM
    self.scheduler.add_job(
        self.cleanup_old_content_task,
        CronTrigger(hour=3, minute=0),
        id='cleanup_old_content',
        name='Cleanup old rejected content',
        replace_existing=True
    )
    
    # API monitoring: Daily at 8:00 AM
    self.scheduler.add_job(
        self.check_all_api_tokens_task,
        CronTrigger(hour=8, minute=0),
        id='check_api_tokens',
        name='Check all API tokens',
        replace_existing=True
    )
    
    # Start the scheduler
    self.scheduler.start()
    
    # Print schedule summary
    logger.info("=" * 60)
    logger.info("‚úÖ GRADUS MEDIA AI AGENT - FULLY OPERATIONAL")
    logger.info("=" * 60)
    logger.info("")
    logger.info("üì∞ CONTENT SOURCES:")
    logger.info("   LinkedIn (3x/week):")
    logger.info("      ‚Ä¢ The Spirits Business üá¨üáß")
    logger.info("      ‚Ä¢ Delo.ua üá∫üá¶")
    logger.info("      ‚Ä¢ MinFin.ua üá∫üá¶")
    logger.info("")
    logger.info("   Facebook (Daily):")
    logger.info("      ‚Ä¢ Just Drinks üá¨üáß")
    logger.info("      ‚Ä¢ Restorator.ua üá∫üá¶")
    logger.info("      ‚Ä¢ The Drinks Report üá¨üáß")
    logger.info("")
    logger.info("üìÖ SCRAPING SCHEDULE:")
    logger.info("   ‚Ä¢ LinkedIn: Mon/Wed/Fri 1:00 AM")
    logger.info("   ‚Ä¢ Facebook: Daily 2:00 AM")
    logger.info("")
    logger.info("üîÑ PROCESSING:")
    logger.info("   ‚Ä¢ Translation: Every 4 hours at :15")
    logger.info("   ‚Ä¢ Images: Every 4 hours at :30")
    logger.info("")
    logger.info("üì¢ POSTING SCHEDULE:")
    logger.info("   ‚Ä¢ LinkedIn: Mon/Wed/Fri 9:00 AM")
    logger.info("   ‚Ä¢ Facebook: Daily 6:00 PM")
    logger.info("")
    logger.info("üîß MAINTENANCE:")
    logger.info("   ‚Ä¢ API monitoring: Daily 8:00 AM")
    logger.info("   ‚Ä¢ Cleanup: Daily 3:00 AM")
    logger.info("")
    logger.info("=" * 60)
    logger.info("üöÄ System ready! Waiting for next scheduled task...")
    logger.info("=" * 60)
```

VERIFY scheduler has correct task count:

After restart, you should see 8 automated tasks:
1. scrape_linkedin (Mon/Wed/Fri 1am)
2. scrape_facebook (Daily 2am)
3. translate_articles (Every 4h :15)
4. generate_images (Every 4h :30)
5. post_linkedin (Mon/Wed/Fri 9am)
6. post_facebook (Daily 6pm)
7. check_api_tokens (Daily 8am)
8. cleanup_old_content (Daily 3am)

TEST the new setup:
```bash
# Check scheduler status
curl http://localhost:8000/api/scheduler/status

# Manually trigger LinkedIn scraping (to test)
curl -X POST http://localhost:8000/api/scheduler/trigger/scrape_linkedin

# Manually trigger Facebook scraping (to test)
curl -X POST http://localhost:8000/api/scheduler/trigger/scrape_facebook

# Check scraped content
curl http://localhost:8000/api/content?status=draft&limit=10
```

Please update scheduler with platform-specific scraping!
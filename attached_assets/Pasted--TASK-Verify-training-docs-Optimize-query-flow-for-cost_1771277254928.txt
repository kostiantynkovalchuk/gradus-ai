# TASK: Verify training docs + Optimize query flow for cost efficiency

## PART 1: DATABASE VERIFICATION (2 minutes)

Run these queries and show results:
```sql
-- 1. Show all content sections
SELECT section, COUNT(*) as count, 
       STRING_AGG(DISTINCT subsection, ', ') as subsections
FROM hr_content
GROUP BY section
ORDER BY section;

-- 2. Check specifically for training/–ù–∞–≤—á–∞–Ω–Ω—è
SELECT id, section, subsection, title, 
       LENGTH(content) as content_length,
       LEFT(content, 150) as preview
FROM hr_content
WHERE section = 'training'
   OR title ILIKE '%–Ω–∞–≤—á–∞–Ω–Ω—è%'
   OR title ILIKE '%–±–ª—ñ—Ü%'
   OR title ILIKE '%hr-–ø—Ä–æ—Ü–µ—Å%';

-- 3. Verify embeddings exist for training content
SELECT 
    c.id,
    c.title,
    COUNT(e.id) as embedding_count
FROM hr_content c
LEFT JOIN hr_embeddings e ON c.id = e.content_id
WHERE c.section = 'training'
GROUP BY c.id, c.title;
```

Show me the full results so I can verify the "–ù–∞–≤—á–∞–Ω–Ω—è" document is in the database.

---

## PART 2: OPTIMIZE QUERY FLOW (15 minutes)

**Current problem:** RAG semantic search (which costs money) runs on EVERY query that doesn't hit presets. But many queries could be answered with FREE PostgreSQL keyword search first.

**Required change:**
```
BEFORE: Preset ‚Üí RAG (costs $$$) ‚Üí Keyword fallback
AFTER:  Preset ‚Üí Keyword (FREE) ‚Üí RAG (only when needed)
```

### Find and refactor the query processing code:

**Files to check:**
- `backend/services/hr_rag_service.py`
- `backend/routes/chat_endpoints.py`
- Wherever user queries are processed

**Change the flow to:**
```python
async def process_hr_query(user_message: str):
    # 1. Check presets (instant, free)
    preset = await check_preset_answers(user_message)
    if preset:
        logger.info(f"‚úÖ PRESET hit for: {user_message[:50]}")
        return preset
    
    # 2. PostgreSQL keyword search (fast, free)
    keyword_results = await keyword_search_content(user_message)
    if keyword_results and len(keyword_results) > 0:
        logger.info(f"‚úÖ KEYWORD search hit for: {user_message[:50]}")
        # Use keyword results to build context
        context = build_context_from_results(keyword_results)
        return await generate_answer_with_context(user_message, context)
    
    # 3. RAG semantic search (only when needed, costs $$$)
    logger.info(f"üîç RAG semantic search for: {user_message[:50]}")
    rag_results = await rag_semantic_search(user_message)
    if rag_results and len(rag_results.matches) > 0:
        logger.info(f"‚úÖ RAG hit - top score: {rag_results.matches[0].score}")
        context = build_context_from_rag(rag_results)
        return await generate_answer_with_context(user_message, context)
    
    # 4. No results from any method
    logger.warning(f"‚ùå NO RESULTS for: {user_message[:50]}")
    return "–í–∏–±–∞—á—Ç–µ, —è –Ω–µ –∑–Ω–∞–π—à–æ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –±–∞–∑—ñ –∑–Ω–∞–Ω—å. –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –∑–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ HR-–≤—ñ–¥–¥—ñ–ª—É."
```

### Enhance keyword search:

Make sure keyword search is robust with PostgreSQL full-text search:
```python
async def keyword_search_content(query: str, limit: int = 3):
    """
    Fast PostgreSQL full-text search using Ukrainian dictionary
    Returns top matching content from hr_content table
    """
    # Clean query
    query_clean = query.strip()
    
    # PostgreSQL full-text search with ranking
    results = await db.fetch("""
        SELECT 
            id,
            section,
            title,
            content,
            ts_rank_cd(
                to_tsvector('ukrainian', title || ' ' || content),
                plainto_tsquery('ukrainian', $1)
            ) as rank
        FROM hr_content
        WHERE to_tsvector('ukrainian', title || ' ' || content) 
              @@ plainto_tsquery('ukrainian', $1)
        ORDER BY rank DESC
        LIMIT $2
    """, query_clean, limit)
    
    # Log results
    if results:
        logger.info(f"  ‚Üí Found {len(results)} keyword matches")
        logger.info(f"  ‚Üí Top match: {results[0]['title']} (rank: {results[0]['rank']:.4f})")
    
    return results if results else []
```

### Add detailed logging:
```python
# At the start of query processing
logger.info(f"üì® Query received: '{user_message[:100]}...'")
start_time = time.time()

# After each method
logger.info(f"‚è±Ô∏è  Response time: {(time.time() - start_time)*1000:.0f}ms")
logger.info(f"üí∞ Method: {method}")  # preset/keyword/rag
logger.info(f"üíµ Cost: ${cost}")  # $0 for preset/keyword, $0.0001 for RAG
```

---

## PART 3: TESTING (5 minutes)

After implementing, test these queries and show me which method each uses:

1. **Should hit preset:**
   - "–Ø–∫ –æ—Ñ–æ—Ä–º–∏—Ç–∏ –≤—ñ–¥–ø—É—Å—Ç–∫—É?"
   - Expected log: "‚úÖ PRESET hit"

2. **Should hit keyword search:**
   - "–†–æ–±–æ—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º—ñ –ë–ª—ñ—Ü"
   - "–ö–æ–Ω—Ç–∞–∫—Ç–∏ HR –≤—ñ–¥–¥—ñ–ª—É"
   - Expected log: "‚úÖ KEYWORD search hit"

3. **Should fall back to RAG:**
   - "–Ø–∫ –º–µ–Ω—ñ –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏—Å—è –¥–æ –ø–µ—Ä—à–æ–≥–æ —Ä–æ–±–æ—á–æ–≥–æ –¥–Ω—è?"
   - Expected log: "üîç RAG semantic search"

Run all 4 test queries and show me the logs for each.

---

## DELIVERABLES

1. ‚úÖ Database verification results (confirm training doc exists)
2. ‚úÖ Refactored query flow (Preset ‚Üí Keyword ‚Üí RAG)
3. ‚úÖ Enhanced keyword search with logging
4. ‚úÖ Test results showing which method each query uses
5. ‚úÖ Backend running with optimized flow

This will reduce RAG API costs by 60-80% while maintaining answer quality.
```

---

## **üí∞ EXPECTED OUTCOME**

**Cost for this task:** ~$5-10

**Benefits:**
- ‚úÖ Verify training docs are in database
- ‚úÖ Optimize query flow (keyword before RAG)
- ‚úÖ Save 60-80% on embedding API costs
- ‚úÖ Faster response times
- ‚úÖ Better logging for monitoring

**After optimization:**
```
Remaining credits: $64-70 of $100
ROI: This optimization pays for itself in ~6 months of operation
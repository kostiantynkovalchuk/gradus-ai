ðŸ“ PROMPT Ð”Ð›Ð¯ REPLIT AGENT:
Great work on Telegram notifications! Now let's add a simple news scraper.

Please implement ONLY a basic news scraper - no automation yet, just manual trigger:

1. CREATE backend/services/news_scraper.py:
```python
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

class NewsScraper:
    def __init__(self):
        self.sources = {
            'spirits_business': 'https://www.thespiritsbusiness.com/'
        }
    
    def scrape_spirits_business(self, limit: int = 5) -> List[Dict]:
        """
        Scrape recent articles from The Spirits Business
        Returns list of articles with: title, url, summary, image_url, published_date
        """
        articles = []
        
        try:
            # Fetch the homepage
            response = requests.get(
                self.sources['spirits_business'],
                headers={'User-Agent': 'Mozilla/5.0'},
                timeout=15
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find article containers (adjust selectors based on actual site structure)
            # This is a basic example - may need adjustment
            article_elements = soup.find_all('article', limit=limit)
            
            for article in article_elements:
                try:
                    # Extract title
                    title_elem = article.find('h2') or article.find('h3')
                    title = title_elem.get_text(strip=True) if title_elem else None
                    
                    # Extract link
                    link_elem = article.find('a', href=True)
                    url = link_elem['href'] if link_elem else None
                    if url and not url.startswith('http'):
                        url = self.sources['spirits_business'].rstrip('/') + '/' + url.lstrip('/')
                    
                    # Extract summary/excerpt
                    summary_elem = article.find('p')
                    summary = summary_elem.get_text(strip=True) if summary_elem else ""
                    
                    # Extract image
                    img_elem = article.find('img')
                    image_url = img_elem.get('src') or img_elem.get('data-src') if img_elem else None
                    if image_url and not image_url.startswith('http'):
                        image_url = self.sources['spirits_business'].rstrip('/') + '/' + image_url.lstrip('/')
                    
                    if title and url:
                        articles.append({
                            'title': title,
                            'url': url,
                            'summary': summary[:500] if summary else "",
                            'image_url': image_url,
                            'source': 'The Spirits Business',
                            'published_date': datetime.now().isoformat(),
                            'scraped_at': datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    logger.error(f"Error parsing article: {e}")
                    continue
            
            logger.info(f"Scraped {len(articles)} articles from The Spirits Business")
            return articles
            
        except Exception as e:
            logger.error(f"Error scraping The Spirits Business: {e}")
            return []
    
    def scrape_all_sources(self, limit_per_source: int = 5) -> List[Dict]:
        """Scrape all configured sources"""
        all_articles = []
        
        # For now, just The Spirits Business
        articles = self.scrape_spirits_business(limit=limit_per_source)
        all_articles.extend(articles)
        
        return all_articles
```

2. ADD MANUAL SCRAPER ENDPOINT in backend/main.py:
```python
from services.news_scraper import NewsScraper
from services.notification_service import NotificationService

@app.post("/api/scraper/run")
async def run_scraper_manually(limit: int = 5):
    """
    Manually trigger news scraper
    Returns scraped articles
    """
    scraper = NewsScraper()
    articles = scraper.scrape_all_sources(limit_per_source=limit)
    
    return {
        "status": "success",
        "articles_found": len(articles),
        "articles": articles
    }

@app.post("/api/scraper/test")
async def test_scraper():
    """Test scraper and return first article"""
    scraper = NewsScraper()
    articles = scraper.scrape_spirits_business(limit=1)
    
    if articles:
        return {
            "status": "success",
            "sample_article": articles[0]
        }
    else:
        return {
            "status": "error",
            "message": "No articles found"
        }
```

3. ADD BeautifulSoup to requirements:

If not already there, add to backend/requirements.txt:
beautifulsoup4==4.12.2
lxml==4.9.3

That's it! Just implement:
- news_scraper.py with scrape_spirits_business()
- /api/scraper/run endpoint (manual trigger)
- /api/scraper/test endpoint (test with 1 article)

We'll test it manually first, then integrate with database and notifications in the next step.

Please implement and let me know when ready to test!